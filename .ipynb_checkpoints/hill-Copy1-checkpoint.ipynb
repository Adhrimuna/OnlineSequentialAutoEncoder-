{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606, 100) (606, 100) (606,) (606,) (1212, 100)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import mnist_handler\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"; \n",
    "INPUT_DIMENSION = 100\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "train_hill_data = pd.read_csv(\"UCI dataset/Hill_Valley_with_noise_Training.csv\")\n",
    "test_hill_data = pd.read_csv(\"UCI dataset/Hill_Valley_with_noise_Testing.csv\")\n",
    "\n",
    "x_train = train_hill_data.iloc[:,0:100]\n",
    "y_train = train_hill_data.iloc[:,100]\n",
    "\n",
    "x_test = test_hill_data.iloc[:,0:100]\n",
    "y_test = test_hill_data.iloc[:,100]\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "data = np.concatenate((x_train,x_test),axis = 0)\n",
    "\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape,data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.23410461e-04 3.26826856e-04 3.13785122e-04 ... 3.31526542e-04\n",
      "  3.22973439e-04 3.33616018e-04]\n",
      " [7.72450182e-06 7.43831843e-06 7.72911145e-06 ... 7.90166888e-06\n",
      "  7.41453098e-06 6.98489438e-06]\n",
      " [5.78716027e-01 6.07348067e-01 6.13128820e-01 ... 6.10342597e-01\n",
      "  5.94957501e-01 6.54129683e-01]\n",
      " ...\n",
      " [8.62404977e-02 8.30862924e-02 7.55612217e-02 ... 8.90258702e-02\n",
      "  7.94121866e-02 8.12420050e-02]\n",
      " [2.87674029e-04 3.13786841e-04 2.86733232e-04 ... 2.71662812e-04\n",
      "  2.81752177e-04 2.96246833e-04]\n",
      " [7.20544077e-02 7.04535483e-02 7.67138675e-02 ... 7.45382462e-02\n",
      "  7.40429187e-02 7.60647140e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = data[0:606,:]\n",
    "x_test = data[606:,:]\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "border = 106\n",
    "x_train_init = x_train[:border]\n",
    "y_train_init = y_train[:border]\n",
    "x_train_seq = x_train[border:]\n",
    "y_train_seq = y_train[border:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # clear all the tensors\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "\"\"\"Placeholders\"\"\"\n",
    "X_ = tf.placeholder(tf.float32, [None, INPUT_DIMENSION])\n",
    "#X_ = tf.reshape(X, [-1, INPUT_DIMENSION]) # Flatten X: [N,D]\n",
    "Y = tf.placeholder(tf.int64, [None]) # labels\n",
    "Y_ = tf.one_hot(indices=Y, depth=NUM_CLASSES) # one_hot labels: [N,M]\n",
    "\n",
    "\"\"\"Some constants\"\"\"\n",
    "D = INPUT_DIMENSION\n",
    "M = NUM_CLASSES # Number of outputs\n",
    "C = tf.constant(2.0**(-5))\n",
    "\n",
    "\"\"\"Weights\"\"\"\n",
    "alpha_1 = tf.get_variable('alpha_1',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 1st subnetwork\n",
    "alpha_2 = tf.get_variable('alpha_2',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 2st subnetwork\n",
    "alpha_3 = tf.get_variable('alpha_3',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_1 = tf.get_variable('beta_1',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_2 = tf.get_variable('beta_2',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_3 = tf.get_variable('beta_3',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.get_variable('k',shape=[D, D],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "m = tf.get_variable('m',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions\"\"\"\n",
    "def mul(A, B):\n",
    "    return tf.matmul(A, B)\n",
    "\n",
    "def inv(A):\n",
    "    return tf.matrix_inverse(A)\n",
    "\n",
    "def t(A):\n",
    "    return tf.transpose(A)\n",
    "\n",
    "def sin(A):\n",
    "    return tf.math.sin(A)\n",
    "\n",
    "def asin(A):\n",
    "    return tf.math.asin(A)\n",
    "\n",
    "def sqrt(A):\n",
    "    return tf.sqrt(A)\n",
    "\n",
    "def sqr(A):\n",
    "    return tf.math.pow(A, 2)\n",
    "\n",
    "def pseudo_inv(A, I, C):\n",
    "    C_I = I/C\n",
    "    return mul(t(A), inv(C_I + mul(A, t(A))))\n",
    "\n",
    "def h(A):\n",
    "    '''activation function'''\n",
    "    return sin(A)\n",
    "\n",
    "def h_(A):\n",
    "    '''inverse activation function'''\n",
    "    return asin(A)\n",
    "\n",
    "def u(A):\n",
    "    '''normalize the input to (0,1]'''\n",
    "    return tf.math.sigmoid(A) # sigmoid\n",
    "    \n",
    "def u_(A):\n",
    "    '''the inverse of u'''\n",
    "    ONE = tf.constant(1.0)\n",
    "    return -(tf.math.log(ONE/A - ONE)) # the inverse of sigmoid\n",
    "    \n",
    "def subnet_output(alpha, beta, A):\n",
    "    return t(mul(beta, h(mul(t(alpha), t(A))))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "'''some pre-computations'''\n",
    "X_init = t(X_) # [D,N]\n",
    "Y_init = t(Y_) # [M,N]\n",
    "N_init = D # number of dimensions\n",
    "I_DxD = tf.eye(N_init, dtype=tf.float32) # [D,D]\n",
    "I_MxM = tf.eye(M, dtype=tf.float32) # [M,M]\n",
    "C_I = I_DxD/C\n",
    "H_I = I_MxM/C\n",
    "\n",
    "add = C_I + mul(X_init, t(X_init))\n",
    "k = tf.assign(k,add)\n",
    "X_inv_init = pseudo_inv(X_init, I_DxD, C) # [N,D]\n",
    "\n",
    "'''1st subnet'''\n",
    "alpha_1_init_calculated = t(mul(h_(Y_init), X_inv_init)) # ([M,N]x[N,D])T=[D,M]\n",
    "alpha_1_init = tf.assign(alpha_1, alpha_1_init_calculated) # [D,M]\n",
    "H_1_init = h(mul(t(alpha_1_init), X_init)) # [M,N]\n",
    "H_add = H_I + mul(H_1_init,t(H_1_init))\n",
    "m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_1_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_1_init_calculated = mul(Y_init, t(H_1_init))/sqr(tf.norm(H_1_init)) # [M,M]\n",
    "beta_1_init_calculated = mul(Y_init,H_pseudo_init)\n",
    "\n",
    "beta_1_init = tf.assign(beta_1, beta_1_init_calculated) # [M,M]\n",
    "H_beta_1_init = mul(beta_1_init, t(mul(t(X_init), alpha_1_init))) # [M,N]\n",
    "E_1_init = Y_init - H_beta_1_init # [M,N]\n",
    "\n",
    "'''2nd subnet'''\n",
    "#alpha_2_init_calculated = t(mul(h_(E_1_init), X_inv_init)) # [D,M]    \n",
    "alpha_2_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_2_init = tf.assign(alpha_2, alpha_2_init_calculated) # [D,M]\n",
    "H_2_init = h(mul(t(alpha_2_init), X_init)) # [M,N]\n",
    "H_2_inv_init = pseudo_inv(H_2_init, I_MxM, C) # [M,N]\n",
    "H_add = H_I + mul(H_2_init,t(H_2_init))\n",
    "#m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_2_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_2_init_calculated = mul(E_1_init, t(H_2_init))/sqr(tf.norm(H_2_init)) # [M,M]\n",
    "beta_2_init_calculated = mul(E_1_init, H_pseudo_init)\n",
    "\n",
    "beta_2_init = tf.assign(beta_2, beta_2_init_calculated) # [M,M]\n",
    "H_beta_2_init = mul(beta_2_init, t(mul(t(X_init), alpha_2_init))) # [M,N]\n",
    "E_2_init = Y_init - (H_beta_1_init+H_beta_2_init) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "alpha_3_init_calculated = t(mul(h_(E_2_init), X_inv_init)) # [D,M]    \n",
    "alpha_3_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_3_init = tf.assign(alpha_3, alpha_3_init_calculated) # [D,M]\n",
    "H_3_init = h(mul(t(alpha_3_init), X_init)) # [M,N]\n",
    "H_3_inv_init = pseudo_inv(H_3_init, I_MxM, C) # [M,N]\n",
    "\n",
    "beta_3_init_calculated = mul(E_2_init, t(H_3_init))/sqr(tf.norm(H_3_init)) # [M,M]\n",
    "beta_3_init_calculated = mul(E_2_init, H_3_inv_init)\n",
    "\n",
    "beta_3_init = tf.assign(beta_3, beta_3_init_calculated) # [M,M]\n",
    "H_beta_3_init = mul(beta_3_init, t(mul(t(X_init), alpha_3_init))) # [M,N]\n",
    "E_3_init = Y_init - (H_beta_3_init+H_beta_2_init+ H_beta_1_init) # [M,N]\n",
    "\n",
    "#init_train_graph = H_beta_1_init\n",
    "init_train_graph = E_3_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_3:0' shape=(2, 2) dtype=float32_ref>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_1_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''With one subnetwork'''\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.47783542\n",
      "Initial train training accuracy:  0.49339935\n",
      "Initial train testing loss:  0.47645548\n",
      "Initial train testing accuracy:  0.5066007\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(E_1_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.46714404\n",
      "Initial train training accuracy:  0.49339935\n",
      "Initial train testing loss:  0.46499318\n",
      "Initial train testing accuracy:  0.5066007\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(E_2_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.46178526\n",
      "Initial train training accuracy:  0.49339935\n",
      "Initial train testing loss:  0.4591867\n",
      "Initial train testing accuracy:  0.5066007\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(init_train_graph, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-288609ea9fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"visualize subnet nodes\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mvisualize_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mvisualize_beta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mvisualize_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-288609ea9fc8>\u001b[0m in \u001b[0;36mvisualize_alpha\u001b[0;34m(alpha, size)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_beta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f809c072c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_alpha(alpha, size):\n",
    "    tmp = sess.run(alpha)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            plt.subplot(2,5,i*5+j+1)\n",
    "            plt.imshow(np.reshape(tmp[:,i*5+j], [size,size]))\n",
    "\n",
    "def visualize_beta(beta):\n",
    "    tmp = sess.run(beta)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(tmp)\n",
    "    \n",
    "            \n",
    "\"\"\"visualize subnet nodes\"\"\"            \n",
    "visualize_alpha(alpha_1, 10)\n",
    "visualize_beta(beta_1)\n",
    "visualize_alpha(alpha_2, 10)\n",
    "visualize_beta(beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_init = subnet_output(alpha_1, beta_1, X_)+ subnet_output(alpha_2, beta_2, X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "logic = sess.run(logits_init, feed_dict={X_ : [x_test[4]]})\n",
    "print(np.argmax(logic,axis =1))\n",
    "print(y_test[4])\n",
    "#plt.imshow(x_test[4200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4000 is out of bounds for axis 0 with size 606",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-00a895900ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#plt.imshow(x_test[4000])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4000 is out of bounds for axis 0 with size 606"
     ]
    }
   ],
   "source": [
    "logic = sess.run(logits_init, feed_dict={X_ : [x_test[4000]]})\n",
    "print(np.argmax(logic,axis =1))\n",
    "print(y_test[4000])\n",
    "#plt.imshow(x_test[4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sequential Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "X_seq = t(X_) # [D,N]\n",
    "Y_seq = t(Y_) # [M,N]\n",
    "pseudo = mul(X_seq, X_) #DXD\n",
    "k = tf.assign(k, tf.add(k,pseudo)) #DXD\n",
    "k_inv = inv(k)\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_1))\n",
    "alpha1_seq = tf.assign(alpha_1,tf.add(alpha_1,new)) #DXM\n",
    "H_1_seq = h(mul(t(alpha1_seq), X_seq)) # [M,N]\n",
    "m_su = mul(H_1_seq,t(H_1_seq))\n",
    "m = tf.assign(m,tf.add(m,m_su))\n",
    "m_inv = inv(m)\n",
    "#update = tf.matmul(tf.matmul(m_inv,H_1_seq),h_(Y_seq)- tf.matmul())\n",
    "H_pseudo_init = pseudo_inv(H_1_seq,I_MxM,C) #[N,M]\n",
    "#UPDATE = tf.matmul(tf.matmul(K_inverse, HT), inverse_acti_y - tf.matmul(H, self.__outputWeight))\n",
    "beta_1_seq_calculated = mul(Y_seq, H_pseudo_init) # [M,M]\n",
    "beta_1_seq = tf.assign(beta_1, beta_1_seq_calculated) # [M,M]\n",
    "H_beta_1_seq = mul(beta_1_seq, t(mul(X_, alpha1_seq))) # [M,N]\n",
    "E_1_seq = Y_seq - H_beta_1_seq # [M,N]\n",
    "\n",
    "'''2nd subnetwork'''\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_2))\n",
    "alpha2_seq = tf.assign(alpha_2,tf.add(alpha_2,new)) #DXM\n",
    "H_2_seq = h(mul(t(alpha2_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_2_seq,I_MxM,C) #[N,M]\n",
    "beta_2_seq_calculated = mul(E_1_seq, H_pseudo_init) # [M,M]\n",
    "beta_2_seq = tf.assign(beta_2, beta_2_seq_calculated) # [M,M]\n",
    "H_beta_2_seq = mul(beta_2_seq, t(mul(t(X_seq), alpha2_seq))) # [M,N]\n",
    "E_2_seq = Y_seq - (H_beta_2_seq+ H_beta_1_seq) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_3))\n",
    "alpha3_seq = tf.assign(alpha_3,tf.add(alpha_3,new)) #DXM\n",
    "H_3_seq = h(mul(t(alpha3_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_3_seq,I_MxM,C) #[N,M]\n",
    "beta_3_seq_calculated = mul(E_2_seq, H_pseudo_init) # [M,M]\n",
    "beta_3_seq = tf.assign(beta_3, beta_3_seq_calculated) # [M,M]\n",
    "H_beta_3_seq = mul(beta_3_seq, t(mul(t(X_seq), alpha3_seq))) # [M,N]\n",
    "E_3_seq = Y_seq - (H_beta_3_seq +H_beta_2_seq + H_beta_1_seq )# [M,N]\n",
    "seq_train_graph = E_3_seq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.454951, train_accuracy: 0.493399\n",
      "test_loss: 0.451328, test_accuracy: 0.506601\n",
      "train_loss: 0.454537, train_accuracy: 0.493399\n",
      "test_loss: 0.450925, test_accuracy: 0.506601\n",
      "train_loss: 0.454345, train_accuracy: 0.493399\n",
      "test_loss: 0.450750, test_accuracy: 0.506601\n",
      "train_loss: 0.454200, train_accuracy: 0.493399\n",
      "test_loss: 0.450627, test_accuracy: 0.506601\n",
      "train_loss: 0.454059, train_accuracy: 0.493399\n",
      "test_loss: 0.450510, test_accuracy: 0.506601\n",
      "train_loss: 0.453910, train_accuracy: 0.493399\n",
      "test_loss: 0.450387, test_accuracy: 0.506601\n",
      "train_loss: 0.453753, train_accuracy: 0.493399\n",
      "test_loss: 0.450258, test_accuracy: 0.506601\n",
      "train_loss: 0.453587, train_accuracy: 0.493399\n",
      "test_loss: 0.450121, test_accuracy: 0.506601\n",
      "train_loss: 0.453413, train_accuracy: 0.493399\n",
      "test_loss: 0.449978, test_accuracy: 0.506601\n",
      "train_loss: 0.453230, train_accuracy: 0.493399\n",
      "test_loss: 0.449828, test_accuracy: 0.506601\n",
      "train_loss: 0.453040, train_accuracy: 0.493399\n",
      "test_loss: 0.449672, test_accuracy: 0.506601\n",
      "train_loss: 0.452843, train_accuracy: 0.493399\n",
      "test_loss: 0.449511, test_accuracy: 0.506601\n",
      "train_loss: 0.452640, train_accuracy: 0.493399\n",
      "test_loss: 0.449345, test_accuracy: 0.506601\n",
      "train_loss: 0.452431, train_accuracy: 0.493399\n",
      "test_loss: 0.449175, test_accuracy: 0.506601\n",
      "train_loss: 0.452217, train_accuracy: 0.491749\n",
      "test_loss: 0.449001, test_accuracy: 0.508251\n",
      "train_loss: 0.451999, train_accuracy: 0.491749\n",
      "test_loss: 0.448824, test_accuracy: 0.508251\n",
      "train_loss: 0.451776, train_accuracy: 0.491749\n",
      "test_loss: 0.448644, test_accuracy: 0.508251\n",
      "train_loss: 0.451550, train_accuracy: 0.491749\n",
      "test_loss: 0.448461, test_accuracy: 0.508251\n",
      "train_loss: 0.451321, train_accuracy: 0.491749\n",
      "test_loss: 0.448277, test_accuracy: 0.508251\n",
      "train_loss: 0.451089, train_accuracy: 0.491749\n",
      "test_loss: 0.448090, test_accuracy: 0.508251\n",
      "train_loss: 0.450854, train_accuracy: 0.493399\n",
      "test_loss: 0.447902, test_accuracy: 0.508251\n",
      "train_loss: 0.450618, train_accuracy: 0.493399\n",
      "test_loss: 0.447712, test_accuracy: 0.508251\n",
      "train_loss: 0.450380, train_accuracy: 0.493399\n",
      "test_loss: 0.447522, test_accuracy: 0.508251\n",
      "train_loss: 0.450140, train_accuracy: 0.496700\n",
      "test_loss: 0.447331, test_accuracy: 0.516502\n",
      "train_loss: 0.449899, train_accuracy: 0.500000\n",
      "test_loss: 0.447140, test_accuracy: 0.521452\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(25):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(seq_train_graph, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\"\n",
    "#tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_train, Y: y_train})\n",
    "#ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_test, Y: y_test})\n",
    "#print(\"Sequential train training loss: \", tr_loss)\n",
    "#print(\"Sequential train training accuracy: \", tr_acc)\n",
    "#print(\"Sequential train testing loss: \", ts_loss)\n",
    "#print(\"Sequential train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.425384, train_accuracy: 0.933993\n",
      "test_loss: 0.437825, test_accuracy: 0.912541\n",
      "train_loss: 0.425368, train_accuracy: 0.933993\n",
      "test_loss: 0.437823, test_accuracy: 0.912541\n",
      "train_loss: 0.425352, train_accuracy: 0.933993\n",
      "test_loss: 0.437821, test_accuracy: 0.914191\n",
      "train_loss: 0.425336, train_accuracy: 0.933993\n",
      "test_loss: 0.437820, test_accuracy: 0.914191\n",
      "train_loss: 0.425320, train_accuracy: 0.933993\n",
      "test_loss: 0.437818, test_accuracy: 0.914191\n",
      "train_loss: 0.425304, train_accuracy: 0.933993\n",
      "test_loss: 0.437816, test_accuracy: 0.914191\n",
      "train_loss: 0.425288, train_accuracy: 0.933993\n",
      "test_loss: 0.437815, test_accuracy: 0.914191\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(7):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_2_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.449823, train_accuracy: 0.496700\n",
      "test_loss: 0.447789, test_accuracy: 0.504951\n",
      "train_loss: 0.453945, train_accuracy: 0.495050\n",
      "test_loss: 0.451430, test_accuracy: 0.506601\n",
      "train_loss: 0.459627, train_accuracy: 0.495050\n",
      "test_loss: 0.457234, test_accuracy: 0.504951\n",
      "train_loss: 0.466304, train_accuracy: 0.493399\n",
      "test_loss: 0.464544, test_accuracy: 0.501650\n",
      "train_loss: 0.474462, train_accuracy: 0.490099\n",
      "test_loss: 0.473569, test_accuracy: 0.503300\n",
      "train_loss: 0.484234, train_accuracy: 0.495050\n",
      "test_loss: 0.484100, test_accuracy: 0.504951\n",
      "train_loss: 0.493714, train_accuracy: 0.575908\n",
      "test_loss: 0.493900, test_accuracy: 0.566007\n",
      "train_loss: 0.499252, train_accuracy: 0.612211\n",
      "test_loss: 0.499230, test_accuracy: 0.618812\n",
      "train_loss: 0.499102, train_accuracy: 0.640264\n",
      "test_loss: 0.498565, test_accuracy: 0.636964\n",
      "train_loss: 0.495006, train_accuracy: 0.580858\n",
      "test_loss: 0.493918, test_accuracy: 0.580858\n",
      "train_loss: 0.489612, train_accuracy: 0.546205\n",
      "test_loss: 0.488062, test_accuracy: 0.569307\n",
      "train_loss: 0.484491, train_accuracy: 0.516502\n",
      "test_loss: 0.482590, test_accuracy: 0.559406\n",
      "train_loss: 0.480166, train_accuracy: 0.509901\n",
      "test_loss: 0.478007, test_accuracy: 0.539604\n",
      "train_loss: 0.476659, train_accuracy: 0.504951\n",
      "test_loss: 0.474312, test_accuracy: 0.531353\n",
      "train_loss: 0.473839, train_accuracy: 0.506601\n",
      "test_loss: 0.471355, test_accuracy: 0.526403\n",
      "train_loss: 0.471557, train_accuracy: 0.504951\n",
      "test_loss: 0.468973, test_accuracy: 0.518152\n",
      "train_loss: 0.469692, train_accuracy: 0.504951\n",
      "test_loss: 0.467035, test_accuracy: 0.518152\n",
      "train_loss: 0.468148, train_accuracy: 0.501650\n",
      "test_loss: 0.465437, test_accuracy: 0.518152\n",
      "train_loss: 0.466855, train_accuracy: 0.501650\n",
      "test_loss: 0.464106, test_accuracy: 0.518152\n",
      "train_loss: 0.465760, train_accuracy: 0.500000\n",
      "test_loss: 0.462983, test_accuracy: 0.514852\n",
      "train_loss: 0.464824, train_accuracy: 0.500000\n",
      "test_loss: 0.462029, test_accuracy: 0.514852\n",
      "train_loss: 0.464018, train_accuracy: 0.498350\n",
      "test_loss: 0.461210, test_accuracy: 0.514852\n",
      "train_loss: 0.463317, train_accuracy: 0.496700\n",
      "test_loss: 0.460501, test_accuracy: 0.513201\n",
      "train_loss: 0.462703, train_accuracy: 0.496700\n",
      "test_loss: 0.459885, test_accuracy: 0.513201\n",
      "train_loss: 0.462163, train_accuracy: 0.496700\n",
      "test_loss: 0.459345, test_accuracy: 0.513201\n",
      "train_loss: 0.461685, train_accuracy: 0.496700\n",
      "test_loss: 0.458869, test_accuracy: 0.513201\n",
      "train_loss: 0.461258, train_accuracy: 0.496700\n",
      "test_loss: 0.458447, test_accuracy: 0.513201\n",
      "train_loss: 0.460875, train_accuracy: 0.498350\n",
      "test_loss: 0.458071, test_accuracy: 0.513201\n",
      "train_loss: 0.460529, train_accuracy: 0.498350\n",
      "test_loss: 0.457733, test_accuracy: 0.513201\n",
      "train_loss: 0.460216, train_accuracy: 0.498350\n",
      "test_loss: 0.457430, test_accuracy: 0.513201\n",
      "train_loss: 0.459930, train_accuracy: 0.496700\n",
      "test_loss: 0.457155, test_accuracy: 0.513201\n",
      "train_loss: 0.459667, train_accuracy: 0.498350\n",
      "test_loss: 0.456905, test_accuracy: 0.513201\n",
      "train_loss: 0.459425, train_accuracy: 0.498350\n",
      "test_loss: 0.456676, test_accuracy: 0.513201\n",
      "train_loss: 0.459201, train_accuracy: 0.496700\n",
      "test_loss: 0.456466, test_accuracy: 0.513201\n",
      "train_loss: 0.458991, train_accuracy: 0.496700\n",
      "test_loss: 0.456271, test_accuracy: 0.516502\n",
      "train_loss: 0.458795, train_accuracy: 0.496700\n",
      "test_loss: 0.456091, test_accuracy: 0.518152\n",
      "train_loss: 0.458610, train_accuracy: 0.496700\n",
      "test_loss: 0.455922, test_accuracy: 0.518152\n",
      "train_loss: 0.458435, train_accuracy: 0.496700\n",
      "test_loss: 0.455764, test_accuracy: 0.518152\n",
      "train_loss: 0.458269, train_accuracy: 0.496700\n",
      "test_loss: 0.455616, test_accuracy: 0.518152\n",
      "train_loss: 0.458110, train_accuracy: 0.496700\n",
      "test_loss: 0.455475, test_accuracy: 0.518152\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(40):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_1_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_ = subnet_output(alpha_1, beta_1, X_) \n",
    "logits__ = sess.run(logits_, feed_dict={X: [x_test[4000]]})\n",
    "print(logits__)\n",
    "print(np.argmax(logits__))\n",
    "print(y_test[4000])\n",
    "plt.imshow(x_test[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alpha(alpha, size):\n",
    "    tmp = sess.run(alpha)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            plt.subplot(2,5,i*5+j+1)\n",
    "            plt.imshow(np.reshape(tmp[:,i*5+j], [size,size]))\n",
    "\n",
    "def visualize_beta(beta):\n",
    "    tmp = sess.run(beta)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(tmp)\n",
    "    \n",
    "            \n",
    "\"\"\"visualize subnet nodes\"\"\"            \n",
    "visualize_alpha(alpha_1, 10)\n",
    "visualize_beta(beta_1)\n",
    "visualize_alpha(alpha_2, 10)\n",
    "visualize_beta(beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.math.sin(tf.constant([0.9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
