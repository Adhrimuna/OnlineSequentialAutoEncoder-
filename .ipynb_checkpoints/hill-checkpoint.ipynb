{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adhrimuna/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606, 100) (606, 100) (606,) (606,) (1212, 100)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import mnist_handler\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"; \n",
    "INPUT_DIMENSION = 100\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "train_hill_data = pd.read_csv(\"Hill_Valley_with_noise_Training.csv\")\n",
    "test_hill_data = pd.read_csv(\"Hill_Valley_with_noise_Testing.csv\")\n",
    "\n",
    "x_train = train_hill_data.iloc[:,0:100]\n",
    "y_train = train_hill_data.iloc[:,100]\n",
    "\n",
    "x_test = test_hill_data.iloc[:,0:100]\n",
    "y_test = test_hill_data.iloc[:,100]\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "data = np.concatenate((x_train,x_test),axis = 0)\n",
    "\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape,data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.23410461e-04 3.26826856e-04 3.13785122e-04 ... 3.31526542e-04\n",
      "  3.22973439e-04 3.33616018e-04]\n",
      " [7.72450182e-06 7.43831843e-06 7.72911145e-06 ... 7.90166888e-06\n",
      "  7.41453098e-06 6.98489438e-06]\n",
      " [5.78716027e-01 6.07348067e-01 6.13128820e-01 ... 6.10342597e-01\n",
      "  5.94957501e-01 6.54129683e-01]\n",
      " ...\n",
      " [8.62404977e-02 8.30862924e-02 7.55612217e-02 ... 8.90258702e-02\n",
      "  7.94121866e-02 8.12420050e-02]\n",
      " [2.87674029e-04 3.13786841e-04 2.86733232e-04 ... 2.71662812e-04\n",
      "  2.81752177e-04 2.96246833e-04]\n",
      " [7.20544077e-02 7.04535483e-02 7.67138675e-02 ... 7.45382462e-02\n",
      "  7.40429187e-02 7.60647140e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = data[0:606,:]\n",
    "x_test = data[606:,:]\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "border = 100\n",
    "x_train_init = x_train[:border]\n",
    "y_train_init = y_train[:border]\n",
    "x_train_seq = x_train[border:]\n",
    "y_train_seq = y_train[border:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # clear all the tensors\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "\"\"\"Placeholders\"\"\"\n",
    "X_ = tf.placeholder(tf.float32, [None, INPUT_DIMENSION])\n",
    "#X_ = tf.reshape(X, [-1, INPUT_DIMENSION]) # Flatten X: [N,D]\n",
    "Y = tf.placeholder(tf.int64, [None]) # labels\n",
    "Y_ = tf.one_hot(indices=Y, depth=NUM_CLASSES) # one_hot labels: [N,M]\n",
    "\n",
    "\"\"\"Some constants\"\"\"\n",
    "D = INPUT_DIMENSION\n",
    "M = NUM_CLASSES # Number of outputs\n",
    "C = tf.constant(2.0**(-5))\n",
    "\n",
    "\"\"\"Weights\"\"\"\n",
    "alpha_1 = tf.get_variable('alpha_1',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 1st subnetwork\n",
    "alpha_2 = tf.get_variable('alpha_2',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 2st subnetwork\n",
    "alpha_3 = tf.get_variable('alpha_3',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_1 = tf.get_variable('beta_1',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_2 = tf.get_variable('beta_2',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_3 = tf.get_variable('beta_3',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.get_variable('k',shape=[D, D],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "m = tf.get_variable('m',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions\"\"\"\n",
    "def mul(A, B):\n",
    "    return tf.matmul(A, B)\n",
    "\n",
    "def inv(A):\n",
    "    return tf.matrix_inverse(A)\n",
    "\n",
    "def t(A):\n",
    "    return tf.transpose(A)\n",
    "\n",
    "def sin(A):\n",
    "    return tf.math.sin(A)\n",
    "\n",
    "def asin(A):\n",
    "    return tf.math.asin(A)\n",
    "\n",
    "def sqrt(A):\n",
    "    return tf.sqrt(A)\n",
    "\n",
    "def sqr(A):\n",
    "    return tf.math.pow(A, 2)\n",
    "\n",
    "def pseudo_inv(A, I, C):\n",
    "    C_I = I/C\n",
    "    return mul(t(A), inv(C_I + mul(A, t(A))))\n",
    "\n",
    "def h(A):\n",
    "    '''activation function'''\n",
    "    return sin(A)\n",
    "\n",
    "def h_(A):\n",
    "    '''inverse activation function'''\n",
    "    return asin(A)\n",
    "\n",
    "def u(A):\n",
    "    '''normalize the input to (0,1]'''\n",
    "    return tf.math.sigmoid(A) # sigmoid\n",
    "    \n",
    "def u_(A):\n",
    "    '''the inverse of u'''\n",
    "    ONE = tf.constant(1.0)\n",
    "    return -(tf.math.log(ONE/A - ONE)) # the inverse of sigmoid\n",
    "    \n",
    "def subnet_output(alpha, beta, A):\n",
    "    return t(mul(beta, h(mul(t(alpha), t(A))))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "'''some pre-computations'''\n",
    "X_init = t(X_) # [D,N]\n",
    "Y_init = t(Y_) # [M,N]\n",
    "N_init = D # number of dimensions\n",
    "I_DxD = tf.eye(N_init, dtype=tf.float32) # [D,D]\n",
    "I_MxM = tf.eye(M, dtype=tf.float32) # [M,M]\n",
    "C_I = I_DxD/C\n",
    "H_I = I_MxM/C\n",
    "\n",
    "add = C_I + mul(X_init, t(X_init))\n",
    "k = tf.assign(k,add)\n",
    "X_inv_init = pseudo_inv(X_init, I_DxD, C) # [N,D]\n",
    "\n",
    "'''1st subnet'''\n",
    "alpha_1_init_calculated = t(mul(h_(Y_init), X_inv_init)) # ([M,N]x[N,D])T=[D,M]\n",
    "alpha_1_init = tf.assign(alpha_1, alpha_1_init_calculated) # [D,M]\n",
    "H_1_init = h(mul(t(alpha_1_init), X_init)) # [M,N]\n",
    "H_add = H_I + mul(H_1_init,t(H_1_init))\n",
    "m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_1_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_1_init_calculated = mul(Y_init, t(H_1_init))/sqr(tf.norm(H_1_init)) # [M,M]\n",
    "beta_1_init_calculated = mul(Y_init,H_pseudo_init)\n",
    "\n",
    "beta_1_init = tf.assign(beta_1, beta_1_init_calculated) # [M,M]\n",
    "H_beta_1_init = mul(beta_1_init, t(mul(t(X_init), alpha_1_init))) # [M,N]\n",
    "E_1_init = Y_init - H_beta_1_init # [M,N]\n",
    "\n",
    "'''2nd subnet'''\n",
    "#alpha_2_init_calculated = t(mul(h_(E_1_init), X_inv_init)) # [D,M]    \n",
    "alpha_2_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_2_init = tf.assign(alpha_2, alpha_2_init_calculated) # [D,M]\n",
    "H_2_init = h(mul(t(alpha_2_init), X_init)) # [M,N]\n",
    "H_2_inv_init = pseudo_inv(H_2_init, I_MxM, C) # [M,N]\n",
    "H_add = H_I + mul(H_2_init,t(H_2_init))\n",
    "#m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_2_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_2_init_calculated = mul(E_1_init, t(H_2_init))/sqr(tf.norm(H_2_init)) # [M,M]\n",
    "beta_2_init_calculated = mul(E_1_init, H_pseudo_init)\n",
    "\n",
    "beta_2_init = tf.assign(beta_2, beta_2_init_calculated) # [M,M]\n",
    "H_beta_2_init = mul(beta_2_init, t(mul(t(X_init), alpha_2_init))) # [M,N]\n",
    "E_2_init = Y_init - (H_beta_1_init+H_beta_2_init) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "alpha_3_init_calculated = t(mul(h_(E_2_init), X_inv_init)) # [D,M]    \n",
    "alpha_3_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_3_init = tf.assign(alpha_3, alpha_3_init_calculated) # [D,M]\n",
    "H_3_init = h(mul(t(alpha_3_init), X_init)) # [M,N]\n",
    "H_3_inv_init = pseudo_inv(H_3_init, I_MxM, C) # [M,N]\n",
    "\n",
    "beta_3_init_calculated = mul(E_2_init, t(H_3_init))/sqr(tf.norm(H_3_init)) # [M,M]\n",
    "beta_3_init_calculated = mul(E_2_init, H_3_inv_init)\n",
    "\n",
    "beta_3_init = tf.assign(beta_3, beta_3_init_calculated) # [M,M]\n",
    "H_beta_3_init = mul(beta_3_init, t(mul(t(X_init), alpha_3_init))) # [M,N]\n",
    "E_3_init = Y_init - (H_beta_3_init+H_beta_2_init+ H_beta_1_init) # [M,N]\n",
    "\n",
    "#init_train_graph = H_beta_1_init\n",
    "init_train_graph = E_3_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_3:0' shape=(2, 2) dtype=float32_ref>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_1_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''With one subnetwork'''\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.4788519\n",
      "Initial train training accuracy:  0.49339935\n",
      "Initial train testing loss:  0.47748458\n",
      "Initial train testing accuracy:  0.5066007\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(E_1_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.46830493\n",
      "Initial train training accuracy:  0.49339935\n",
      "Initial train testing loss:  0.4661495\n",
      "Initial train testing accuracy:  0.5066007\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(E_2_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(init_train_graph, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alpha(alpha, size):\n",
    "    tmp = sess.run(alpha)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            plt.subplot(2,5,i*5+j+1)\n",
    "            plt.imshow(np.reshape(tmp[:,i*5+j], [size,size]))\n",
    "\n",
    "def visualize_beta(beta):\n",
    "    tmp = sess.run(beta)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(tmp)\n",
    "    \n",
    "            \n",
    "\"\"\"visualize subnet nodes\"\"\"            \n",
    "visualize_alpha(alpha_1, 10)\n",
    "visualize_beta(beta_1)\n",
    "visualize_alpha(alpha_2, 10)\n",
    "visualize_beta(beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_init = subnet_output(alpha_1, beta_1, X_)+ subnet_output(alpha_2, beta_2, X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic = sess.run(logits_init, feed_dict={X_ : [x_test[4]]})\n",
    "print(np.argmax(logic,axis =1))\n",
    "print(y_test[4])\n",
    "#plt.imshow(x_test[4200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic = sess.run(logits_init, feed_dict={X_ : [x_test[4000]]})\n",
    "print(np.argmax(logic,axis =1))\n",
    "print(y_test[4000])\n",
    "#plt.imshow(x_test[4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sequential Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "X_seq = t(X_) # [D,N]\n",
    "Y_seq = t(Y_) # [M,N]\n",
    "pseudo = mul(X_seq, X_) #DXD\n",
    "k = tf.assign(k, tf.add(k,pseudo)) #DXD\n",
    "k_inv = inv(k)\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_1))\n",
    "alpha1_seq = tf.assign(alpha_1,tf.add(alpha_1,new)) #DXM\n",
    "H_1_seq = h(mul(t(alpha1_seq), X_seq)) # [M,N]\n",
    "m_su = mul(H_1_seq,t(H_1_seq))\n",
    "m = tf.assign(m,tf.add(m,m_su))\n",
    "m_inv = inv(m)\n",
    "#update = tf.matmul(tf.matmul(m_inv,H_1_seq),h_(Y_seq)- tf.matmul())\n",
    "H_pseudo_init = pseudo_inv(H_1_seq,I_MxM,C) #[N,M]\n",
    "#UPDATE = tf.matmul(tf.matmul(K_inverse, HT), inverse_acti_y - tf.matmul(H, self.__outputWeight))\n",
    "beta_1_seq_calculated = mul(Y_seq, H_pseudo_init) # [M,M]\n",
    "beta_1_seq = tf.assign(beta_1, beta_1_seq_calculated) # [M,M]\n",
    "H_beta_1_seq = mul(beta_1_seq, t(mul(X_, alpha1_seq))) # [M,N]\n",
    "E_1_seq = Y_seq - H_beta_1_seq # [M,N]\n",
    "\n",
    "'''2nd subnetwork'''\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_2))\n",
    "alpha2_seq = tf.assign(alpha_2,tf.add(alpha_2,new)) #DXM\n",
    "H_2_seq = h(mul(t(alpha2_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_2_seq,I_MxM,C) #[N,M]\n",
    "beta_2_seq_calculated = mul(E_1_seq, H_pseudo_init) # [M,M]\n",
    "beta_2_seq = tf.assign(beta_2, beta_2_seq_calculated) # [M,M]\n",
    "H_beta_2_seq = mul(beta_2_seq, t(mul(t(X_seq), alpha2_seq))) # [M,N]\n",
    "E_2_seq = Y_seq - (H_beta_2_seq+ H_beta_1_seq) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_3))\n",
    "alpha3_seq = tf.assign(alpha_3,tf.add(alpha_3,new)) #DXM\n",
    "H_3_seq = h(mul(t(alpha3_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_3_seq,I_MxM,C) #[N,M]\n",
    "beta_3_seq_calculated = mul(E_2_seq, H_pseudo_init) # [M,M]\n",
    "beta_3_seq = tf.assign(beta_3, beta_3_seq_calculated) # [M,M]\n",
    "H_beta_3_seq = mul(beta_3_seq, t(mul(t(X_seq), alpha3_seq))) # [M,N]\n",
    "E_3_seq = Y_seq - (H_beta_3_seq +H_beta_2_seq + H_beta_1_seq )# [M,N]\n",
    "seq_train_graph = E_3_seq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.420385, train_accuracy: 0.935644\n",
      "test_loss: 0.434714, test_accuracy: 0.909241\n",
      "train_loss: 0.420369, train_accuracy: 0.935644\n",
      "test_loss: 0.434715, test_accuracy: 0.909241\n",
      "train_loss: 0.420354, train_accuracy: 0.935644\n",
      "test_loss: 0.434716, test_accuracy: 0.909241\n",
      "train_loss: 0.420338, train_accuracy: 0.935644\n",
      "test_loss: 0.434716, test_accuracy: 0.909241\n",
      "train_loss: 0.420321, train_accuracy: 0.935644\n",
      "test_loss: 0.434717, test_accuracy: 0.909241\n",
      "train_loss: 0.420305, train_accuracy: 0.935644\n",
      "test_loss: 0.434717, test_accuracy: 0.909241\n",
      "train_loss: 0.420289, train_accuracy: 0.935644\n",
      "test_loss: 0.434717, test_accuracy: 0.910891\n",
      "train_loss: 0.420273, train_accuracy: 0.935644\n",
      "test_loss: 0.434718, test_accuracy: 0.910891\n",
      "train_loss: 0.420257, train_accuracy: 0.935644\n",
      "test_loss: 0.434718, test_accuracy: 0.910891\n",
      "train_loss: 0.420241, train_accuracy: 0.935644\n",
      "test_loss: 0.434718, test_accuracy: 0.910891\n",
      "train_loss: 0.420225, train_accuracy: 0.935644\n",
      "test_loss: 0.434718, test_accuracy: 0.910891\n",
      "train_loss: 0.420208, train_accuracy: 0.935644\n",
      "test_loss: 0.434717, test_accuracy: 0.910891\n",
      "train_loss: 0.420192, train_accuracy: 0.935644\n",
      "test_loss: 0.434717, test_accuracy: 0.910891\n",
      "train_loss: 0.420176, train_accuracy: 0.935644\n",
      "test_loss: 0.434717, test_accuracy: 0.910891\n",
      "train_loss: 0.420160, train_accuracy: 0.935644\n",
      "test_loss: 0.434717, test_accuracy: 0.910891\n",
      "train_loss: 0.420144, train_accuracy: 0.935644\n",
      "test_loss: 0.434716, test_accuracy: 0.910891\n",
      "train_loss: 0.420128, train_accuracy: 0.935644\n",
      "test_loss: 0.434716, test_accuracy: 0.910891\n",
      "train_loss: 0.420112, train_accuracy: 0.935644\n",
      "test_loss: 0.434716, test_accuracy: 0.910891\n",
      "train_loss: 0.420096, train_accuracy: 0.935644\n",
      "test_loss: 0.434715, test_accuracy: 0.910891\n",
      "train_loss: 0.420079, train_accuracy: 0.935644\n",
      "test_loss: 0.434715, test_accuracy: 0.910891\n",
      "train_loss: 0.420064, train_accuracy: 0.935644\n",
      "test_loss: 0.434714, test_accuracy: 0.910891\n",
      "train_loss: 0.420048, train_accuracy: 0.935644\n",
      "test_loss: 0.434714, test_accuracy: 0.910891\n",
      "train_loss: 0.420032, train_accuracy: 0.935644\n",
      "test_loss: 0.434713, test_accuracy: 0.910891\n",
      "train_loss: 0.420016, train_accuracy: 0.935644\n",
      "test_loss: 0.434713, test_accuracy: 0.910891\n",
      "train_loss: 0.420000, train_accuracy: 0.933993\n",
      "test_loss: 0.434712, test_accuracy: 0.910891\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(25):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(seq_train_graph, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\"\n",
    "#tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_train, Y: y_train})\n",
    "#ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_test, Y: y_test})\n",
    "#print(\"Sequential train training loss: \", tr_loss)\n",
    "#print(\"Sequential train training accuracy: \", tr_acc)\n",
    "#print(\"Sequential train testing loss: \", ts_loss)\n",
    "#print(\"Sequential train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.425384, train_accuracy: 0.933993\n",
      "test_loss: 0.437825, test_accuracy: 0.912541\n",
      "train_loss: 0.425368, train_accuracy: 0.933993\n",
      "test_loss: 0.437823, test_accuracy: 0.912541\n",
      "train_loss: 0.425352, train_accuracy: 0.933993\n",
      "test_loss: 0.437821, test_accuracy: 0.914191\n",
      "train_loss: 0.425336, train_accuracy: 0.933993\n",
      "test_loss: 0.437820, test_accuracy: 0.914191\n",
      "train_loss: 0.425320, train_accuracy: 0.933993\n",
      "test_loss: 0.437818, test_accuracy: 0.914191\n",
      "train_loss: 0.425304, train_accuracy: 0.933993\n",
      "test_loss: 0.437816, test_accuracy: 0.914191\n",
      "train_loss: 0.425288, train_accuracy: 0.933993\n",
      "test_loss: 0.437815, test_accuracy: 0.914191\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(7):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_2_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.449823, train_accuracy: 0.496700\n",
      "test_loss: 0.447789, test_accuracy: 0.504951\n",
      "train_loss: 0.453945, train_accuracy: 0.495050\n",
      "test_loss: 0.451430, test_accuracy: 0.506601\n",
      "train_loss: 0.459627, train_accuracy: 0.495050\n",
      "test_loss: 0.457234, test_accuracy: 0.504951\n",
      "train_loss: 0.466304, train_accuracy: 0.493399\n",
      "test_loss: 0.464544, test_accuracy: 0.501650\n",
      "train_loss: 0.474462, train_accuracy: 0.490099\n",
      "test_loss: 0.473569, test_accuracy: 0.503300\n",
      "train_loss: 0.484234, train_accuracy: 0.495050\n",
      "test_loss: 0.484100, test_accuracy: 0.504951\n",
      "train_loss: 0.493714, train_accuracy: 0.575908\n",
      "test_loss: 0.493900, test_accuracy: 0.566007\n",
      "train_loss: 0.499252, train_accuracy: 0.612211\n",
      "test_loss: 0.499230, test_accuracy: 0.618812\n",
      "train_loss: 0.499102, train_accuracy: 0.640264\n",
      "test_loss: 0.498565, test_accuracy: 0.636964\n",
      "train_loss: 0.495006, train_accuracy: 0.580858\n",
      "test_loss: 0.493918, test_accuracy: 0.580858\n",
      "train_loss: 0.489612, train_accuracy: 0.546205\n",
      "test_loss: 0.488062, test_accuracy: 0.569307\n",
      "train_loss: 0.484491, train_accuracy: 0.516502\n",
      "test_loss: 0.482590, test_accuracy: 0.559406\n",
      "train_loss: 0.480166, train_accuracy: 0.509901\n",
      "test_loss: 0.478007, test_accuracy: 0.539604\n",
      "train_loss: 0.476659, train_accuracy: 0.504951\n",
      "test_loss: 0.474312, test_accuracy: 0.531353\n",
      "train_loss: 0.473839, train_accuracy: 0.506601\n",
      "test_loss: 0.471355, test_accuracy: 0.526403\n",
      "train_loss: 0.471557, train_accuracy: 0.504951\n",
      "test_loss: 0.468973, test_accuracy: 0.518152\n",
      "train_loss: 0.469692, train_accuracy: 0.504951\n",
      "test_loss: 0.467035, test_accuracy: 0.518152\n",
      "train_loss: 0.468148, train_accuracy: 0.501650\n",
      "test_loss: 0.465437, test_accuracy: 0.518152\n",
      "train_loss: 0.466855, train_accuracy: 0.501650\n",
      "test_loss: 0.464106, test_accuracy: 0.518152\n",
      "train_loss: 0.465760, train_accuracy: 0.500000\n",
      "test_loss: 0.462983, test_accuracy: 0.514852\n",
      "train_loss: 0.464824, train_accuracy: 0.500000\n",
      "test_loss: 0.462029, test_accuracy: 0.514852\n",
      "train_loss: 0.464018, train_accuracy: 0.498350\n",
      "test_loss: 0.461210, test_accuracy: 0.514852\n",
      "train_loss: 0.463317, train_accuracy: 0.496700\n",
      "test_loss: 0.460501, test_accuracy: 0.513201\n",
      "train_loss: 0.462703, train_accuracy: 0.496700\n",
      "test_loss: 0.459885, test_accuracy: 0.513201\n",
      "train_loss: 0.462163, train_accuracy: 0.496700\n",
      "test_loss: 0.459345, test_accuracy: 0.513201\n",
      "train_loss: 0.461685, train_accuracy: 0.496700\n",
      "test_loss: 0.458869, test_accuracy: 0.513201\n",
      "train_loss: 0.461258, train_accuracy: 0.496700\n",
      "test_loss: 0.458447, test_accuracy: 0.513201\n",
      "train_loss: 0.460875, train_accuracy: 0.498350\n",
      "test_loss: 0.458071, test_accuracy: 0.513201\n",
      "train_loss: 0.460529, train_accuracy: 0.498350\n",
      "test_loss: 0.457733, test_accuracy: 0.513201\n",
      "train_loss: 0.460216, train_accuracy: 0.498350\n",
      "test_loss: 0.457430, test_accuracy: 0.513201\n",
      "train_loss: 0.459930, train_accuracy: 0.496700\n",
      "test_loss: 0.457155, test_accuracy: 0.513201\n",
      "train_loss: 0.459667, train_accuracy: 0.498350\n",
      "test_loss: 0.456905, test_accuracy: 0.513201\n",
      "train_loss: 0.459425, train_accuracy: 0.498350\n",
      "test_loss: 0.456676, test_accuracy: 0.513201\n",
      "train_loss: 0.459201, train_accuracy: 0.496700\n",
      "test_loss: 0.456466, test_accuracy: 0.513201\n",
      "train_loss: 0.458991, train_accuracy: 0.496700\n",
      "test_loss: 0.456271, test_accuracy: 0.516502\n",
      "train_loss: 0.458795, train_accuracy: 0.496700\n",
      "test_loss: 0.456091, test_accuracy: 0.518152\n",
      "train_loss: 0.458610, train_accuracy: 0.496700\n",
      "test_loss: 0.455922, test_accuracy: 0.518152\n",
      "train_loss: 0.458435, train_accuracy: 0.496700\n",
      "test_loss: 0.455764, test_accuracy: 0.518152\n",
      "train_loss: 0.458269, train_accuracy: 0.496700\n",
      "test_loss: 0.455616, test_accuracy: 0.518152\n",
      "train_loss: 0.458110, train_accuracy: 0.496700\n",
      "test_loss: 0.455475, test_accuracy: 0.518152\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(40):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_1_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_ = subnet_output(alpha_1, beta_1, X_) \n",
    "logits__ = sess.run(logits_, feed_dict={X: [x_test[4000]]})\n",
    "print(logits__)\n",
    "print(np.argmax(logits__))\n",
    "print(y_test[4000])\n",
    "plt.imshow(x_test[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alpha(alpha, size):\n",
    "    tmp = sess.run(alpha)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            plt.subplot(2,5,i*5+j+1)\n",
    "            plt.imshow(np.reshape(tmp[:,i*5+j], [size,size]))\n",
    "\n",
    "def visualize_beta(beta):\n",
    "    tmp = sess.run(beta)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(tmp)\n",
    "    \n",
    "            \n",
    "\"\"\"visualize subnet nodes\"\"\"            \n",
    "visualize_alpha(alpha_1, 10)\n",
    "visualize_beta(beta_1)\n",
    "visualize_alpha(alpha_2, 10)\n",
    "visualize_beta(beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.math.sin(tf.constant([0.9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
