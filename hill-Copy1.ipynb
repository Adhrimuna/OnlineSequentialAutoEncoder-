{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606, 100) (606, 100) (606,) (606,) (1212, 100)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import mnist_handler\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"; \n",
    "INPUT_DIMENSION = 100\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "train_hill_data = pd.read_csv(\"UCI dataset/Hill_Valley_with_noise_Training.csv\")\n",
    "test_hill_data = pd.read_csv(\"UCI dataset/Hill_Valley_with_noise_Testing.csv\")\n",
    "\n",
    "x_train = train_hill_data.iloc[:,0:100]\n",
    "y_train = train_hill_data.iloc[:,100]\n",
    "\n",
    "x_test = test_hill_data.iloc[:,0:100]\n",
    "y_test = test_hill_data.iloc[:,100]\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "data = np.concatenate((x_train,x_test),axis = 0)\n",
    "\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape,data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.23410461e-04 3.26826856e-04 3.13785122e-04 ... 3.31526542e-04\n",
      "  3.22973439e-04 3.33616018e-04]\n",
      " [7.72450182e-06 7.43831843e-06 7.72911145e-06 ... 7.90166888e-06\n",
      "  7.41453098e-06 6.98489438e-06]\n",
      " [5.78716027e-01 6.07348067e-01 6.13128820e-01 ... 6.10342597e-01\n",
      "  5.94957501e-01 6.54129683e-01]\n",
      " ...\n",
      " [8.62404977e-02 8.30862924e-02 7.55612217e-02 ... 8.90258702e-02\n",
      "  7.94121866e-02 8.12420050e-02]\n",
      " [2.87674029e-04 3.13786841e-04 2.86733232e-04 ... 2.71662812e-04\n",
      "  2.81752177e-04 2.96246833e-04]\n",
      " [7.20544077e-02 7.04535483e-02 7.67138675e-02 ... 7.45382462e-02\n",
      "  7.40429187e-02 7.60647140e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = data[0:606,:]\n",
    "x_test = data[606:,:]\n",
    "x_test.shape\n",
    "n_input_nodes = 100\n",
    "n_output_nodes = 100\n",
    "n_hidden_nodes = 2\n",
    "y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy:  0.5280528\n",
      "Testing accuracy:  0.5132013\n",
      "Testing accuracy:  0.5132013\n",
      "Testing accuracy:  0.52640265\n",
      "Testing accuracy:  0.5247525\n",
      "Testing accuracy:  0.519802\n",
      "Testing accuracy:  0.5247525\n",
      "Testing accuracy:  0.50825083\n",
      "Testing accuracy:  0.5247525\n",
      "Testing accuracy:  0.5330033\n",
      "==========================================\n",
      "BP\t ===================================\n",
      "==========================================\n",
      "Average time:  0.8238993644714355\n",
      "Average accuracy:  0.5216171622276307\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "accuracy_sum = 0\n",
    "time_sum = 0\n",
    "for PPAP in range(10):\n",
    "\ttf.reset_default_graph()\n",
    "\n",
    "\t\"\"\"BP autoencoder\"\"\"\n",
    "\tX = tf.placeholder(tf.float32, [None, n_input_nodes])\n",
    "\tY = tf.placeholder(tf.float32, [None, n_output_nodes])\n",
    "\n",
    "\tencoding_layer = tf.layers.dense(inputs=X,units=n_hidden_nodes,activation=tf.math.sin)\n",
    "\tY_hat = tf.layers.dense(inputs=encoding_layer,units=n_output_nodes,activation=None)\n",
    "\n",
    "\tloss = tf.losses.mean_squared_error(labels=Y,predictions=Y_hat)\n",
    "\n",
    "\toptimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "\tsess = tf.Session()\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\n",
    "\tt1 = time.time()\n",
    "\tBATCH_SIZE = 128\n",
    "\tfor epoch in range(100):\n",
    "\t\ti = 0\n",
    "\t\twhile i < len(x_train):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tbatch_x = x_train[i:i+BATCH_SIZE]\n",
    "\t\t\t\tbatch_y = x_train[i:i+BATCH_SIZE]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tbatch_x = x_train[i:]\n",
    "\t\t\t\tbatch_y = x_train[i:]\n",
    "\t\t\ti+=BATCH_SIZE\n",
    "\t\t\tsess.run(optimizer, feed_dict={X:batch_x, Y:batch_y})\n",
    "\tt2 = time.time()\n",
    "\ttime_sum+=(t2-t1)\n",
    "\n",
    "\t\"\"\"classification\"\"\"\n",
    "\tx_train_encoded = sess.run(encoding_layer, feed_dict={X: x_train})\n",
    "\tx_test_encoded = sess.run(encoding_layer, feed_dict={X: x_test})\n",
    "\n",
    "\tX = tf.placeholder(tf.float32, [None, n_hidden_nodes])\n",
    "\tY = tf.placeholder(tf.int64, [None])\n",
    "\tY_ = tf.one_hot(indices=Y, depth=2) # one_hot labels: [N,M]\n",
    "\n",
    "\tfc1 = tf.layers.dense(inputs=X,units=512,activation=tf.nn.relu)\n",
    "\tfc2 = tf.layers.dense(inputs=fc1,units=512,activation=tf.nn.relu)\n",
    "\tout = tf.layers.dense(inputs=fc2,units=2,activation=None)\n",
    "\n",
    "\tloss = tf.losses.softmax_cross_entropy(logits=out,onehot_labels=Y_)\n",
    "\taccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(out,axis=1),tf.argmax(Y_,axis=1)),dtype=tf.float32))\n",
    "\n",
    "\toptimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "\tsess = tf.Session()\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\n",
    "\tBATCH_SIZE = 128\n",
    "\tfor epoch in range(100):\n",
    "\t\ti = 0\n",
    "\t\twhile i < len(x_train_encoded):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tbatch_x = x_train_encoded[i:i+BATCH_SIZE]\n",
    "\t\t\t\tbatch_y = y_train[i:i+BATCH_SIZE]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tbatch_x = x_train_encoded[i:]\n",
    "\t\t\t\tbatch_y = y_train[i:]\n",
    "\t\t\ti+=BATCH_SIZE\n",
    "\t\t\tsess.run(optimizer, feed_dict={X:batch_x, Y:batch_y})\n",
    "\n",
    "\t\"\"\"evaluation\"\"\"\n",
    "\tavg_testing_acc = sess.run(accuracy, feed_dict={X: x_test_encoded, Y: y_test})\n",
    "\tprint(\"Testing accuracy: \",avg_testing_acc)\n",
    "\taccuracy_sum+=avg_testing_acc\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"BP\t ===================================\")\n",
    "print(\"==========================================\")\n",
    "print(\"Average time: \", time_sum/10.0)\n",
    "print(\"Average accuracy: \", accuracy_sum/10.0)\n",
    "print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.savetxt(\"hillx_train.csv\", x_train, delimiter=\",\")\n",
    "np.savetxt(\"hillyt_train.csv\", y_train, delimiter=\",\")\n",
    "np.savetxt(\"hillx_test.csv\", x_test, delimiter=\",\")\n",
    "np.savetxt(\"hilly_test.csv\", y_test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "border = 106\n",
    "x_train_init = x_train[:border]\n",
    "y_train_init = y_train[:border]\n",
    "x_train_seq = x_train[border:]\n",
    "y_train_seq = y_train[border:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # clear all the tensors\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "\"\"\"Placeholders\"\"\"\n",
    "X_ = tf.placeholder(tf.float32, [None, INPUT_DIMENSION])\n",
    "#X_ = tf.reshape(X, [-1, INPUT_DIMENSION]) # Flatten X: [N,D]\n",
    "Y = tf.placeholder(tf.int64, [None]) # labels\n",
    "Y_ = tf.one_hot(indices=Y, depth=NUM_CLASSES) # one_hot labels: [N,M]\n",
    "\n",
    "\"\"\"Some constants\"\"\"\n",
    "D = INPUT_DIMENSION\n",
    "M = NUM_CLASSES # Number of outputs\n",
    "C = tf.constant(2.0**(-5))\n",
    "\n",
    "\"\"\"Weights\"\"\"\n",
    "alpha_1 = tf.get_variable('alpha_1',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 1st subnetwork\n",
    "alpha_2 = tf.get_variable('alpha_2',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 2st subnetwork\n",
    "alpha_3 = tf.get_variable('alpha_3',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_1 = tf.get_variable('beta_1',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_2 = tf.get_variable('beta_2',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_3 = tf.get_variable('beta_3',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.get_variable('k',shape=[D, D],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "m = tf.get_variable('m',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions\"\"\"\n",
    "def mul(A, B):\n",
    "    return tf.matmul(A, B)\n",
    "\n",
    "def inv(A):\n",
    "    return tf.matrix_inverse(A)\n",
    "\n",
    "def t(A):\n",
    "    return tf.transpose(A)\n",
    "\n",
    "def sin(A):\n",
    "    return tf.math.sin(A)\n",
    "\n",
    "def asin(A):\n",
    "    return tf.math.asin(A)\n",
    "\n",
    "def sqrt(A):\n",
    "    return tf.sqrt(A)\n",
    "\n",
    "def sqr(A):\n",
    "    return tf.math.pow(A, 2)\n",
    "\n",
    "def pseudo_inv(A, I, C):\n",
    "    C_I = I/C\n",
    "    return mul(t(A), inv(C_I + mul(A, t(A))))\n",
    "\n",
    "def h(A):\n",
    "    '''activation function'''\n",
    "    return sin(A)\n",
    "\n",
    "def h_(A):\n",
    "    '''inverse activation function'''\n",
    "    return asin(A)\n",
    "\n",
    "def u(A):\n",
    "    '''normalize the input to (0,1]'''\n",
    "    return tf.math.sigmoid(A) # sigmoid\n",
    "    \n",
    "def u_(A):\n",
    "    '''the inverse of u'''\n",
    "    ONE = tf.constant(1.0)\n",
    "    return -(tf.math.log(ONE/A - ONE)) # the inverse of sigmoid\n",
    "    \n",
    "def subnet_output(alpha, beta, A):\n",
    "    return t(mul(beta, h(mul(t(alpha), t(A))))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "'''some pre-computations'''\n",
    "X_init = t(X_) # [D,N]\n",
    "Y_init = t(Y_) # [M,N]\n",
    "N_init = D # number of dimensions\n",
    "I_DxD = tf.eye(N_init, dtype=tf.float32) # [D,D]\n",
    "I_MxM = tf.eye(M, dtype=tf.float32) # [M,M]\n",
    "C_I = I_DxD/C\n",
    "H_I = I_MxM/C\n",
    "\n",
    "add = C_I + mul(X_init, t(X_init))\n",
    "k = tf.assign(k,add)\n",
    "X_inv_init = pseudo_inv(X_init, I_DxD, C) # [N,D]\n",
    "\n",
    "'''1st subnet'''\n",
    "alpha_1_init_calculated = t(mul(h_(Y_init), X_inv_init)) # ([M,N]x[N,D])T=[D,M]\n",
    "alpha_1_init = tf.assign(alpha_1, alpha_1_init_calculated) # [D,M]\n",
    "H_1_init = h(mul(t(alpha_1_init), X_init)) # [M,N]\n",
    "H_add = H_I + mul(H_1_init,t(H_1_init))\n",
    "m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_1_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_1_init_calculated = mul(Y_init, t(H_1_init))/sqr(tf.norm(H_1_init)) # [M,M]\n",
    "beta_1_init_calculated = mul(Y_init,H_pseudo_init)\n",
    "\n",
    "beta_1_init = tf.assign(beta_1, beta_1_init_calculated) # [M,M]\n",
    "H_beta_1_init = mul(beta_1_init, t(mul(t(X_init), alpha_1_init))) # [M,N]\n",
    "E_1_init = Y_init - H_beta_1_init # [M,N]\n",
    "\n",
    "'''2nd subnet'''\n",
    "#alpha_2_init_calculated = t(mul(h_(E_1_init), X_inv_init)) # [D,M]    \n",
    "alpha_2_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_2_init = tf.assign(alpha_2, alpha_2_init_calculated) # [D,M]\n",
    "H_2_init = h(mul(t(alpha_2_init), X_init)) # [M,N]\n",
    "H_2_inv_init = pseudo_inv(H_2_init, I_MxM, C) # [M,N]\n",
    "H_add = H_I + mul(H_2_init,t(H_2_init))\n",
    "#m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_2_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_2_init_calculated = mul(E_1_init, t(H_2_init))/sqr(tf.norm(H_2_init)) # [M,M]\n",
    "beta_2_init_calculated = mul(E_1_init, H_pseudo_init)\n",
    "\n",
    "beta_2_init = tf.assign(beta_2, beta_2_init_calculated) # [M,M]\n",
    "H_beta_2_init = mul(beta_2_init, t(mul(t(X_init), alpha_2_init))) # [M,N]\n",
    "E_2_init = Y_init - (H_beta_1_init+H_beta_2_init) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "alpha_3_init_calculated = t(mul(h_(E_2_init), X_inv_init)) # [D,M]    \n",
    "alpha_3_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_3_init = tf.assign(alpha_3, alpha_3_init_calculated) # [D,M]\n",
    "H_3_init = h(mul(t(alpha_3_init), X_init)) # [M,N]\n",
    "H_3_inv_init = pseudo_inv(H_3_init, I_MxM, C) # [M,N]\n",
    "\n",
    "beta_3_init_calculated = mul(E_2_init, t(H_3_init))/sqr(tf.norm(H_3_init)) # [M,M]\n",
    "beta_3_init_calculated = mul(E_2_init, H_3_inv_init)\n",
    "\n",
    "beta_3_init = tf.assign(beta_3, beta_3_init_calculated) # [M,M]\n",
    "H_beta_3_init = mul(beta_3_init, t(mul(t(X_init), alpha_3_init))) # [M,N]\n",
    "E_3_init = Y_init - (H_beta_3_init+H_beta_2_init+ H_beta_1_init) # [M,N]\n",
    "\n",
    "#init_train_graph = H_beta_1_init\n",
    "init_train_graph = E_3_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_3:0' shape=(2, 2) dtype=float32_ref>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_1_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''With one subnetwork'''\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.47783542\n",
      "Initial train training accuracy:  0.49339935\n",
      "Initial train testing loss:  0.47645548\n",
      "Initial train testing accuracy:  0.5066007\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(E_1_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.46714404\n",
      "Initial train training accuracy:  0.49339935\n",
      "Initial train testing loss:  0.46499318\n",
      "Initial train testing accuracy:  0.5066007\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "import time\n",
    "s = 0\n",
    "t1 = time.time()\n",
    "sess.run(E_2_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "t2 = time.time()\n",
    "s = s+ (t2-t1)\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.46178526\n",
      "Initial train training accuracy:  0.49339935\n",
      "Initial train testing loss:  0.4591867\n",
      "Initial train testing accuracy:  0.5066007\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(init_train_graph, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-288609ea9fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"visualize subnet nodes\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mvisualize_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mvisualize_beta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mvisualize_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-288609ea9fc8>\u001b[0m in \u001b[0;36mvisualize_alpha\u001b[0;34m(alpha, size)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_beta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6e2009c668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_alpha(alpha, size):\n",
    "    tmp = sess.run(alpha)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            plt.subplot(2,5,i*5+j+1)\n",
    "            plt.imshow(np.reshape(tmp[:,i*5+j], [size,size]))\n",
    "\n",
    "def visualize_beta(beta):\n",
    "    tmp = sess.run(beta)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(tmp)\n",
    "    \n",
    "            \n",
    "\"\"\"visualize subnet nodes\"\"\"            \n",
    "visualize_alpha(alpha_1, 10)\n",
    "visualize_beta(beta_1)\n",
    "visualize_alpha(alpha_2, 10)\n",
    "visualize_beta(beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_init = subnet_output(alpha_1, beta_1, X_)+ subnet_output(alpha_2, beta_2, X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic = sess.run(logits_init, feed_dict={X_ : [x_test[4]]})\n",
    "print(np.argmax(logic,axis =1))\n",
    "print(y_test[4])\n",
    "#plt.imshow(x_test[4200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic = sess.run(logits_init, feed_dict={X_ : [x_test[4000]]})\n",
    "print(np.argmax(logic,axis =1))\n",
    "print(y_test[4000])\n",
    "#plt.imshow(x_test[4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sequential Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "X_seq = t(X_) # [D,N]\n",
    "Y_seq = t(Y_) # [M,N]\n",
    "pseudo = mul(X_seq, X_) #DXD\n",
    "k = tf.assign(k, tf.add(k,pseudo)) #DXD\n",
    "k_inv = inv(k)\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_1))\n",
    "alpha1_seq = tf.assign(alpha_1,tf.add(alpha_1,new)) #DXM\n",
    "H_1_seq = h(mul(t(alpha1_seq), X_seq)) # [M,N]\n",
    "m_su = mul(H_1_seq,t(H_1_seq))\n",
    "m = tf.assign(m,tf.add(m,m_su))\n",
    "m_inv = inv(m)\n",
    "#update = tf.matmul(tf.matmul(m_inv,H_1_seq),h_(Y_seq)- tf.matmul())\n",
    "H_pseudo_init = pseudo_inv(H_1_seq,I_MxM,C) #[N,M]\n",
    "#UPDATE = tf.matmul(tf.matmul(K_inverse, HT), inverse_acti_y - tf.matmul(H, self.__outputWeight))\n",
    "beta_1_seq_calculated = mul(Y_seq, H_pseudo_init) # [M,M]\n",
    "beta_1_seq = tf.assign(beta_1, beta_1_seq_calculated) # [M,M]\n",
    "H_beta_1_seq = mul(beta_1_seq, t(mul(X_, alpha1_seq))) # [M,N]\n",
    "E_1_seq = Y_seq - H_beta_1_seq # [M,N]\n",
    "\n",
    "'''2nd subnetwork'''\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_2))\n",
    "alpha2_seq = tf.assign(alpha_2,tf.add(alpha_2,new)) #DXM\n",
    "H_2_seq = h(mul(t(alpha2_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_2_seq,I_MxM,C) #[N,M]\n",
    "beta_2_seq_calculated = mul(E_1_seq, H_pseudo_init) # [M,M]\n",
    "beta_2_seq = tf.assign(beta_2, beta_2_seq_calculated) # [M,M]\n",
    "H_beta_2_seq = mul(beta_2_seq, t(mul(t(X_seq), alpha2_seq))) # [M,N]\n",
    "E_2_seq = Y_seq - (H_beta_2_seq+ H_beta_1_seq) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_3))\n",
    "alpha3_seq = tf.assign(alpha_3,tf.add(alpha_3,new)) #DXM\n",
    "H_3_seq = h(mul(t(alpha3_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_3_seq,I_MxM,C) #[N,M]\n",
    "beta_3_seq_calculated = mul(E_2_seq, H_pseudo_init) # [M,M]\n",
    "beta_3_seq = tf.assign(beta_3, beta_3_seq_calculated) # [M,M]\n",
    "H_beta_3_seq = mul(beta_3_seq, t(mul(t(X_seq), alpha3_seq))) # [M,N]\n",
    "E_3_seq = Y_seq - (H_beta_3_seq +H_beta_2_seq + H_beta_1_seq )# [M,N]\n",
    "seq_train_graph = E_3_seq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.454951, train_accuracy: 0.493399\n",
      "test_loss: 0.451328, test_accuracy: 0.506601\n",
      "train_loss: 0.454537, train_accuracy: 0.493399\n",
      "test_loss: 0.450925, test_accuracy: 0.506601\n",
      "train_loss: 0.454345, train_accuracy: 0.493399\n",
      "test_loss: 0.450750, test_accuracy: 0.506601\n",
      "train_loss: 0.454200, train_accuracy: 0.493399\n",
      "test_loss: 0.450627, test_accuracy: 0.506601\n",
      "train_loss: 0.454059, train_accuracy: 0.493399\n",
      "test_loss: 0.450510, test_accuracy: 0.506601\n",
      "train_loss: 0.453910, train_accuracy: 0.493399\n",
      "test_loss: 0.450387, test_accuracy: 0.506601\n",
      "train_loss: 0.453753, train_accuracy: 0.493399\n",
      "test_loss: 0.450258, test_accuracy: 0.506601\n",
      "train_loss: 0.453587, train_accuracy: 0.493399\n",
      "test_loss: 0.450121, test_accuracy: 0.506601\n",
      "train_loss: 0.453413, train_accuracy: 0.493399\n",
      "test_loss: 0.449978, test_accuracy: 0.506601\n",
      "train_loss: 0.453230, train_accuracy: 0.493399\n",
      "test_loss: 0.449828, test_accuracy: 0.506601\n",
      "train_loss: 0.453040, train_accuracy: 0.493399\n",
      "test_loss: 0.449672, test_accuracy: 0.506601\n",
      "train_loss: 0.452843, train_accuracy: 0.493399\n",
      "test_loss: 0.449511, test_accuracy: 0.506601\n",
      "train_loss: 0.452640, train_accuracy: 0.493399\n",
      "test_loss: 0.449345, test_accuracy: 0.506601\n",
      "train_loss: 0.452431, train_accuracy: 0.493399\n",
      "test_loss: 0.449175, test_accuracy: 0.506601\n",
      "train_loss: 0.452217, train_accuracy: 0.491749\n",
      "test_loss: 0.449001, test_accuracy: 0.508251\n",
      "train_loss: 0.451999, train_accuracy: 0.491749\n",
      "test_loss: 0.448824, test_accuracy: 0.508251\n",
      "train_loss: 0.451776, train_accuracy: 0.491749\n",
      "test_loss: 0.448644, test_accuracy: 0.508251\n",
      "train_loss: 0.451550, train_accuracy: 0.491749\n",
      "test_loss: 0.448461, test_accuracy: 0.508251\n",
      "train_loss: 0.451321, train_accuracy: 0.491749\n",
      "test_loss: 0.448277, test_accuracy: 0.508251\n",
      "train_loss: 0.451089, train_accuracy: 0.491749\n",
      "test_loss: 0.448090, test_accuracy: 0.508251\n",
      "train_loss: 0.450854, train_accuracy: 0.493399\n",
      "test_loss: 0.447902, test_accuracy: 0.508251\n",
      "train_loss: 0.450618, train_accuracy: 0.493399\n",
      "test_loss: 0.447712, test_accuracy: 0.508251\n",
      "train_loss: 0.450380, train_accuracy: 0.493399\n",
      "test_loss: 0.447522, test_accuracy: 0.508251\n",
      "train_loss: 0.450140, train_accuracy: 0.496700\n",
      "test_loss: 0.447331, test_accuracy: 0.516502\n",
      "train_loss: 0.449899, train_accuracy: 0.500000\n",
      "test_loss: 0.447140, test_accuracy: 0.521452\n",
      "train_loss: 0.449658, train_accuracy: 0.503300\n",
      "test_loss: 0.446949, test_accuracy: 0.529703\n",
      "train_loss: 0.449416, train_accuracy: 0.508251\n",
      "test_loss: 0.446757, test_accuracy: 0.533003\n",
      "train_loss: 0.449173, train_accuracy: 0.513201\n",
      "test_loss: 0.446566, test_accuracy: 0.537954\n",
      "train_loss: 0.448931, train_accuracy: 0.516502\n",
      "test_loss: 0.446376, test_accuracy: 0.544554\n",
      "train_loss: 0.448689, train_accuracy: 0.518152\n",
      "test_loss: 0.446186, test_accuracy: 0.549505\n",
      "train_loss: 0.448447, train_accuracy: 0.524752\n",
      "test_loss: 0.445997, test_accuracy: 0.554455\n",
      "train_loss: 0.448205, train_accuracy: 0.533003\n",
      "test_loss: 0.445808, test_accuracy: 0.559406\n",
      "train_loss: 0.447964, train_accuracy: 0.536304\n",
      "test_loss: 0.445621, test_accuracy: 0.567657\n",
      "train_loss: 0.447724, train_accuracy: 0.546205\n",
      "test_loss: 0.445435, test_accuracy: 0.570957\n",
      "train_loss: 0.447485, train_accuracy: 0.546205\n",
      "test_loss: 0.445250, test_accuracy: 0.579208\n",
      "train_loss: 0.447247, train_accuracy: 0.562706\n",
      "test_loss: 0.445067, test_accuracy: 0.585809\n",
      "train_loss: 0.447010, train_accuracy: 0.569307\n",
      "test_loss: 0.444885, test_accuracy: 0.589109\n",
      "train_loss: 0.446774, train_accuracy: 0.577558\n",
      "test_loss: 0.444705, test_accuracy: 0.592409\n",
      "train_loss: 0.446540, train_accuracy: 0.579208\n",
      "test_loss: 0.444526, test_accuracy: 0.600660\n",
      "train_loss: 0.446307, train_accuracy: 0.584158\n",
      "test_loss: 0.444349, test_accuracy: 0.603960\n",
      "train_loss: 0.446076, train_accuracy: 0.599010\n",
      "test_loss: 0.444173, test_accuracy: 0.605611\n",
      "train_loss: 0.445846, train_accuracy: 0.603960\n",
      "test_loss: 0.444000, test_accuracy: 0.615512\n",
      "train_loss: 0.445618, train_accuracy: 0.612211\n",
      "test_loss: 0.443828, test_accuracy: 0.632013\n",
      "train_loss: 0.445391, train_accuracy: 0.618812\n",
      "test_loss: 0.443658, test_accuracy: 0.636964\n",
      "train_loss: 0.445166, train_accuracy: 0.618812\n",
      "test_loss: 0.443490, test_accuracy: 0.645214\n",
      "train_loss: 0.444943, train_accuracy: 0.625413\n",
      "test_loss: 0.443324, test_accuracy: 0.651815\n",
      "train_loss: 0.444722, train_accuracy: 0.635314\n",
      "test_loss: 0.443160, test_accuracy: 0.651815\n",
      "train_loss: 0.444503, train_accuracy: 0.643564\n",
      "test_loss: 0.442998, test_accuracy: 0.661716\n",
      "train_loss: 0.444285, train_accuracy: 0.653465\n",
      "test_loss: 0.442837, test_accuracy: 0.668317\n",
      "train_loss: 0.444070, train_accuracy: 0.658416\n",
      "test_loss: 0.442679, test_accuracy: 0.668317\n",
      "train_loss: 0.443856, train_accuracy: 0.666667\n",
      "test_loss: 0.442523, test_accuracy: 0.673267\n",
      "train_loss: 0.443644, train_accuracy: 0.679868\n",
      "test_loss: 0.442369, test_accuracy: 0.674918\n",
      "train_loss: 0.443434, train_accuracy: 0.683168\n",
      "test_loss: 0.442217, test_accuracy: 0.678218\n",
      "train_loss: 0.443226, train_accuracy: 0.686469\n",
      "test_loss: 0.442067, test_accuracy: 0.679868\n",
      "train_loss: 0.443020, train_accuracy: 0.688119\n",
      "test_loss: 0.441919, test_accuracy: 0.686469\n",
      "train_loss: 0.442816, train_accuracy: 0.693069\n",
      "test_loss: 0.441773, test_accuracy: 0.688119\n",
      "train_loss: 0.442614, train_accuracy: 0.694719\n",
      "test_loss: 0.441629, test_accuracy: 0.693069\n",
      "train_loss: 0.442414, train_accuracy: 0.699670\n",
      "test_loss: 0.441487, test_accuracy: 0.696370\n",
      "train_loss: 0.442215, train_accuracy: 0.707921\n",
      "test_loss: 0.441347, test_accuracy: 0.702970\n",
      "train_loss: 0.442019, train_accuracy: 0.719472\n",
      "test_loss: 0.441210, test_accuracy: 0.706271\n",
      "train_loss: 0.441825, train_accuracy: 0.731023\n",
      "test_loss: 0.441074, test_accuracy: 0.716172\n",
      "train_loss: 0.441632, train_accuracy: 0.731023\n",
      "test_loss: 0.440940, test_accuracy: 0.727723\n",
      "train_loss: 0.441442, train_accuracy: 0.734323\n",
      "test_loss: 0.440808, test_accuracy: 0.729373\n",
      "train_loss: 0.441253, train_accuracy: 0.739274\n",
      "test_loss: 0.440678, test_accuracy: 0.737624\n",
      "train_loss: 0.441066, train_accuracy: 0.745875\n",
      "test_loss: 0.440551, test_accuracy: 0.735974\n",
      "train_loss: 0.440881, train_accuracy: 0.747525\n",
      "test_loss: 0.440425, test_accuracy: 0.735974\n",
      "train_loss: 0.440698, train_accuracy: 0.750825\n",
      "test_loss: 0.440301, test_accuracy: 0.740924\n",
      "train_loss: 0.440517, train_accuracy: 0.752475\n",
      "test_loss: 0.440179, test_accuracy: 0.740924\n",
      "train_loss: 0.440338, train_accuracy: 0.759076\n",
      "test_loss: 0.440059, test_accuracy: 0.745875\n",
      "train_loss: 0.440160, train_accuracy: 0.762376\n",
      "test_loss: 0.439941, test_accuracy: 0.750825\n",
      "train_loss: 0.439985, train_accuracy: 0.764026\n",
      "test_loss: 0.439825, test_accuracy: 0.752475\n",
      "train_loss: 0.439811, train_accuracy: 0.765677\n",
      "test_loss: 0.439710, test_accuracy: 0.755776\n",
      "train_loss: 0.439639, train_accuracy: 0.767327\n",
      "test_loss: 0.439598, test_accuracy: 0.757426\n",
      "train_loss: 0.439468, train_accuracy: 0.770627\n",
      "test_loss: 0.439488, test_accuracy: 0.760726\n",
      "train_loss: 0.439300, train_accuracy: 0.772277\n",
      "test_loss: 0.439379, test_accuracy: 0.765677\n",
      "train_loss: 0.439133, train_accuracy: 0.778878\n",
      "test_loss: 0.439272, test_accuracy: 0.768977\n",
      "train_loss: 0.438968, train_accuracy: 0.778878\n",
      "test_loss: 0.439167, test_accuracy: 0.775578\n",
      "train_loss: 0.438804, train_accuracy: 0.783828\n",
      "test_loss: 0.439064, test_accuracy: 0.777228\n",
      "train_loss: 0.438643, train_accuracy: 0.783828\n",
      "test_loss: 0.438962, test_accuracy: 0.778878\n",
      "train_loss: 0.438482, train_accuracy: 0.788779\n",
      "test_loss: 0.438863, test_accuracy: 0.783828\n",
      "train_loss: 0.438324, train_accuracy: 0.788779\n",
      "test_loss: 0.438765, test_accuracy: 0.787129\n",
      "train_loss: 0.438167, train_accuracy: 0.792079\n",
      "test_loss: 0.438669, test_accuracy: 0.788779\n",
      "train_loss: 0.438012, train_accuracy: 0.797030\n",
      "test_loss: 0.438575, test_accuracy: 0.790429\n",
      "train_loss: 0.437858, train_accuracy: 0.797030\n",
      "test_loss: 0.438482, test_accuracy: 0.793729\n",
      "train_loss: 0.437706, train_accuracy: 0.798680\n",
      "test_loss: 0.438391, test_accuracy: 0.797030\n",
      "train_loss: 0.437556, train_accuracy: 0.803630\n",
      "test_loss: 0.438302, test_accuracy: 0.800330\n",
      "train_loss: 0.437407, train_accuracy: 0.803630\n",
      "test_loss: 0.438214, test_accuracy: 0.805281\n",
      "train_loss: 0.437259, train_accuracy: 0.803630\n",
      "test_loss: 0.438128, test_accuracy: 0.805281\n",
      "train_loss: 0.437113, train_accuracy: 0.805281\n",
      "test_loss: 0.438044, test_accuracy: 0.805281\n",
      "train_loss: 0.436969, train_accuracy: 0.808581\n",
      "test_loss: 0.437961, test_accuracy: 0.806931\n",
      "train_loss: 0.436826, train_accuracy: 0.808581\n",
      "test_loss: 0.437880, test_accuracy: 0.810231\n",
      "train_loss: 0.436684, train_accuracy: 0.815181\n",
      "test_loss: 0.437800, test_accuracy: 0.810231\n",
      "train_loss: 0.436544, train_accuracy: 0.816832\n",
      "test_loss: 0.437722, test_accuracy: 0.811881\n",
      "train_loss: 0.436405, train_accuracy: 0.816832\n",
      "test_loss: 0.437646, test_accuracy: 0.813531\n",
      "train_loss: 0.436268, train_accuracy: 0.818482\n",
      "test_loss: 0.437571, test_accuracy: 0.816832\n",
      "train_loss: 0.436132, train_accuracy: 0.823432\n",
      "test_loss: 0.437498, test_accuracy: 0.818482\n",
      "train_loss: 0.435997, train_accuracy: 0.823432\n",
      "test_loss: 0.437426, test_accuracy: 0.821782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.435864, train_accuracy: 0.826733\n",
      "test_loss: 0.437356, test_accuracy: 0.825082\n",
      "train_loss: 0.435732, train_accuracy: 0.828383\n",
      "test_loss: 0.437287, test_accuracy: 0.826733\n",
      "train_loss: 0.435601, train_accuracy: 0.830033\n",
      "test_loss: 0.437220, test_accuracy: 0.826733\n",
      "train_loss: 0.435472, train_accuracy: 0.831683\n",
      "test_loss: 0.437154, test_accuracy: 0.830033\n",
      "train_loss: 0.435344, train_accuracy: 0.833333\n",
      "test_loss: 0.437089, test_accuracy: 0.830033\n",
      "train_loss: 0.435217, train_accuracy: 0.838284\n",
      "test_loss: 0.437026, test_accuracy: 0.831683\n",
      "train_loss: 0.435092, train_accuracy: 0.839934\n",
      "test_loss: 0.436964, test_accuracy: 0.833333\n",
      "train_loss: 0.434967, train_accuracy: 0.839934\n",
      "test_loss: 0.436904, test_accuracy: 0.836634\n",
      "train_loss: 0.434844, train_accuracy: 0.844885\n",
      "test_loss: 0.436845, test_accuracy: 0.836634\n",
      "train_loss: 0.434722, train_accuracy: 0.848185\n",
      "test_loss: 0.436788, test_accuracy: 0.836634\n",
      "train_loss: 0.434602, train_accuracy: 0.848185\n",
      "test_loss: 0.436731, test_accuracy: 0.843234\n",
      "train_loss: 0.434482, train_accuracy: 0.853135\n",
      "test_loss: 0.436676, test_accuracy: 0.843234\n",
      "train_loss: 0.434364, train_accuracy: 0.853135\n",
      "test_loss: 0.436623, test_accuracy: 0.844885\n",
      "train_loss: 0.434246, train_accuracy: 0.853135\n",
      "test_loss: 0.436570, test_accuracy: 0.846535\n",
      "train_loss: 0.434130, train_accuracy: 0.853135\n",
      "test_loss: 0.436519, test_accuracy: 0.851485\n",
      "train_loss: 0.434015, train_accuracy: 0.854786\n",
      "test_loss: 0.436469, test_accuracy: 0.853135\n",
      "train_loss: 0.433901, train_accuracy: 0.858086\n",
      "test_loss: 0.436420, test_accuracy: 0.853135\n",
      "train_loss: 0.433788, train_accuracy: 0.859736\n",
      "test_loss: 0.436373, test_accuracy: 0.859736\n",
      "train_loss: 0.433677, train_accuracy: 0.859736\n",
      "test_loss: 0.436326, test_accuracy: 0.859736\n",
      "train_loss: 0.433566, train_accuracy: 0.861386\n",
      "test_loss: 0.436281, test_accuracy: 0.858086\n",
      "train_loss: 0.433456, train_accuracy: 0.861386\n",
      "test_loss: 0.436237, test_accuracy: 0.859736\n",
      "train_loss: 0.433348, train_accuracy: 0.863036\n",
      "test_loss: 0.436194, test_accuracy: 0.859736\n",
      "train_loss: 0.433240, train_accuracy: 0.864686\n",
      "test_loss: 0.436152, test_accuracy: 0.861386\n",
      "train_loss: 0.433133, train_accuracy: 0.864686\n",
      "test_loss: 0.436112, test_accuracy: 0.861386\n",
      "train_loss: 0.433027, train_accuracy: 0.867987\n",
      "test_loss: 0.436072, test_accuracy: 0.861386\n",
      "train_loss: 0.432923, train_accuracy: 0.869637\n",
      "test_loss: 0.436033, test_accuracy: 0.863036\n",
      "train_loss: 0.432819, train_accuracy: 0.869637\n",
      "test_loss: 0.435996, test_accuracy: 0.863036\n",
      "train_loss: 0.432716, train_accuracy: 0.871287\n",
      "test_loss: 0.435959, test_accuracy: 0.863036\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(125):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(seq_train_graph, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\"\n",
    "#tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_train, Y: y_train})\n",
    "#ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_test, Y: y_test})\n",
    "#print(\"Sequential train training loss: \", tr_loss)\n",
    "#print(\"Sequential train training accuracy: \", tr_acc)\n",
    "#print(\"Sequential train testing loss: \", ts_loss)\n",
    "#print(\"Sequential train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.438455, train_accuracy: 0.818482\n",
      "test_loss: 0.440444, test_accuracy: 0.810231\n",
      "train_loss: 0.438367, train_accuracy: 0.821782\n",
      "test_loss: 0.440413, test_accuracy: 0.811881\n",
      "train_loss: 0.438280, train_accuracy: 0.823432\n",
      "test_loss: 0.440382, test_accuracy: 0.815181\n",
      "train_loss: 0.438194, train_accuracy: 0.825082\n",
      "test_loss: 0.440352, test_accuracy: 0.815181\n",
      "train_loss: 0.438108, train_accuracy: 0.825082\n",
      "test_loss: 0.440323, test_accuracy: 0.815181\n",
      "train_loss: 0.438023, train_accuracy: 0.826733\n",
      "test_loss: 0.440295, test_accuracy: 0.818482\n",
      "train_loss: 0.437938, train_accuracy: 0.828383\n",
      "test_loss: 0.440267, test_accuracy: 0.818482\n",
      "train_loss: 0.437855, train_accuracy: 0.830033\n",
      "test_loss: 0.440240, test_accuracy: 0.820132\n",
      "train_loss: 0.437772, train_accuracy: 0.830033\n",
      "test_loss: 0.440214, test_accuracy: 0.823432\n",
      "train_loss: 0.437689, train_accuracy: 0.833333\n",
      "test_loss: 0.440188, test_accuracy: 0.825082\n",
      "train_loss: 0.437607, train_accuracy: 0.834984\n",
      "test_loss: 0.440163, test_accuracy: 0.826733\n",
      "train_loss: 0.437526, train_accuracy: 0.836634\n",
      "test_loss: 0.440139, test_accuracy: 0.826733\n",
      "train_loss: 0.437446, train_accuracy: 0.836634\n",
      "test_loss: 0.440116, test_accuracy: 0.826733\n",
      "train_loss: 0.437366, train_accuracy: 0.841584\n",
      "test_loss: 0.440092, test_accuracy: 0.830033\n",
      "train_loss: 0.437287, train_accuracy: 0.841584\n",
      "test_loss: 0.440070, test_accuracy: 0.831683\n",
      "train_loss: 0.437208, train_accuracy: 0.843234\n",
      "test_loss: 0.440048, test_accuracy: 0.833333\n",
      "train_loss: 0.437130, train_accuracy: 0.844885\n",
      "test_loss: 0.440027, test_accuracy: 0.834984\n",
      "train_loss: 0.437052, train_accuracy: 0.844885\n",
      "test_loss: 0.440006, test_accuracy: 0.836634\n",
      "train_loss: 0.436975, train_accuracy: 0.846535\n",
      "test_loss: 0.439986, test_accuracy: 0.838284\n",
      "train_loss: 0.436899, train_accuracy: 0.849835\n",
      "test_loss: 0.439966, test_accuracy: 0.838284\n",
      "train_loss: 0.436823, train_accuracy: 0.849835\n",
      "test_loss: 0.439947, test_accuracy: 0.841584\n",
      "train_loss: 0.436748, train_accuracy: 0.849835\n",
      "test_loss: 0.439929, test_accuracy: 0.846535\n",
      "train_loss: 0.436673, train_accuracy: 0.849835\n",
      "test_loss: 0.439910, test_accuracy: 0.848185\n",
      "train_loss: 0.436599, train_accuracy: 0.851485\n",
      "test_loss: 0.439893, test_accuracy: 0.849835\n",
      "train_loss: 0.436525, train_accuracy: 0.851485\n",
      "test_loss: 0.439876, test_accuracy: 0.851485\n",
      "train_loss: 0.436452, train_accuracy: 0.856436\n",
      "test_loss: 0.439859, test_accuracy: 0.851485\n",
      "train_loss: 0.436380, train_accuracy: 0.856436\n",
      "test_loss: 0.439842, test_accuracy: 0.851485\n",
      "train_loss: 0.436308, train_accuracy: 0.858086\n",
      "test_loss: 0.439826, test_accuracy: 0.853135\n",
      "train_loss: 0.436236, train_accuracy: 0.858086\n",
      "test_loss: 0.439811, test_accuracy: 0.856436\n",
      "train_loss: 0.436165, train_accuracy: 0.858086\n",
      "test_loss: 0.439796, test_accuracy: 0.858086\n",
      "train_loss: 0.436095, train_accuracy: 0.858086\n",
      "test_loss: 0.439781, test_accuracy: 0.858086\n",
      "train_loss: 0.436025, train_accuracy: 0.859736\n",
      "test_loss: 0.439766, test_accuracy: 0.858086\n",
      "train_loss: 0.435955, train_accuracy: 0.859736\n",
      "test_loss: 0.439752, test_accuracy: 0.858086\n",
      "train_loss: 0.435886, train_accuracy: 0.859736\n",
      "test_loss: 0.439739, test_accuracy: 0.858086\n",
      "train_loss: 0.435818, train_accuracy: 0.859736\n",
      "test_loss: 0.439725, test_accuracy: 0.858086\n",
      "train_loss: 0.435750, train_accuracy: 0.861386\n",
      "test_loss: 0.439712, test_accuracy: 0.858086\n",
      "train_loss: 0.435682, train_accuracy: 0.861386\n",
      "test_loss: 0.439699, test_accuracy: 0.861386\n",
      "train_loss: 0.435615, train_accuracy: 0.861386\n",
      "test_loss: 0.439686, test_accuracy: 0.861386\n",
      "train_loss: 0.435549, train_accuracy: 0.861386\n",
      "test_loss: 0.439674, test_accuracy: 0.863036\n",
      "train_loss: 0.435482, train_accuracy: 0.863036\n",
      "test_loss: 0.439662, test_accuracy: 0.863036\n",
      "train_loss: 0.435417, train_accuracy: 0.863036\n",
      "test_loss: 0.439650, test_accuracy: 0.863036\n",
      "train_loss: 0.435351, train_accuracy: 0.864686\n",
      "test_loss: 0.439638, test_accuracy: 0.863036\n",
      "train_loss: 0.435287, train_accuracy: 0.864686\n",
      "test_loss: 0.439627, test_accuracy: 0.866337\n",
      "train_loss: 0.435222, train_accuracy: 0.866337\n",
      "test_loss: 0.439616, test_accuracy: 0.866337\n",
      "train_loss: 0.435158, train_accuracy: 0.867987\n",
      "test_loss: 0.439604, test_accuracy: 0.866337\n",
      "train_loss: 0.435095, train_accuracy: 0.867987\n",
      "test_loss: 0.439594, test_accuracy: 0.866337\n",
      "train_loss: 0.435032, train_accuracy: 0.871287\n",
      "test_loss: 0.439583, test_accuracy: 0.866337\n",
      "train_loss: 0.434969, train_accuracy: 0.871287\n",
      "test_loss: 0.439572, test_accuracy: 0.869637\n",
      "train_loss: 0.434906, train_accuracy: 0.871287\n",
      "test_loss: 0.439562, test_accuracy: 0.871287\n",
      "train_loss: 0.434845, train_accuracy: 0.871287\n",
      "test_loss: 0.439551, test_accuracy: 0.871287\n",
      "train_loss: 0.434783, train_accuracy: 0.871287\n",
      "test_loss: 0.439541, test_accuracy: 0.871287\n",
      "train_loss: 0.434722, train_accuracy: 0.871287\n",
      "test_loss: 0.439531, test_accuracy: 0.871287\n",
      "train_loss: 0.434661, train_accuracy: 0.874587\n",
      "test_loss: 0.439521, test_accuracy: 0.871287\n",
      "train_loss: 0.434601, train_accuracy: 0.874587\n",
      "test_loss: 0.439511, test_accuracy: 0.872937\n",
      "train_loss: 0.434541, train_accuracy: 0.874587\n",
      "test_loss: 0.439501, test_accuracy: 0.872937\n",
      "train_loss: 0.434481, train_accuracy: 0.874587\n",
      "test_loss: 0.439491, test_accuracy: 0.872937\n",
      "train_loss: 0.434422, train_accuracy: 0.874587\n",
      "test_loss: 0.439481, test_accuracy: 0.874587\n",
      "train_loss: 0.434363, train_accuracy: 0.876238\n",
      "test_loss: 0.439472, test_accuracy: 0.874587\n",
      "train_loss: 0.434305, train_accuracy: 0.876238\n",
      "test_loss: 0.439462, test_accuracy: 0.874587\n",
      "train_loss: 0.434247, train_accuracy: 0.879538\n",
      "test_loss: 0.439452, test_accuracy: 0.874587\n",
      "train_loss: 0.434189, train_accuracy: 0.879538\n",
      "test_loss: 0.439442, test_accuracy: 0.874587\n",
      "train_loss: 0.434132, train_accuracy: 0.879538\n",
      "test_loss: 0.439433, test_accuracy: 0.874587\n",
      "train_loss: 0.434075, train_accuracy: 0.879538\n",
      "test_loss: 0.439423, test_accuracy: 0.874587\n",
      "train_loss: 0.434018, train_accuracy: 0.881188\n",
      "test_loss: 0.439414, test_accuracy: 0.874587\n",
      "train_loss: 0.433962, train_accuracy: 0.881188\n",
      "test_loss: 0.439404, test_accuracy: 0.874587\n",
      "train_loss: 0.433906, train_accuracy: 0.881188\n",
      "test_loss: 0.439394, test_accuracy: 0.874587\n",
      "train_loss: 0.433850, train_accuracy: 0.882838\n",
      "test_loss: 0.439384, test_accuracy: 0.874587\n",
      "train_loss: 0.433795, train_accuracy: 0.884488\n",
      "test_loss: 0.439375, test_accuracy: 0.874587\n",
      "train_loss: 0.433740, train_accuracy: 0.884488\n",
      "test_loss: 0.439365, test_accuracy: 0.876238\n",
      "train_loss: 0.433685, train_accuracy: 0.884488\n",
      "test_loss: 0.439355, test_accuracy: 0.876238\n",
      "train_loss: 0.433630, train_accuracy: 0.886139\n",
      "test_loss: 0.439345, test_accuracy: 0.876238\n",
      "train_loss: 0.433576, train_accuracy: 0.886139\n",
      "test_loss: 0.439335, test_accuracy: 0.876238\n",
      "train_loss: 0.433523, train_accuracy: 0.886139\n",
      "test_loss: 0.439325, test_accuracy: 0.876238\n",
      "train_loss: 0.433469, train_accuracy: 0.886139\n",
      "test_loss: 0.439315, test_accuracy: 0.876238\n",
      "train_loss: 0.433416, train_accuracy: 0.889439\n",
      "test_loss: 0.439304, test_accuracy: 0.876238\n",
      "train_loss: 0.433363, train_accuracy: 0.889439\n",
      "test_loss: 0.439294, test_accuracy: 0.876238\n",
      "train_loss: 0.433311, train_accuracy: 0.891089\n",
      "test_loss: 0.439284, test_accuracy: 0.876238\n",
      "train_loss: 0.433259, train_accuracy: 0.892739\n",
      "test_loss: 0.439273, test_accuracy: 0.876238\n",
      "train_loss: 0.433207, train_accuracy: 0.892739\n",
      "test_loss: 0.439262, test_accuracy: 0.876238\n",
      "train_loss: 0.433155, train_accuracy: 0.892739\n",
      "test_loss: 0.439252, test_accuracy: 0.876238\n",
      "train_loss: 0.433104, train_accuracy: 0.892739\n",
      "test_loss: 0.439241, test_accuracy: 0.876238\n",
      "train_loss: 0.433053, train_accuracy: 0.892739\n",
      "test_loss: 0.439230, test_accuracy: 0.876238\n",
      "train_loss: 0.433002, train_accuracy: 0.892739\n",
      "test_loss: 0.439219, test_accuracy: 0.877888\n",
      "train_loss: 0.432951, train_accuracy: 0.892739\n",
      "test_loss: 0.439208, test_accuracy: 0.877888\n",
      "train_loss: 0.432901, train_accuracy: 0.892739\n",
      "test_loss: 0.439196, test_accuracy: 0.877888\n",
      "train_loss: 0.432851, train_accuracy: 0.894389\n",
      "test_loss: 0.439185, test_accuracy: 0.879538\n",
      "train_loss: 0.432802, train_accuracy: 0.894389\n",
      "test_loss: 0.439173, test_accuracy: 0.879538\n",
      "train_loss: 0.432752, train_accuracy: 0.894389\n",
      "test_loss: 0.439162, test_accuracy: 0.879538\n",
      "train_loss: 0.432703, train_accuracy: 0.894389\n",
      "test_loss: 0.439150, test_accuracy: 0.881188\n",
      "train_loss: 0.432654, train_accuracy: 0.894389\n",
      "test_loss: 0.439138, test_accuracy: 0.881188\n",
      "train_loss: 0.432605, train_accuracy: 0.894389\n",
      "test_loss: 0.439126, test_accuracy: 0.881188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.432557, train_accuracy: 0.894389\n",
      "test_loss: 0.439113, test_accuracy: 0.882838\n",
      "train_loss: 0.432509, train_accuracy: 0.894389\n",
      "test_loss: 0.439101, test_accuracy: 0.882838\n",
      "train_loss: 0.432461, train_accuracy: 0.894389\n",
      "test_loss: 0.439089, test_accuracy: 0.882838\n",
      "train_loss: 0.432413, train_accuracy: 0.894389\n",
      "test_loss: 0.439076, test_accuracy: 0.882838\n",
      "train_loss: 0.432366, train_accuracy: 0.894389\n",
      "test_loss: 0.439063, test_accuracy: 0.882838\n",
      "train_loss: 0.432319, train_accuracy: 0.894389\n",
      "test_loss: 0.439050, test_accuracy: 0.882838\n",
      "train_loss: 0.432272, train_accuracy: 0.896040\n",
      "test_loss: 0.439037, test_accuracy: 0.882838\n",
      "train_loss: 0.432225, train_accuracy: 0.897690\n",
      "test_loss: 0.439024, test_accuracy: 0.884488\n",
      "train_loss: 0.432179, train_accuracy: 0.897690\n",
      "test_loss: 0.439011, test_accuracy: 0.886139\n",
      "train_loss: 0.432133, train_accuracy: 0.897690\n",
      "test_loss: 0.438997, test_accuracy: 0.886139\n",
      "train_loss: 0.432087, train_accuracy: 0.897690\n",
      "test_loss: 0.438983, test_accuracy: 0.886139\n",
      "train_loss: 0.432041, train_accuracy: 0.897690\n",
      "test_loss: 0.438970, test_accuracy: 0.887789\n",
      "train_loss: 0.431996, train_accuracy: 0.897690\n",
      "test_loss: 0.438956, test_accuracy: 0.887789\n",
      "train_loss: 0.431950, train_accuracy: 0.897690\n",
      "test_loss: 0.438942, test_accuracy: 0.887789\n",
      "train_loss: 0.431905, train_accuracy: 0.897690\n",
      "test_loss: 0.438927, test_accuracy: 0.887789\n",
      "train_loss: 0.431861, train_accuracy: 0.897690\n",
      "test_loss: 0.438913, test_accuracy: 0.887789\n",
      "train_loss: 0.431816, train_accuracy: 0.897690\n",
      "test_loss: 0.438899, test_accuracy: 0.887789\n",
      "train_loss: 0.431772, train_accuracy: 0.897690\n",
      "test_loss: 0.438884, test_accuracy: 0.887789\n",
      "train_loss: 0.431727, train_accuracy: 0.897690\n",
      "test_loss: 0.438869, test_accuracy: 0.887789\n",
      "train_loss: 0.431683, train_accuracy: 0.897690\n",
      "test_loss: 0.438854, test_accuracy: 0.889439\n",
      "train_loss: 0.431640, train_accuracy: 0.897690\n",
      "test_loss: 0.438839, test_accuracy: 0.889439\n",
      "train_loss: 0.431596, train_accuracy: 0.897690\n",
      "test_loss: 0.438824, test_accuracy: 0.892739\n",
      "train_loss: 0.431553, train_accuracy: 0.897690\n",
      "test_loss: 0.438809, test_accuracy: 0.892739\n",
      "train_loss: 0.431510, train_accuracy: 0.897690\n",
      "test_loss: 0.438793, test_accuracy: 0.894389\n",
      "train_loss: 0.431467, train_accuracy: 0.897690\n",
      "test_loss: 0.438778, test_accuracy: 0.894389\n",
      "train_loss: 0.431424, train_accuracy: 0.897690\n",
      "test_loss: 0.438762, test_accuracy: 0.894389\n",
      "train_loss: 0.431381, train_accuracy: 0.897690\n",
      "test_loss: 0.438746, test_accuracy: 0.894389\n",
      "train_loss: 0.431339, train_accuracy: 0.897690\n",
      "test_loss: 0.438731, test_accuracy: 0.894389\n",
      "train_loss: 0.431297, train_accuracy: 0.899340\n",
      "test_loss: 0.438715, test_accuracy: 0.894389\n",
      "train_loss: 0.431255, train_accuracy: 0.899340\n",
      "test_loss: 0.438699, test_accuracy: 0.896040\n",
      "train_loss: 0.431213, train_accuracy: 0.900990\n",
      "test_loss: 0.438682, test_accuracy: 0.897690\n",
      "train_loss: 0.431171, train_accuracy: 0.900990\n",
      "test_loss: 0.438666, test_accuracy: 0.897690\n",
      "train_loss: 0.431130, train_accuracy: 0.900990\n",
      "test_loss: 0.438650, test_accuracy: 0.897690\n",
      "train_loss: 0.431089, train_accuracy: 0.900990\n",
      "test_loss: 0.438633, test_accuracy: 0.897690\n",
      "train_loss: 0.431048, train_accuracy: 0.900990\n",
      "test_loss: 0.438617, test_accuracy: 0.897690\n",
      "train_loss: 0.431007, train_accuracy: 0.900990\n",
      "test_loss: 0.438600, test_accuracy: 0.899340\n",
      "train_loss: 0.430966, train_accuracy: 0.900990\n",
      "test_loss: 0.438583, test_accuracy: 0.902640\n",
      "train_loss: 0.430926, train_accuracy: 0.902640\n",
      "test_loss: 0.438566, test_accuracy: 0.904290\n",
      "train_loss: 0.430886, train_accuracy: 0.902640\n",
      "test_loss: 0.438549, test_accuracy: 0.904290\n",
      "train_loss: 0.430845, train_accuracy: 0.902640\n",
      "test_loss: 0.438532, test_accuracy: 0.904290\n",
      "train_loss: 0.430805, train_accuracy: 0.904290\n",
      "test_loss: 0.438515, test_accuracy: 0.904290\n",
      "train_loss: 0.430766, train_accuracy: 0.904290\n",
      "test_loss: 0.438498, test_accuracy: 0.904290\n",
      "train_loss: 0.430726, train_accuracy: 0.905941\n",
      "test_loss: 0.438481, test_accuracy: 0.904290\n",
      "train_loss: 0.430687, train_accuracy: 0.905941\n",
      "test_loss: 0.438464, test_accuracy: 0.904290\n",
      "train_loss: 0.430647, train_accuracy: 0.905941\n",
      "test_loss: 0.438446, test_accuracy: 0.904290\n",
      "train_loss: 0.430608, train_accuracy: 0.905941\n",
      "test_loss: 0.438429, test_accuracy: 0.904290\n",
      "train_loss: 0.430569, train_accuracy: 0.905941\n",
      "test_loss: 0.438411, test_accuracy: 0.904290\n",
      "train_loss: 0.430531, train_accuracy: 0.905941\n",
      "test_loss: 0.438394, test_accuracy: 0.904290\n",
      "train_loss: 0.430492, train_accuracy: 0.907591\n",
      "test_loss: 0.438376, test_accuracy: 0.904290\n",
      "train_loss: 0.430453, train_accuracy: 0.907591\n",
      "test_loss: 0.438359, test_accuracy: 0.904290\n",
      "train_loss: 0.430415, train_accuracy: 0.907591\n",
      "test_loss: 0.438341, test_accuracy: 0.904290\n",
      "train_loss: 0.430377, train_accuracy: 0.907591\n",
      "test_loss: 0.438324, test_accuracy: 0.904290\n",
      "train_loss: 0.430339, train_accuracy: 0.909241\n",
      "test_loss: 0.438306, test_accuracy: 0.904290\n",
      "train_loss: 0.430301, train_accuracy: 0.909241\n",
      "test_loss: 0.438288, test_accuracy: 0.904290\n",
      "train_loss: 0.430263, train_accuracy: 0.909241\n",
      "test_loss: 0.438271, test_accuracy: 0.904290\n",
      "train_loss: 0.430226, train_accuracy: 0.910891\n",
      "test_loss: 0.438253, test_accuracy: 0.905941\n",
      "train_loss: 0.430189, train_accuracy: 0.912541\n",
      "test_loss: 0.438235, test_accuracy: 0.905941\n",
      "train_loss: 0.430151, train_accuracy: 0.912541\n",
      "test_loss: 0.438217, test_accuracy: 0.905941\n",
      "train_loss: 0.430114, train_accuracy: 0.912541\n",
      "test_loss: 0.438200, test_accuracy: 0.907591\n",
      "train_loss: 0.430077, train_accuracy: 0.912541\n",
      "test_loss: 0.438182, test_accuracy: 0.907591\n",
      "train_loss: 0.430041, train_accuracy: 0.912541\n",
      "test_loss: 0.438164, test_accuracy: 0.907591\n",
      "train_loss: 0.430004, train_accuracy: 0.912541\n",
      "test_loss: 0.438146, test_accuracy: 0.907591\n",
      "train_loss: 0.429967, train_accuracy: 0.914191\n",
      "test_loss: 0.438129, test_accuracy: 0.907591\n",
      "train_loss: 0.429931, train_accuracy: 0.914191\n",
      "test_loss: 0.438111, test_accuracy: 0.907591\n",
      "train_loss: 0.429895, train_accuracy: 0.912541\n",
      "test_loss: 0.438093, test_accuracy: 0.907591\n",
      "train_loss: 0.429859, train_accuracy: 0.914191\n",
      "test_loss: 0.438076, test_accuracy: 0.905941\n",
      "train_loss: 0.429823, train_accuracy: 0.914191\n",
      "test_loss: 0.438058, test_accuracy: 0.905941\n",
      "train_loss: 0.429787, train_accuracy: 0.914191\n",
      "test_loss: 0.438040, test_accuracy: 0.905941\n",
      "train_loss: 0.429751, train_accuracy: 0.915842\n",
      "test_loss: 0.438023, test_accuracy: 0.905941\n",
      "train_loss: 0.429716, train_accuracy: 0.915842\n",
      "test_loss: 0.438005, test_accuracy: 0.905941\n",
      "train_loss: 0.429680, train_accuracy: 0.915842\n",
      "test_loss: 0.437988, test_accuracy: 0.907591\n",
      "train_loss: 0.429645, train_accuracy: 0.915842\n",
      "test_loss: 0.437970, test_accuracy: 0.907591\n",
      "train_loss: 0.429610, train_accuracy: 0.915842\n",
      "test_loss: 0.437953, test_accuracy: 0.907591\n",
      "train_loss: 0.429575, train_accuracy: 0.915842\n",
      "test_loss: 0.437935, test_accuracy: 0.907591\n",
      "train_loss: 0.429540, train_accuracy: 0.915842\n",
      "test_loss: 0.437918, test_accuracy: 0.907591\n",
      "train_loss: 0.429505, train_accuracy: 0.915842\n",
      "test_loss: 0.437901, test_accuracy: 0.907591\n",
      "train_loss: 0.429471, train_accuracy: 0.915842\n",
      "test_loss: 0.437884, test_accuracy: 0.907591\n",
      "train_loss: 0.429436, train_accuracy: 0.915842\n",
      "test_loss: 0.437867, test_accuracy: 0.907591\n",
      "train_loss: 0.429402, train_accuracy: 0.917492\n",
      "test_loss: 0.437850, test_accuracy: 0.907591\n",
      "train_loss: 0.429368, train_accuracy: 0.917492\n",
      "test_loss: 0.437833, test_accuracy: 0.907591\n",
      "train_loss: 0.429334, train_accuracy: 0.917492\n",
      "test_loss: 0.437816, test_accuracy: 0.907591\n",
      "train_loss: 0.429300, train_accuracy: 0.917492\n",
      "test_loss: 0.437799, test_accuracy: 0.907591\n",
      "train_loss: 0.429266, train_accuracy: 0.917492\n",
      "test_loss: 0.437782, test_accuracy: 0.909241\n",
      "train_loss: 0.429232, train_accuracy: 0.917492\n",
      "test_loss: 0.437765, test_accuracy: 0.909241\n",
      "train_loss: 0.429199, train_accuracy: 0.917492\n",
      "test_loss: 0.437749, test_accuracy: 0.909241\n",
      "train_loss: 0.429165, train_accuracy: 0.917492\n",
      "test_loss: 0.437732, test_accuracy: 0.909241\n",
      "train_loss: 0.429132, train_accuracy: 0.917492\n",
      "test_loss: 0.437716, test_accuracy: 0.912541\n",
      "train_loss: 0.429098, train_accuracy: 0.919142\n",
      "test_loss: 0.437700, test_accuracy: 0.912541\n",
      "train_loss: 0.429065, train_accuracy: 0.919142\n",
      "test_loss: 0.437683, test_accuracy: 0.912541\n",
      "train_loss: 0.429032, train_accuracy: 0.919142\n",
      "test_loss: 0.437667, test_accuracy: 0.912541\n",
      "train_loss: 0.428999, train_accuracy: 0.919142\n",
      "test_loss: 0.437651, test_accuracy: 0.912541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.428967, train_accuracy: 0.920792\n",
      "test_loss: 0.437635, test_accuracy: 0.912541\n",
      "train_loss: 0.428934, train_accuracy: 0.920792\n",
      "test_loss: 0.437620, test_accuracy: 0.912541\n",
      "train_loss: 0.428901, train_accuracy: 0.920792\n",
      "test_loss: 0.437604, test_accuracy: 0.912541\n",
      "train_loss: 0.428869, train_accuracy: 0.920792\n",
      "test_loss: 0.437588, test_accuracy: 0.912541\n",
      "train_loss: 0.428837, train_accuracy: 0.920792\n",
      "test_loss: 0.437573, test_accuracy: 0.912541\n",
      "train_loss: 0.428804, train_accuracy: 0.920792\n",
      "test_loss: 0.437558, test_accuracy: 0.912541\n",
      "train_loss: 0.428772, train_accuracy: 0.920792\n",
      "test_loss: 0.437542, test_accuracy: 0.912541\n",
      "train_loss: 0.428740, train_accuracy: 0.920792\n",
      "test_loss: 0.437527, test_accuracy: 0.912541\n",
      "train_loss: 0.428708, train_accuracy: 0.920792\n",
      "test_loss: 0.437512, test_accuracy: 0.912541\n",
      "train_loss: 0.428677, train_accuracy: 0.922442\n",
      "test_loss: 0.437497, test_accuracy: 0.912541\n",
      "train_loss: 0.428645, train_accuracy: 0.922442\n",
      "test_loss: 0.437483, test_accuracy: 0.912541\n",
      "train_loss: 0.428613, train_accuracy: 0.922442\n",
      "test_loss: 0.437468, test_accuracy: 0.912541\n",
      "train_loss: 0.428582, train_accuracy: 0.922442\n",
      "test_loss: 0.437454, test_accuracy: 0.912541\n",
      "train_loss: 0.428551, train_accuracy: 0.922442\n",
      "test_loss: 0.437439, test_accuracy: 0.912541\n",
      "train_loss: 0.428519, train_accuracy: 0.922442\n",
      "test_loss: 0.437425, test_accuracy: 0.912541\n",
      "train_loss: 0.428488, train_accuracy: 0.922442\n",
      "test_loss: 0.437411, test_accuracy: 0.912541\n",
      "train_loss: 0.428457, train_accuracy: 0.920792\n",
      "test_loss: 0.437397, test_accuracy: 0.912541\n",
      "train_loss: 0.428426, train_accuracy: 0.920792\n",
      "test_loss: 0.437383, test_accuracy: 0.912541\n",
      "train_loss: 0.428395, train_accuracy: 0.920792\n",
      "test_loss: 0.437370, test_accuracy: 0.912541\n",
      "train_loss: 0.428365, train_accuracy: 0.920792\n",
      "test_loss: 0.437356, test_accuracy: 0.912541\n",
      "train_loss: 0.428334, train_accuracy: 0.922442\n",
      "test_loss: 0.437343, test_accuracy: 0.912541\n",
      "train_loss: 0.428303, train_accuracy: 0.922442\n",
      "test_loss: 0.437329, test_accuracy: 0.912541\n",
      "train_loss: 0.428273, train_accuracy: 0.922442\n",
      "test_loss: 0.437316, test_accuracy: 0.914191\n",
      "train_loss: 0.428243, train_accuracy: 0.922442\n",
      "test_loss: 0.437303, test_accuracy: 0.915842\n",
      "train_loss: 0.428212, train_accuracy: 0.922442\n",
      "test_loss: 0.437291, test_accuracy: 0.915842\n",
      "train_loss: 0.428182, train_accuracy: 0.924092\n",
      "test_loss: 0.437278, test_accuracy: 0.917492\n",
      "train_loss: 0.428152, train_accuracy: 0.924092\n",
      "test_loss: 0.437265, test_accuracy: 0.917492\n",
      "train_loss: 0.428122, train_accuracy: 0.925743\n",
      "test_loss: 0.437253, test_accuracy: 0.917492\n",
      "train_loss: 0.428092, train_accuracy: 0.927393\n",
      "test_loss: 0.437241, test_accuracy: 0.917492\n",
      "train_loss: 0.428063, train_accuracy: 0.927393\n",
      "test_loss: 0.437229, test_accuracy: 0.917492\n",
      "train_loss: 0.428033, train_accuracy: 0.927393\n",
      "test_loss: 0.437217, test_accuracy: 0.917492\n",
      "train_loss: 0.428003, train_accuracy: 0.927393\n",
      "test_loss: 0.437205, test_accuracy: 0.917492\n",
      "train_loss: 0.427974, train_accuracy: 0.927393\n",
      "test_loss: 0.437193, test_accuracy: 0.917492\n",
      "train_loss: 0.427945, train_accuracy: 0.927393\n",
      "test_loss: 0.437182, test_accuracy: 0.917492\n",
      "train_loss: 0.427915, train_accuracy: 0.929043\n",
      "test_loss: 0.437171, test_accuracy: 0.917492\n",
      "train_loss: 0.427886, train_accuracy: 0.929043\n",
      "test_loss: 0.437159, test_accuracy: 0.917492\n",
      "train_loss: 0.427857, train_accuracy: 0.929043\n",
      "test_loss: 0.437148, test_accuracy: 0.917492\n",
      "train_loss: 0.427828, train_accuracy: 0.929043\n",
      "test_loss: 0.437138, test_accuracy: 0.917492\n",
      "train_loss: 0.427799, train_accuracy: 0.929043\n",
      "test_loss: 0.437127, test_accuracy: 0.915842\n",
      "train_loss: 0.427770, train_accuracy: 0.929043\n",
      "test_loss: 0.437116, test_accuracy: 0.915842\n",
      "train_loss: 0.427742, train_accuracy: 0.929043\n",
      "test_loss: 0.437106, test_accuracy: 0.915842\n",
      "train_loss: 0.427713, train_accuracy: 0.929043\n",
      "test_loss: 0.437096, test_accuracy: 0.915842\n",
      "train_loss: 0.427684, train_accuracy: 0.930693\n",
      "test_loss: 0.437086, test_accuracy: 0.915842\n",
      "train_loss: 0.427656, train_accuracy: 0.930693\n",
      "test_loss: 0.437076, test_accuracy: 0.915842\n",
      "train_loss: 0.427627, train_accuracy: 0.930693\n",
      "test_loss: 0.437066, test_accuracy: 0.915842\n",
      "train_loss: 0.427599, train_accuracy: 0.930693\n",
      "test_loss: 0.437056, test_accuracy: 0.915842\n",
      "train_loss: 0.427571, train_accuracy: 0.930693\n",
      "test_loss: 0.437047, test_accuracy: 0.915842\n",
      "train_loss: 0.427543, train_accuracy: 0.930693\n",
      "test_loss: 0.437037, test_accuracy: 0.915842\n",
      "train_loss: 0.427515, train_accuracy: 0.930693\n",
      "test_loss: 0.437028, test_accuracy: 0.915842\n",
      "train_loss: 0.427487, train_accuracy: 0.930693\n",
      "test_loss: 0.437019, test_accuracy: 0.915842\n",
      "train_loss: 0.427459, train_accuracy: 0.930693\n",
      "test_loss: 0.437010, test_accuracy: 0.917492\n",
      "train_loss: 0.427431, train_accuracy: 0.930693\n",
      "test_loss: 0.437002, test_accuracy: 0.917492\n",
      "train_loss: 0.427403, train_accuracy: 0.930693\n",
      "test_loss: 0.436993, test_accuracy: 0.917492\n",
      "train_loss: 0.427376, train_accuracy: 0.930693\n",
      "test_loss: 0.436985, test_accuracy: 0.917492\n",
      "train_loss: 0.427348, train_accuracy: 0.930693\n",
      "test_loss: 0.436977, test_accuracy: 0.917492\n",
      "train_loss: 0.427321, train_accuracy: 0.930693\n",
      "test_loss: 0.436968, test_accuracy: 0.917492\n",
      "train_loss: 0.427294, train_accuracy: 0.930693\n",
      "test_loss: 0.436961, test_accuracy: 0.917492\n",
      "train_loss: 0.427266, train_accuracy: 0.930693\n",
      "test_loss: 0.436953, test_accuracy: 0.915842\n",
      "train_loss: 0.427239, train_accuracy: 0.930693\n",
      "test_loss: 0.436945, test_accuracy: 0.915842\n",
      "train_loss: 0.427212, train_accuracy: 0.930693\n",
      "test_loss: 0.436937, test_accuracy: 0.915842\n",
      "train_loss: 0.427185, train_accuracy: 0.930693\n",
      "test_loss: 0.436930, test_accuracy: 0.915842\n",
      "train_loss: 0.427158, train_accuracy: 0.930693\n",
      "test_loss: 0.436923, test_accuracy: 0.917492\n",
      "train_loss: 0.427131, train_accuracy: 0.929043\n",
      "test_loss: 0.436916, test_accuracy: 0.917492\n",
      "train_loss: 0.427104, train_accuracy: 0.929043\n",
      "test_loss: 0.436909, test_accuracy: 0.917492\n",
      "train_loss: 0.427077, train_accuracy: 0.929043\n",
      "test_loss: 0.436902, test_accuracy: 0.917492\n",
      "train_loss: 0.427051, train_accuracy: 0.929043\n",
      "test_loss: 0.436895, test_accuracy: 0.917492\n",
      "train_loss: 0.427024, train_accuracy: 0.929043\n",
      "test_loss: 0.436889, test_accuracy: 0.917492\n",
      "train_loss: 0.426998, train_accuracy: 0.929043\n",
      "test_loss: 0.436883, test_accuracy: 0.917492\n",
      "train_loss: 0.426971, train_accuracy: 0.929043\n",
      "test_loss: 0.436877, test_accuracy: 0.917492\n",
      "train_loss: 0.426945, train_accuracy: 0.930693\n",
      "test_loss: 0.436870, test_accuracy: 0.917492\n",
      "train_loss: 0.426919, train_accuracy: 0.930693\n",
      "test_loss: 0.436864, test_accuracy: 0.917492\n",
      "train_loss: 0.426893, train_accuracy: 0.930693\n",
      "test_loss: 0.436859, test_accuracy: 0.917492\n",
      "train_loss: 0.426867, train_accuracy: 0.930693\n",
      "test_loss: 0.436853, test_accuracy: 0.917492\n",
      "train_loss: 0.426840, train_accuracy: 0.930693\n",
      "test_loss: 0.436848, test_accuracy: 0.917492\n",
      "train_loss: 0.426814, train_accuracy: 0.930693\n",
      "test_loss: 0.436842, test_accuracy: 0.917492\n",
      "train_loss: 0.426789, train_accuracy: 0.930693\n",
      "test_loss: 0.436837, test_accuracy: 0.917492\n",
      "train_loss: 0.426763, train_accuracy: 0.930693\n",
      "test_loss: 0.436832, test_accuracy: 0.917492\n",
      "train_loss: 0.426737, train_accuracy: 0.930693\n",
      "test_loss: 0.436827, test_accuracy: 0.917492\n",
      "train_loss: 0.426711, train_accuracy: 0.930693\n",
      "test_loss: 0.436822, test_accuracy: 0.917492\n",
      "train_loss: 0.426686, train_accuracy: 0.930693\n",
      "test_loss: 0.436817, test_accuracy: 0.917492\n",
      "train_loss: 0.426660, train_accuracy: 0.930693\n",
      "test_loss: 0.436813, test_accuracy: 0.917492\n",
      "train_loss: 0.426635, train_accuracy: 0.930693\n",
      "test_loss: 0.436808, test_accuracy: 0.917492\n",
      "train_loss: 0.426610, train_accuracy: 0.930693\n",
      "test_loss: 0.436804, test_accuracy: 0.917492\n",
      "train_loss: 0.426584, train_accuracy: 0.930693\n",
      "test_loss: 0.436799, test_accuracy: 0.917492\n",
      "train_loss: 0.426559, train_accuracy: 0.930693\n",
      "test_loss: 0.436795, test_accuracy: 0.917492\n",
      "train_loss: 0.426534, train_accuracy: 0.932343\n",
      "test_loss: 0.436792, test_accuracy: 0.917492\n",
      "train_loss: 0.426509, train_accuracy: 0.933993\n",
      "test_loss: 0.436788, test_accuracy: 0.917492\n",
      "train_loss: 0.426484, train_accuracy: 0.933993\n",
      "test_loss: 0.436784, test_accuracy: 0.917492\n",
      "train_loss: 0.426459, train_accuracy: 0.935644\n",
      "test_loss: 0.436780, test_accuracy: 0.917492\n",
      "train_loss: 0.426434, train_accuracy: 0.935644\n",
      "test_loss: 0.436777, test_accuracy: 0.917492\n",
      "train_loss: 0.426409, train_accuracy: 0.935644\n",
      "test_loss: 0.436773, test_accuracy: 0.917492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.426385, train_accuracy: 0.935644\n",
      "test_loss: 0.436770, test_accuracy: 0.917492\n",
      "train_loss: 0.426360, train_accuracy: 0.935644\n",
      "test_loss: 0.436767, test_accuracy: 0.917492\n",
      "train_loss: 0.426335, train_accuracy: 0.935644\n",
      "test_loss: 0.436764, test_accuracy: 0.917492\n",
      "train_loss: 0.426311, train_accuracy: 0.935644\n",
      "test_loss: 0.436761, test_accuracy: 0.917492\n",
      "train_loss: 0.426286, train_accuracy: 0.935644\n",
      "test_loss: 0.436758, test_accuracy: 0.915842\n",
      "train_loss: 0.426262, train_accuracy: 0.935644\n",
      "test_loss: 0.436755, test_accuracy: 0.915842\n",
      "train_loss: 0.426238, train_accuracy: 0.935644\n",
      "test_loss: 0.436753, test_accuracy: 0.915842\n",
      "train_loss: 0.426213, train_accuracy: 0.935644\n",
      "test_loss: 0.436750, test_accuracy: 0.915842\n",
      "train_loss: 0.426189, train_accuracy: 0.935644\n",
      "test_loss: 0.436748, test_accuracy: 0.915842\n",
      "train_loss: 0.426165, train_accuracy: 0.935644\n",
      "test_loss: 0.436746, test_accuracy: 0.915842\n",
      "train_loss: 0.426141, train_accuracy: 0.935644\n",
      "test_loss: 0.436743, test_accuracy: 0.915842\n",
      "train_loss: 0.426117, train_accuracy: 0.935644\n",
      "test_loss: 0.436741, test_accuracy: 0.915842\n",
      "train_loss: 0.426093, train_accuracy: 0.935644\n",
      "test_loss: 0.436739, test_accuracy: 0.915842\n",
      "train_loss: 0.426070, train_accuracy: 0.935644\n",
      "test_loss: 0.436738, test_accuracy: 0.915842\n",
      "train_loss: 0.426046, train_accuracy: 0.935644\n",
      "test_loss: 0.436736, test_accuracy: 0.915842\n",
      "train_loss: 0.426022, train_accuracy: 0.935644\n",
      "test_loss: 0.436734, test_accuracy: 0.915842\n",
      "train_loss: 0.425998, train_accuracy: 0.935644\n",
      "test_loss: 0.436732, test_accuracy: 0.915842\n",
      "train_loss: 0.425975, train_accuracy: 0.935644\n",
      "test_loss: 0.436731, test_accuracy: 0.915842\n",
      "train_loss: 0.425951, train_accuracy: 0.937294\n",
      "test_loss: 0.436730, test_accuracy: 0.915842\n",
      "train_loss: 0.425928, train_accuracy: 0.937294\n",
      "test_loss: 0.436728, test_accuracy: 0.915842\n",
      "train_loss: 0.425905, train_accuracy: 0.937294\n",
      "test_loss: 0.436727, test_accuracy: 0.915842\n",
      "train_loss: 0.425881, train_accuracy: 0.937294\n",
      "test_loss: 0.436726, test_accuracy: 0.915842\n",
      "train_loss: 0.425858, train_accuracy: 0.937294\n",
      "test_loss: 0.436725, test_accuracy: 0.915842\n",
      "train_loss: 0.425835, train_accuracy: 0.938944\n",
      "test_loss: 0.436724, test_accuracy: 0.915842\n",
      "train_loss: 0.425812, train_accuracy: 0.938944\n",
      "test_loss: 0.436723, test_accuracy: 0.915842\n",
      "train_loss: 0.425789, train_accuracy: 0.938944\n",
      "test_loss: 0.436722, test_accuracy: 0.915842\n",
      "train_loss: 0.425766, train_accuracy: 0.938944\n",
      "test_loss: 0.436721, test_accuracy: 0.915842\n",
      "train_loss: 0.425743, train_accuracy: 0.938944\n",
      "test_loss: 0.436721, test_accuracy: 0.915842\n",
      "train_loss: 0.425720, train_accuracy: 0.938944\n",
      "test_loss: 0.436720, test_accuracy: 0.915842\n",
      "train_loss: 0.425697, train_accuracy: 0.938944\n",
      "test_loss: 0.436720, test_accuracy: 0.915842\n",
      "train_loss: 0.425674, train_accuracy: 0.938944\n",
      "test_loss: 0.436719, test_accuracy: 0.915842\n",
      "train_loss: 0.425652, train_accuracy: 0.938944\n",
      "test_loss: 0.436719, test_accuracy: 0.915842\n",
      "train_loss: 0.425629, train_accuracy: 0.938944\n",
      "test_loss: 0.436719, test_accuracy: 0.915842\n",
      "train_loss: 0.425607, train_accuracy: 0.938944\n",
      "test_loss: 0.436718, test_accuracy: 0.915842\n",
      "train_loss: 0.425584, train_accuracy: 0.938944\n",
      "test_loss: 0.436718, test_accuracy: 0.915842\n",
      "train_loss: 0.425562, train_accuracy: 0.938944\n",
      "test_loss: 0.436718, test_accuracy: 0.915842\n",
      "train_loss: 0.425539, train_accuracy: 0.938944\n",
      "test_loss: 0.436718, test_accuracy: 0.915842\n",
      "train_loss: 0.425517, train_accuracy: 0.938944\n",
      "test_loss: 0.436718, test_accuracy: 0.915842\n",
      "train_loss: 0.425495, train_accuracy: 0.938944\n",
      "test_loss: 0.436718, test_accuracy: 0.915842\n",
      "train_loss: 0.425473, train_accuracy: 0.938944\n",
      "test_loss: 0.436719, test_accuracy: 0.917492\n",
      "train_loss: 0.425451, train_accuracy: 0.938944\n",
      "test_loss: 0.436719, test_accuracy: 0.917492\n",
      "train_loss: 0.425429, train_accuracy: 0.938944\n",
      "test_loss: 0.436719, test_accuracy: 0.917492\n",
      "train_loss: 0.425407, train_accuracy: 0.938944\n",
      "test_loss: 0.436720, test_accuracy: 0.917492\n",
      "train_loss: 0.425385, train_accuracy: 0.938944\n",
      "test_loss: 0.436720, test_accuracy: 0.917492\n",
      "train_loss: 0.425363, train_accuracy: 0.938944\n",
      "test_loss: 0.436721, test_accuracy: 0.917492\n",
      "train_loss: 0.425341, train_accuracy: 0.938944\n",
      "test_loss: 0.436721, test_accuracy: 0.917492\n",
      "train_loss: 0.425319, train_accuracy: 0.938944\n",
      "test_loss: 0.436722, test_accuracy: 0.917492\n",
      "train_loss: 0.425297, train_accuracy: 0.938944\n",
      "test_loss: 0.436723, test_accuracy: 0.917492\n",
      "train_loss: 0.425276, train_accuracy: 0.938944\n",
      "test_loss: 0.436723, test_accuracy: 0.917492\n",
      "train_loss: 0.425254, train_accuracy: 0.938944\n",
      "test_loss: 0.436724, test_accuracy: 0.915842\n",
      "train_loss: 0.425233, train_accuracy: 0.938944\n",
      "test_loss: 0.436725, test_accuracy: 0.915842\n",
      "train_loss: 0.425211, train_accuracy: 0.938944\n",
      "test_loss: 0.436726, test_accuracy: 0.915842\n",
      "train_loss: 0.425190, train_accuracy: 0.937294\n",
      "test_loss: 0.436727, test_accuracy: 0.915842\n",
      "train_loss: 0.425169, train_accuracy: 0.937294\n",
      "test_loss: 0.436728, test_accuracy: 0.915842\n",
      "train_loss: 0.425147, train_accuracy: 0.937294\n",
      "test_loss: 0.436729, test_accuracy: 0.915842\n",
      "train_loss: 0.425126, train_accuracy: 0.937294\n",
      "test_loss: 0.436730, test_accuracy: 0.915842\n",
      "train_loss: 0.425105, train_accuracy: 0.937294\n",
      "test_loss: 0.436732, test_accuracy: 0.915842\n",
      "train_loss: 0.425084, train_accuracy: 0.937294\n",
      "test_loss: 0.436733, test_accuracy: 0.915842\n",
      "train_loss: 0.425063, train_accuracy: 0.937294\n",
      "test_loss: 0.436734, test_accuracy: 0.915842\n",
      "train_loss: 0.425042, train_accuracy: 0.937294\n",
      "test_loss: 0.436735, test_accuracy: 0.915842\n",
      "train_loss: 0.425021, train_accuracy: 0.937294\n",
      "test_loss: 0.436737, test_accuracy: 0.915842\n",
      "train_loss: 0.425000, train_accuracy: 0.937294\n",
      "test_loss: 0.436738, test_accuracy: 0.915842\n",
      "train_loss: 0.424979, train_accuracy: 0.937294\n",
      "test_loss: 0.436740, test_accuracy: 0.915842\n",
      "train_loss: 0.424958, train_accuracy: 0.937294\n",
      "test_loss: 0.436741, test_accuracy: 0.915842\n",
      "train_loss: 0.424938, train_accuracy: 0.937294\n",
      "test_loss: 0.436743, test_accuracy: 0.917492\n",
      "train_loss: 0.424917, train_accuracy: 0.937294\n",
      "test_loss: 0.436744, test_accuracy: 0.917492\n",
      "train_loss: 0.424896, train_accuracy: 0.937294\n",
      "test_loss: 0.436746, test_accuracy: 0.917492\n",
      "train_loss: 0.424876, train_accuracy: 0.937294\n",
      "test_loss: 0.436748, test_accuracy: 0.917492\n",
      "train_loss: 0.424855, train_accuracy: 0.937294\n",
      "test_loss: 0.436749, test_accuracy: 0.917492\n",
      "train_loss: 0.424835, train_accuracy: 0.937294\n",
      "test_loss: 0.436751, test_accuracy: 0.917492\n",
      "train_loss: 0.424814, train_accuracy: 0.937294\n",
      "test_loss: 0.436753, test_accuracy: 0.917492\n",
      "train_loss: 0.424794, train_accuracy: 0.937294\n",
      "test_loss: 0.436755, test_accuracy: 0.917492\n",
      "train_loss: 0.424774, train_accuracy: 0.937294\n",
      "test_loss: 0.436756, test_accuracy: 0.917492\n",
      "train_loss: 0.424754, train_accuracy: 0.935644\n",
      "test_loss: 0.436758, test_accuracy: 0.917492\n",
      "train_loss: 0.424733, train_accuracy: 0.935644\n",
      "test_loss: 0.436760, test_accuracy: 0.917492\n",
      "train_loss: 0.424713, train_accuracy: 0.935644\n",
      "test_loss: 0.436762, test_accuracy: 0.917492\n",
      "train_loss: 0.424693, train_accuracy: 0.935644\n",
      "test_loss: 0.436764, test_accuracy: 0.917492\n",
      "train_loss: 0.424673, train_accuracy: 0.933993\n",
      "test_loss: 0.436766, test_accuracy: 0.917492\n",
      "train_loss: 0.424653, train_accuracy: 0.933993\n",
      "test_loss: 0.436768, test_accuracy: 0.917492\n",
      "train_loss: 0.424633, train_accuracy: 0.933993\n",
      "test_loss: 0.436770, test_accuracy: 0.917492\n",
      "train_loss: 0.424614, train_accuracy: 0.933993\n",
      "test_loss: 0.436772, test_accuracy: 0.917492\n",
      "train_loss: 0.424594, train_accuracy: 0.933993\n",
      "test_loss: 0.436774, test_accuracy: 0.917492\n",
      "train_loss: 0.424574, train_accuracy: 0.933993\n",
      "test_loss: 0.436777, test_accuracy: 0.917492\n",
      "train_loss: 0.424554, train_accuracy: 0.933993\n",
      "test_loss: 0.436779, test_accuracy: 0.917492\n",
      "train_loss: 0.424535, train_accuracy: 0.933993\n",
      "test_loss: 0.436781, test_accuracy: 0.917492\n",
      "train_loss: 0.424515, train_accuracy: 0.933993\n",
      "test_loss: 0.436783, test_accuracy: 0.917492\n",
      "train_loss: 0.424496, train_accuracy: 0.933993\n",
      "test_loss: 0.436786, test_accuracy: 0.917492\n",
      "train_loss: 0.424476, train_accuracy: 0.933993\n",
      "test_loss: 0.436788, test_accuracy: 0.917492\n",
      "train_loss: 0.424457, train_accuracy: 0.933993\n",
      "test_loss: 0.436790, test_accuracy: 0.917492\n",
      "train_loss: 0.424437, train_accuracy: 0.933993\n",
      "test_loss: 0.436793, test_accuracy: 0.917492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.424418, train_accuracy: 0.933993\n",
      "test_loss: 0.436795, test_accuracy: 0.917492\n",
      "train_loss: 0.424399, train_accuracy: 0.933993\n",
      "test_loss: 0.436797, test_accuracy: 0.917492\n",
      "train_loss: 0.424380, train_accuracy: 0.933993\n",
      "test_loss: 0.436800, test_accuracy: 0.917492\n",
      "train_loss: 0.424360, train_accuracy: 0.933993\n",
      "test_loss: 0.436802, test_accuracy: 0.917492\n",
      "train_loss: 0.424341, train_accuracy: 0.933993\n",
      "test_loss: 0.436805, test_accuracy: 0.917492\n",
      "train_loss: 0.424322, train_accuracy: 0.933993\n",
      "test_loss: 0.436807, test_accuracy: 0.917492\n",
      "train_loss: 0.424303, train_accuracy: 0.933993\n",
      "test_loss: 0.436810, test_accuracy: 0.917492\n",
      "train_loss: 0.424284, train_accuracy: 0.933993\n",
      "test_loss: 0.436812, test_accuracy: 0.917492\n",
      "train_loss: 0.424265, train_accuracy: 0.933993\n",
      "test_loss: 0.436815, test_accuracy: 0.917492\n",
      "train_loss: 0.424247, train_accuracy: 0.933993\n",
      "test_loss: 0.436817, test_accuracy: 0.917492\n",
      "train_loss: 0.424228, train_accuracy: 0.933993\n",
      "test_loss: 0.436820, test_accuracy: 0.917492\n",
      "train_loss: 0.424209, train_accuracy: 0.933993\n",
      "test_loss: 0.436822, test_accuracy: 0.917492\n",
      "train_loss: 0.424190, train_accuracy: 0.933993\n",
      "test_loss: 0.436825, test_accuracy: 0.917492\n",
      "train_loss: 0.424172, train_accuracy: 0.933993\n",
      "test_loss: 0.436828, test_accuracy: 0.917492\n",
      "train_loss: 0.424153, train_accuracy: 0.933993\n",
      "test_loss: 0.436830, test_accuracy: 0.917492\n",
      "train_loss: 0.424134, train_accuracy: 0.933993\n",
      "test_loss: 0.436833, test_accuracy: 0.919142\n",
      "train_loss: 0.424116, train_accuracy: 0.933993\n",
      "test_loss: 0.436836, test_accuracy: 0.919142\n",
      "train_loss: 0.424097, train_accuracy: 0.933993\n",
      "test_loss: 0.436838, test_accuracy: 0.919142\n",
      "train_loss: 0.424079, train_accuracy: 0.933993\n",
      "test_loss: 0.436841, test_accuracy: 0.919142\n",
      "train_loss: 0.424061, train_accuracy: 0.933993\n",
      "test_loss: 0.436844, test_accuracy: 0.919142\n",
      "train_loss: 0.424042, train_accuracy: 0.933993\n",
      "test_loss: 0.436847, test_accuracy: 0.919142\n",
      "train_loss: 0.424024, train_accuracy: 0.933993\n",
      "test_loss: 0.436849, test_accuracy: 0.919142\n",
      "train_loss: 0.424006, train_accuracy: 0.933993\n",
      "test_loss: 0.436852, test_accuracy: 0.919142\n",
      "train_loss: 0.423988, train_accuracy: 0.933993\n",
      "test_loss: 0.436855, test_accuracy: 0.919142\n",
      "train_loss: 0.423970, train_accuracy: 0.933993\n",
      "test_loss: 0.436858, test_accuracy: 0.919142\n",
      "train_loss: 0.423952, train_accuracy: 0.933993\n",
      "test_loss: 0.436861, test_accuracy: 0.919142\n",
      "train_loss: 0.423933, train_accuracy: 0.933993\n",
      "test_loss: 0.436863, test_accuracy: 0.919142\n",
      "train_loss: 0.423916, train_accuracy: 0.933993\n",
      "test_loss: 0.436866, test_accuracy: 0.919142\n",
      "train_loss: 0.423898, train_accuracy: 0.933993\n",
      "test_loss: 0.436869, test_accuracy: 0.919142\n",
      "train_loss: 0.423880, train_accuracy: 0.933993\n",
      "test_loss: 0.436872, test_accuracy: 0.919142\n",
      "train_loss: 0.423862, train_accuracy: 0.933993\n",
      "test_loss: 0.436875, test_accuracy: 0.919142\n",
      "train_loss: 0.423844, train_accuracy: 0.933993\n",
      "test_loss: 0.436878, test_accuracy: 0.919142\n",
      "train_loss: 0.423826, train_accuracy: 0.933993\n",
      "test_loss: 0.436881, test_accuracy: 0.919142\n",
      "train_loss: 0.423809, train_accuracy: 0.933993\n",
      "test_loss: 0.436884, test_accuracy: 0.919142\n",
      "train_loss: 0.423791, train_accuracy: 0.933993\n",
      "test_loss: 0.436887, test_accuracy: 0.919142\n",
      "train_loss: 0.423773, train_accuracy: 0.933993\n",
      "test_loss: 0.436890, test_accuracy: 0.919142\n",
      "train_loss: 0.423756, train_accuracy: 0.933993\n",
      "test_loss: 0.436893, test_accuracy: 0.919142\n",
      "train_loss: 0.423738, train_accuracy: 0.933993\n",
      "test_loss: 0.436896, test_accuracy: 0.919142\n",
      "train_loss: 0.423721, train_accuracy: 0.933993\n",
      "test_loss: 0.436899, test_accuracy: 0.919142\n",
      "train_loss: 0.423703, train_accuracy: 0.933993\n",
      "test_loss: 0.436902, test_accuracy: 0.919142\n",
      "train_loss: 0.423686, train_accuracy: 0.933993\n",
      "test_loss: 0.436905, test_accuracy: 0.919142\n",
      "train_loss: 0.423669, train_accuracy: 0.933993\n",
      "test_loss: 0.436908, test_accuracy: 0.919142\n",
      "train_loss: 0.423651, train_accuracy: 0.933993\n",
      "test_loss: 0.436911, test_accuracy: 0.919142\n",
      "train_loss: 0.423634, train_accuracy: 0.933993\n",
      "test_loss: 0.436914, test_accuracy: 0.919142\n",
      "train_loss: 0.423617, train_accuracy: 0.933993\n",
      "test_loss: 0.436917, test_accuracy: 0.919142\n",
      "train_loss: 0.423600, train_accuracy: 0.933993\n",
      "test_loss: 0.436920, test_accuracy: 0.915842\n",
      "train_loss: 0.423583, train_accuracy: 0.933993\n",
      "test_loss: 0.436923, test_accuracy: 0.915842\n",
      "train_loss: 0.423566, train_accuracy: 0.933993\n",
      "test_loss: 0.436926, test_accuracy: 0.915842\n",
      "train_loss: 0.423549, train_accuracy: 0.933993\n",
      "test_loss: 0.436929, test_accuracy: 0.915842\n",
      "train_loss: 0.423532, train_accuracy: 0.933993\n",
      "test_loss: 0.436932, test_accuracy: 0.915842\n",
      "train_loss: 0.423515, train_accuracy: 0.933993\n",
      "test_loss: 0.436935, test_accuracy: 0.915842\n",
      "train_loss: 0.423498, train_accuracy: 0.933993\n",
      "test_loss: 0.436938, test_accuracy: 0.915842\n",
      "train_loss: 0.423481, train_accuracy: 0.933993\n",
      "test_loss: 0.436942, test_accuracy: 0.915842\n",
      "train_loss: 0.423464, train_accuracy: 0.933993\n",
      "test_loss: 0.436945, test_accuracy: 0.915842\n",
      "train_loss: 0.423448, train_accuracy: 0.933993\n",
      "test_loss: 0.436948, test_accuracy: 0.915842\n",
      "train_loss: 0.423431, train_accuracy: 0.933993\n",
      "test_loss: 0.436951, test_accuracy: 0.915842\n",
      "train_loss: 0.423414, train_accuracy: 0.933993\n",
      "test_loss: 0.436954, test_accuracy: 0.915842\n",
      "train_loss: 0.423398, train_accuracy: 0.933993\n",
      "test_loss: 0.436957, test_accuracy: 0.915842\n",
      "train_loss: 0.423381, train_accuracy: 0.933993\n",
      "test_loss: 0.436961, test_accuracy: 0.915842\n",
      "train_loss: 0.423365, train_accuracy: 0.933993\n",
      "test_loss: 0.436964, test_accuracy: 0.915842\n",
      "train_loss: 0.423348, train_accuracy: 0.933993\n",
      "test_loss: 0.436967, test_accuracy: 0.915842\n",
      "train_loss: 0.423332, train_accuracy: 0.933993\n",
      "test_loss: 0.436970, test_accuracy: 0.915842\n",
      "train_loss: 0.423315, train_accuracy: 0.933993\n",
      "test_loss: 0.436973, test_accuracy: 0.915842\n",
      "train_loss: 0.423299, train_accuracy: 0.933993\n",
      "test_loss: 0.436977, test_accuracy: 0.915842\n",
      "train_loss: 0.423283, train_accuracy: 0.935644\n",
      "test_loss: 0.436980, test_accuracy: 0.915842\n",
      "train_loss: 0.423266, train_accuracy: 0.935644\n",
      "test_loss: 0.436983, test_accuracy: 0.915842\n",
      "train_loss: 0.423250, train_accuracy: 0.935644\n",
      "test_loss: 0.436986, test_accuracy: 0.915842\n",
      "train_loss: 0.423234, train_accuracy: 0.935644\n",
      "test_loss: 0.436990, test_accuracy: 0.915842\n",
      "train_loss: 0.423218, train_accuracy: 0.937294\n",
      "test_loss: 0.436993, test_accuracy: 0.915842\n",
      "train_loss: 0.423202, train_accuracy: 0.937294\n",
      "test_loss: 0.436996, test_accuracy: 0.915842\n",
      "train_loss: 0.423186, train_accuracy: 0.937294\n",
      "test_loss: 0.436999, test_accuracy: 0.915842\n",
      "train_loss: 0.423170, train_accuracy: 0.937294\n",
      "test_loss: 0.437003, test_accuracy: 0.915842\n",
      "train_loss: 0.423154, train_accuracy: 0.937294\n",
      "test_loss: 0.437006, test_accuracy: 0.915842\n",
      "train_loss: 0.423138, train_accuracy: 0.937294\n",
      "test_loss: 0.437009, test_accuracy: 0.915842\n",
      "train_loss: 0.423122, train_accuracy: 0.937294\n",
      "test_loss: 0.437013, test_accuracy: 0.915842\n",
      "train_loss: 0.423106, train_accuracy: 0.937294\n",
      "test_loss: 0.437016, test_accuracy: 0.915842\n",
      "train_loss: 0.423090, train_accuracy: 0.937294\n",
      "test_loss: 0.437019, test_accuracy: 0.915842\n",
      "train_loss: 0.423074, train_accuracy: 0.937294\n",
      "test_loss: 0.437023, test_accuracy: 0.915842\n",
      "train_loss: 0.423059, train_accuracy: 0.937294\n",
      "test_loss: 0.437026, test_accuracy: 0.915842\n",
      "train_loss: 0.423043, train_accuracy: 0.937294\n",
      "test_loss: 0.437029, test_accuracy: 0.915842\n",
      "train_loss: 0.423027, train_accuracy: 0.937294\n",
      "test_loss: 0.437033, test_accuracy: 0.915842\n",
      "train_loss: 0.423012, train_accuracy: 0.937294\n",
      "test_loss: 0.437036, test_accuracy: 0.915842\n",
      "train_loss: 0.422996, train_accuracy: 0.937294\n",
      "test_loss: 0.437040, test_accuracy: 0.915842\n",
      "train_loss: 0.422981, train_accuracy: 0.937294\n",
      "test_loss: 0.437043, test_accuracy: 0.915842\n",
      "train_loss: 0.422965, train_accuracy: 0.937294\n",
      "test_loss: 0.437046, test_accuracy: 0.915842\n",
      "train_loss: 0.422950, train_accuracy: 0.937294\n",
      "test_loss: 0.437050, test_accuracy: 0.915842\n",
      "train_loss: 0.422934, train_accuracy: 0.937294\n",
      "test_loss: 0.437053, test_accuracy: 0.915842\n",
      "train_loss: 0.422919, train_accuracy: 0.937294\n",
      "test_loss: 0.437057, test_accuracy: 0.915842\n",
      "train_loss: 0.422904, train_accuracy: 0.937294\n",
      "test_loss: 0.437060, test_accuracy: 0.915842\n",
      "train_loss: 0.422888, train_accuracy: 0.937294\n",
      "test_loss: 0.437064, test_accuracy: 0.915842\n",
      "train_loss: 0.422873, train_accuracy: 0.937294\n",
      "test_loss: 0.437067, test_accuracy: 0.915842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.422858, train_accuracy: 0.937294\n",
      "test_loss: 0.437070, test_accuracy: 0.915842\n",
      "train_loss: 0.422843, train_accuracy: 0.937294\n",
      "test_loss: 0.437074, test_accuracy: 0.915842\n",
      "train_loss: 0.422828, train_accuracy: 0.937294\n",
      "test_loss: 0.437077, test_accuracy: 0.915842\n",
      "train_loss: 0.422813, train_accuracy: 0.937294\n",
      "test_loss: 0.437081, test_accuracy: 0.915842\n",
      "train_loss: 0.422798, train_accuracy: 0.937294\n",
      "test_loss: 0.437084, test_accuracy: 0.915842\n",
      "train_loss: 0.422782, train_accuracy: 0.937294\n",
      "test_loss: 0.437088, test_accuracy: 0.915842\n",
      "train_loss: 0.422768, train_accuracy: 0.937294\n",
      "test_loss: 0.437091, test_accuracy: 0.915842\n",
      "train_loss: 0.422753, train_accuracy: 0.937294\n",
      "test_loss: 0.437095, test_accuracy: 0.915842\n",
      "train_loss: 0.422738, train_accuracy: 0.937294\n",
      "test_loss: 0.437098, test_accuracy: 0.915842\n",
      "train_loss: 0.422723, train_accuracy: 0.937294\n",
      "test_loss: 0.437102, test_accuracy: 0.915842\n",
      "train_loss: 0.422708, train_accuracy: 0.937294\n",
      "test_loss: 0.437105, test_accuracy: 0.915842\n",
      "train_loss: 0.422693, train_accuracy: 0.937294\n",
      "test_loss: 0.437109, test_accuracy: 0.915842\n",
      "train_loss: 0.422679, train_accuracy: 0.937294\n",
      "test_loss: 0.437112, test_accuracy: 0.917492\n",
      "train_loss: 0.422664, train_accuracy: 0.937294\n",
      "test_loss: 0.437116, test_accuracy: 0.917492\n",
      "train_loss: 0.422649, train_accuracy: 0.937294\n",
      "test_loss: 0.437119, test_accuracy: 0.917492\n",
      "train_loss: 0.422635, train_accuracy: 0.937294\n",
      "test_loss: 0.437123, test_accuracy: 0.919142\n",
      "train_loss: 0.422620, train_accuracy: 0.937294\n",
      "test_loss: 0.437127, test_accuracy: 0.919142\n",
      "train_loss: 0.422605, train_accuracy: 0.937294\n",
      "test_loss: 0.437130, test_accuracy: 0.919142\n",
      "train_loss: 0.422591, train_accuracy: 0.937294\n",
      "test_loss: 0.437134, test_accuracy: 0.919142\n",
      "train_loss: 0.422576, train_accuracy: 0.937294\n",
      "test_loss: 0.437137, test_accuracy: 0.919142\n",
      "train_loss: 0.422562, train_accuracy: 0.937294\n",
      "test_loss: 0.437141, test_accuracy: 0.919142\n",
      "train_loss: 0.422548, train_accuracy: 0.937294\n",
      "test_loss: 0.437144, test_accuracy: 0.919142\n",
      "train_loss: 0.422533, train_accuracy: 0.937294\n",
      "test_loss: 0.437148, test_accuracy: 0.919142\n",
      "train_loss: 0.422519, train_accuracy: 0.937294\n",
      "test_loss: 0.437152, test_accuracy: 0.919142\n",
      "train_loss: 0.422505, train_accuracy: 0.937294\n",
      "test_loss: 0.437155, test_accuracy: 0.919142\n",
      "train_loss: 0.422490, train_accuracy: 0.937294\n",
      "test_loss: 0.437159, test_accuracy: 0.919142\n",
      "train_loss: 0.422476, train_accuracy: 0.937294\n",
      "test_loss: 0.437163, test_accuracy: 0.919142\n",
      "train_loss: 0.422462, train_accuracy: 0.937294\n",
      "test_loss: 0.437166, test_accuracy: 0.919142\n",
      "train_loss: 0.422448, train_accuracy: 0.935644\n",
      "test_loss: 0.437170, test_accuracy: 0.920792\n",
      "train_loss: 0.422434, train_accuracy: 0.935644\n",
      "test_loss: 0.437174, test_accuracy: 0.920792\n",
      "train_loss: 0.422420, train_accuracy: 0.935644\n",
      "test_loss: 0.437177, test_accuracy: 0.920792\n",
      "train_loss: 0.422406, train_accuracy: 0.935644\n",
      "test_loss: 0.437181, test_accuracy: 0.920792\n",
      "train_loss: 0.422392, train_accuracy: 0.935644\n",
      "test_loss: 0.437184, test_accuracy: 0.920792\n",
      "train_loss: 0.422378, train_accuracy: 0.933993\n",
      "test_loss: 0.437188, test_accuracy: 0.920792\n",
      "train_loss: 0.422364, train_accuracy: 0.933993\n",
      "test_loss: 0.437192, test_accuracy: 0.920792\n",
      "train_loss: 0.422350, train_accuracy: 0.933993\n",
      "test_loss: 0.437196, test_accuracy: 0.920792\n",
      "train_loss: 0.422336, train_accuracy: 0.933993\n",
      "test_loss: 0.437199, test_accuracy: 0.920792\n",
      "train_loss: 0.422322, train_accuracy: 0.933993\n",
      "test_loss: 0.437203, test_accuracy: 0.920792\n",
      "train_loss: 0.422308, train_accuracy: 0.933993\n",
      "test_loss: 0.437207, test_accuracy: 0.920792\n",
      "train_loss: 0.422295, train_accuracy: 0.933993\n",
      "test_loss: 0.437210, test_accuracy: 0.920792\n",
      "train_loss: 0.422281, train_accuracy: 0.933993\n",
      "test_loss: 0.437214, test_accuracy: 0.922442\n",
      "train_loss: 0.422267, train_accuracy: 0.933993\n",
      "test_loss: 0.437218, test_accuracy: 0.922442\n",
      "train_loss: 0.422254, train_accuracy: 0.933993\n",
      "test_loss: 0.437222, test_accuracy: 0.922442\n",
      "train_loss: 0.422240, train_accuracy: 0.933993\n",
      "test_loss: 0.437225, test_accuracy: 0.922442\n",
      "train_loss: 0.422226, train_accuracy: 0.933993\n",
      "test_loss: 0.437229, test_accuracy: 0.922442\n",
      "train_loss: 0.422213, train_accuracy: 0.933993\n",
      "test_loss: 0.437233, test_accuracy: 0.922442\n",
      "Sequential training done\n",
      "4.623252630233765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "t1 = time.time()\n",
    "#s = s+ (t2-t1)\n",
    "for epoch in range(500):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_2_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "t2 = time.time()\n",
    "s = s+ (t2-t1)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "print(s)\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011558131575584411"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s/400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(40):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_1_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_ = subnet_output(alpha_1, beta_1, X_) \n",
    "logits__ = sess.run(logits_, feed_dict={X: [x_test[4000]]})\n",
    "print(logits__)\n",
    "print(np.argmax(logits__))\n",
    "print(y_test[4000])\n",
    "plt.imshow(x_test[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alpha(alpha, size):\n",
    "    tmp = sess.run(alpha)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            plt.subplot(2,5,i*5+j+1)\n",
    "            plt.imshow(np.reshape(tmp[:,i*5+j], [size,size]))\n",
    "\n",
    "def visualize_beta(beta):\n",
    "    tmp = sess.run(beta)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(tmp)\n",
    "    \n",
    "            \n",
    "\"\"\"visualize subnet nodes\"\"\"            \n",
    "visualize_alpha(alpha_1, 10)\n",
    "visualize_beta(beta_1)\n",
    "visualize_alpha(alpha_2, 10)\n",
    "visualize_beta(beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.math.sin(tf.constant([0.9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
