{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 7131)\n",
      "[['1:-1.382400' '2:0.062875' '3:0.622518' ... '7127:1.178050'\n",
      "  '7128:-0.348865' '7129:0.084596']\n",
      " ['1:0.651922' '2:-0.354764' '3:2.296340' ... '7127:0.232365'\n",
      "  '7128:-0.643625' '7129:0.824816']\n",
      " ['1:0.654087' '2:1.413430' '3:0.225929' ... '7127:1.569970'\n",
      "  '7128:-0.466704' '7129:0.539913']\n",
      " ...\n",
      " ['1:1.096040' '2:1.643850' '3:0.681936' ... '7127:0.973412'\n",
      "  '7128:-0.264428' '7129:1.837500']\n",
      " ['1:0.503660' '2:0.426793' '3:0.294282' ... '7127:0.036783'\n",
      "  '7128:-0.541209' '7129:0.358362']\n",
      " ['1:-0.455835' '2:-0.071517' '3:-0.526423' ... '7127:-0.110748'\n",
      "  '7128:-0.339931' '7129:-0.068667']]\n",
      "[['1:-1.462360' '2:-0.645135' '3:-0.835925' ... '7127:-0.596842'\n",
      "  '7128:-0.471106' '7129:-0.959575']\n",
      " ['1:-0.664799' '2:0.206146' '3:-0.368575' ... '7127:-0.865241'\n",
      "  '7128:-0.664888' '7129:-0.543433']\n",
      " ['1:-0.200487' '2:0.379941' '3:-2.382780' ... '7127:-0.496304'\n",
      "  '7128:-0.433475' '7129:-0.896774']\n",
      " ...\n",
      " ['1:-1.298680' '2:-1.317620' '3:0.298352' ... '7127:-1.029130'\n",
      "  '7128:-0.466400' '7129:-0.419874']\n",
      " ['1:0.384265' '2:0.825415' '3:0.566278' ... '7127:-0.559180'\n",
      "  '7128:4.123710' '7129:-0.505618']\n",
      " ['1:0.350421' '2:0.195492' '3:0.054057' ... '7127:0.175220'\n",
      "  '7128:3.273680' '7129:0.379149']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38, 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import mnist_handler\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"; \n",
    "INPUT_DIMENSION = 7129\n",
    "NUM_CLASSES = 2\n",
    "train_dataset = np.array(pd.read_csv(\"UCI dataset/leu.t\", header = None,delimiter=' '))\n",
    "test_dataset = np.array(pd.read_csv(\"UCI dataset/leu\",header = None, delimiter=' '))\n",
    "print(train_dataset.shape)\n",
    "label = train_dataset[:,0]\n",
    "data = train_dataset[:,2:]\n",
    "label = label.reshape(label.shape[0], 1)\n",
    "print(data)\n",
    "data1 = []\n",
    "for i in data:\n",
    "    data2 = []\n",
    "    for j in i:\n",
    "        [_, value] = j.split(\":\")\n",
    "        data2.append(value)\n",
    "    data1.append(data2)\n",
    "    \n",
    "data = np.array(data1)\n",
    "\n",
    "label1 = test_dataset[:,0]\n",
    "test_data = test_dataset[:,2:]\n",
    "label1 = label1.reshape(label1.shape[0], 1)\n",
    "print(test_data)\n",
    "dat = []\n",
    "for i in test_data:\n",
    "    data2 = []\n",
    "    for j in i:\n",
    "        [_, value] = j.split(\":\")\n",
    "        data2.append(value)\n",
    "    dat.append(data2)\n",
    "    \n",
    "test_data = np.array(dat)\n",
    "#data = np.concatenate((label, data), axis = 1)\n",
    "\n",
    "label1.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 1)\n",
      "(38, 7130)\n",
      "(34, 7130)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72, 7130)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label1 = np.where(label1==-1, 0, label1)\n",
    "label2 = np.where(label1==1.0, 1, label1)\n",
    "print(label2.shape)\n",
    "test_data = np.concatenate((label2, test_data), axis = 1)\n",
    "\n",
    "print(test_data.shape)\n",
    "tr_lab = np.where(label==-1, 0, label)\n",
    "tr_label = np.where(tr_lab==1.0, 1, tr_lab)\n",
    "#print(tr_label)\n",
    "train_data = np.concatenate((tr_label, data), axis = 1)\n",
    "print(train_data.shape)\n",
    "\n",
    "total = np.concatenate((train_data,test_data),axis = 0)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(total)\n",
    "total = scaler.transform(total)\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 7129)\n",
      "(34, 7129)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "(34,)\n"
     ]
    }
   ],
   "source": [
    "x_train = total[0:38,1:]\n",
    "y_train = total[0:38,0]\n",
    "y_train = np.array(y_train, dtype=np.int16)\n",
    "x_test = total[38:,1:]\n",
    "y_test = total[38:,0]\n",
    "y_test = np.array(y_test, dtype=np.int16)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_elm import AUTO_ELM   # Our proposed algorithm\n",
    "n_input_nodes = 7129\n",
    "n_hidden_nodes = 32  # used to be 256\n",
    "n_output_nodes = 7129\n",
    "\n",
    "border = 32\n",
    "x_train_init = x_train[:border]\n",
    "x_train_seq = x_train[border:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "accuracy_sum = 0\n",
    "time_sum = 0\n",
    "for PPAP in range(10):\n",
    "\ttf.reset_default_graph()\n",
    "\t# ===========================================\n",
    "\t# Instantiate os-elm\n",
    "\t# ===========================================\n",
    "\tauto_elm = AUTO_ELM(\n",
    "\t\t# the number of input nodes.\n",
    "\t\tn_input_nodes=n_input_nodes,\n",
    "\t\t# the number of hidden nodes.\n",
    "\t\tn_hidden_nodes=n_hidden_nodes,\n",
    "\t\t# the number of output nodes.\n",
    "\t\tn_output_nodes=n_output_nodes,\n",
    "\t\t# loss function.\n",
    "\t\t# the default value is 'mean_squared_error'.\n",
    "\t\t# for the other functions, we support\n",
    "\t\t# 'mean_absolute_error', 'categorical_crossentropy', and 'binary_crossentropy'.\n",
    "\t\tc_value = 2.0**(4),\n",
    "\t\tloss='mean_squared_error',\n",
    "\t\t# activation function applied to the hidden nodes.\n",
    "\t\t# the default value is 'sigmoid'.\n",
    "\t\t# for the other functions, we support 'linear' and 'tanh'.\n",
    "\t\t# NOTE: OS-ELM can apply an activation function only to the hidden nodes.\n",
    "\t\tactivation='sin',\n",
    "\t)\n",
    "\t# ===========================================\n",
    "\t# Training\n",
    "\t# ===========================================\n",
    "\t# the initial training phase\n",
    "\tt1 = time.time()\n",
    "\tauto_elm.init_train(x_train_init, x_train_init)\n",
    "\tt2 = time.time()\n",
    "\ttime_sum+=(t2-t1)\n",
    "\n",
    "\t# the sequential training phase\n",
    "\tbatch_size = 6\n",
    "\n",
    "\tt1 = time.time()\n",
    "\tfor epoch in range(100):\n",
    "\t\tfor i in range(0, len(x_train_seq), batch_size):\n",
    "\t\t\tx_batch = x_train_seq[i:i+batch_size]\n",
    "\t\t\tif len(x_batch) != batch_size:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tauto_elm.seq_train(x_batch, x_batch)\n",
    "\tt2 = time.time()\n",
    "\ttime_sum+=(t2-t1)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_encoded = auto_elm.encoding(x_train)\n",
    "x_test_encoded = auto_elm.encoding(x_test)\n",
    "x_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "border = 8\n",
    "x_train_init = x_train_encoded[:border]\n",
    "y_train_init = y_train[:border]\n",
    "x_train_seq = x_train_encoded[border:]\n",
    "y_train_seq = y_train[border:]\n",
    "INPUT_DIMENSION = 32\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "border = 10\n",
    "x_train_init = x_train[:border]\n",
    "y_train_init = y_train[:border]\n",
    "x_train_seq = x_train[border:]\n",
    "y_train_seq = y_train[border:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # clear all the tensors\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "\"\"\"Placeholders\"\"\"\n",
    "X_ = tf.placeholder(tf.float32, [None, INPUT_DIMENSION])\n",
    "#X_ = tf.reshape(X, [-1, INPUT_DIMENSION]) # Flatten X: [N,D]\n",
    "Y = tf.placeholder(tf.int64, [None]) # labels\n",
    "Y_ = tf.one_hot(indices=Y, depth=NUM_CLASSES) # one_hot labels: [N,M]\n",
    "\n",
    "\"\"\"Some constants\"\"\"\n",
    "D = INPUT_DIMENSION\n",
    "M = NUM_CLASSES # Number of outputs\n",
    "C = tf.constant(2.0**(-1))\n",
    "\n",
    "\"\"\"Weights\"\"\"\n",
    "alpha_1 = tf.get_variable('alpha_1',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 1st subnetwork\n",
    "alpha_2 = tf.get_variable('alpha_2',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 2st subnetwork\n",
    "alpha_3 = tf.get_variable('alpha_3',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_1 = tf.get_variable('beta_1',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_2 = tf.get_variable('beta_2',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_3 = tf.get_variable('beta_3',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.get_variable('k',shape=[D, D],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "m = tf.get_variable('m',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions\"\"\"\n",
    "def mul(A, B):\n",
    "    return tf.matmul(A, B)\n",
    "\n",
    "def inv(A):\n",
    "    return tf.matrix_inverse(A)\n",
    "\n",
    "def t(A):\n",
    "    return tf.transpose(A)\n",
    "\n",
    "def sin(A):\n",
    "    return tf.math.sin(A)\n",
    "\n",
    "def asin(A):\n",
    "    return tf.math.asin(A)\n",
    "\n",
    "def sqrt(A):\n",
    "    return tf.sqrt(A)\n",
    "\n",
    "def sqr(A):\n",
    "    return tf.math.pow(A, 2)\n",
    "\n",
    "def pseudo_inv(A, I, C):\n",
    "    C_I = I/C\n",
    "    return mul(t(A), inv(C_I + mul(A, t(A))))\n",
    "\n",
    "def h(A):\n",
    "    '''activation function'''\n",
    "    return sin(A)\n",
    "\n",
    "def h_(A):\n",
    "    '''inverse activation function'''\n",
    "    return asin(A)\n",
    "\n",
    "def u(A):\n",
    "    '''normalize the input to (0,1]'''\n",
    "    return tf.math.sigmoid(A) # sigmoid\n",
    "    \n",
    "def u_(A):\n",
    "    '''the inverse of u'''\n",
    "    ONE = tf.constant(1.0)\n",
    "    return -(tf.math.log(ONE/A - ONE)) # the inverse of sigmoid\n",
    "    \n",
    "def subnet_output(alpha, beta, A):\n",
    "    return t(mul(beta, h(mul(t(alpha), t(A))))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "'''some pre-computations'''\n",
    "X_init = t(X_) # [D,N]\n",
    "Y_init = t(Y_) # [M,N]\n",
    "N_init = D # number of dimensions\n",
    "I_DxD = tf.eye(N_init, dtype=tf.float32) # [D,D]\n",
    "I_MxM = tf.eye(M, dtype=tf.float32) # [M,M]\n",
    "C_I = I_DxD/C\n",
    "H_I = I_MxM/C\n",
    "\n",
    "add = C_I + mul(X_init, t(X_init))\n",
    "k = tf.assign(k,add)\n",
    "X_inv_init = pseudo_inv(X_init, I_DxD, C) # [N,D]\n",
    "\n",
    "'''1st subnet'''\n",
    "alpha_1_init_calculated = t(mul(h_(Y_init), X_inv_init)) # ([M,N]x[N,D])T=[D,M]\n",
    "alpha_1_init = tf.assign(alpha_1, alpha_1_init_calculated) # [D,M]\n",
    "H_1_init = h(mul(t(alpha_1_init), X_init)) # [M,N]\n",
    "H_add = H_I + mul(H_1_init,t(H_1_init))\n",
    "m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_1_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_1_init_calculated = mul(Y_init, t(H_1_init))/sqr(tf.norm(H_1_init)) # [M,M]\n",
    "beta_1_init_calculated = mul(Y_init,H_pseudo_init)\n",
    "\n",
    "beta_1_init = tf.assign(beta_1, beta_1_init_calculated) # [M,M]\n",
    "H_beta_1_init = mul(beta_1_init, t(mul(t(X_init), alpha_1_init))) # [M,N]\n",
    "E_1_init = Y_init - H_beta_1_init # [M,N]\n",
    "\n",
    "'''2nd subnet'''\n",
    "#alpha_2_init_calculated = t(mul(h_(E_1_init), X_inv_init)) # [D,M]    \n",
    "alpha_2_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_2_init = tf.assign(alpha_2, alpha_2_init_calculated) # [D,M]\n",
    "H_2_init = h(mul(t(alpha_2_init), X_init)) # [M,N]\n",
    "H_2_inv_init = pseudo_inv(H_2_init, I_MxM, C) # [M,N]\n",
    "H_add = H_I + mul(H_2_init,t(H_2_init))\n",
    "#m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_2_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_2_init_calculated = mul(E_1_init, t(H_2_init))/sqr(tf.norm(H_2_init)) # [M,M]\n",
    "beta_2_init_calculated = mul(E_1_init, H_pseudo_init)\n",
    "\n",
    "beta_2_init = tf.assign(beta_2, beta_2_init_calculated) # [M,M]\n",
    "H_beta_2_init = mul(beta_2_init, t(mul(t(X_init), alpha_2_init))) # [M,N]\n",
    "E_2_init = Y_init - (H_beta_1_init+H_beta_2_init) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "alpha_3_init_calculated = t(mul(h_(E_2_init), X_inv_init)) # [D,M]    \n",
    "alpha_3_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_3_init = tf.assign(alpha_3, alpha_3_init_calculated) # [D,M]\n",
    "H_3_init = h(mul(t(alpha_3_init), X_init)) # [M,N]\n",
    "H_3_inv_init = pseudo_inv(H_3_init, I_MxM, C) # [M,N]\n",
    "\n",
    "beta_3_init_calculated = mul(E_2_init, t(H_3_init))/sqr(tf.norm(H_3_init)) # [M,M]\n",
    "beta_3_init_calculated = mul(E_2_init, H_3_inv_init)\n",
    "\n",
    "beta_3_init = tf.assign(beta_3, beta_3_init_calculated) # [M,M]\n",
    "H_beta_3_init = mul(beta_3_init, t(mul(t(X_init), alpha_3_init))) # [M,N]\n",
    "E_3_init = Y_init - (H_beta_3_init+H_beta_2_init+ H_beta_1_init) # [M,N]\n",
    "\n",
    "#init_train_graph = H_beta_1_init\n",
    "init_train_graph = E_3_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_3:0' shape=(2, 2) dtype=float32_ref>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_1_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''With one subnetwork'''\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.4596118\n",
      "Initial train training accuracy:  0.65789473\n",
      "Initial train testing loss:  0.58734274\n",
      "Initial train testing accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(E_1_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.44751167\n",
      "Initial train training accuracy:  0.65789473\n",
      "Initial train testing loss:  0.56402206\n",
      "Initial train testing accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(E_2_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.44888106\n",
      "Initial train training accuracy:  0.65789473\n",
      "Initial train testing loss:  0.567152\n",
      "Initial train testing accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(init_train_graph, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_init = subnet_output(alpha_1, beta_1, X_)+ subnet_output(alpha_2, beta_2, X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "logic = sess.run(logits_init, feed_dict={X_ : [x_test_encoded[4]]})\n",
    "print(np.argmax(logic,axis =1))\n",
    "print(y_test[4])\n",
    "#plt.imshow(x_test[4200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sequential Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "X_seq = t(X_) # [D,N]\n",
    "Y_seq = t(Y_) # [M,N]\n",
    "pseudo = mul(X_seq, X_) #DXD\n",
    "k = tf.assign(k, tf.add(k,pseudo)) #DXD\n",
    "k_inv = inv(k)\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_1))\n",
    "alpha1_seq = tf.assign(alpha_1,tf.add(alpha_1,new)) #DXM\n",
    "H_1_seq = h(mul(t(alpha1_seq), X_seq)) # [M,N]\n",
    "m_su = mul(H_1_seq,t(H_1_seq))\n",
    "m = tf.assign(m,tf.add(m,m_su))\n",
    "m_inv = inv(m)\n",
    "#update = tf.matmul(tf.matmul(m_inv,H_1_seq),h_(Y_seq)- tf.matmul())\n",
    "H_pseudo_init = pseudo_inv(H_1_seq,I_MxM,C) #[N,M]\n",
    "#UPDATE = tf.matmul(tf.matmul(K_inverse, HT), inverse_acti_y - tf.matmul(H, self.__outputWeight))\n",
    "beta_1_seq_calculated = mul(Y_seq, H_pseudo_init) # [M,M]\n",
    "beta_1_seq = tf.assign(beta_1, beta_1_seq_calculated) # [M,M]\n",
    "H_beta_1_seq = mul(beta_1_seq, t(mul(X_, alpha1_seq))) # [M,N]\n",
    "E_1_seq = Y_seq - H_beta_1_seq # [M,N]\n",
    "\n",
    "'''2nd subnetwork'''\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_2))\n",
    "alpha2_seq = tf.assign(alpha_2,tf.add(alpha_2,new)) #DXM\n",
    "H_2_seq = h(mul(t(alpha2_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_2_seq,I_MxM,C) #[N,M]\n",
    "beta_2_seq_calculated = mul(E_1_seq, H_pseudo_init) # [M,M]\n",
    "beta_2_seq = tf.assign(beta_2, beta_2_seq_calculated) # [M,M]\n",
    "H_beta_2_seq = mul(beta_2_seq, t(mul(t(X_seq), alpha2_seq))) # [M,N]\n",
    "E_2_seq = Y_seq - (H_beta_2_seq+ H_beta_1_seq) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_3))\n",
    "alpha3_seq = tf.assign(alpha_3,tf.add(alpha_3,new)) #DXM\n",
    "H_3_seq = h(mul(t(alpha3_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_3_seq,I_MxM,C) #[N,M]\n",
    "beta_3_seq_calculated = mul(E_2_seq, H_pseudo_init) # [M,M]\n",
    "beta_3_seq = tf.assign(beta_3, beta_3_seq_calculated) # [M,M]\n",
    "H_beta_3_seq = mul(beta_3_seq, t(mul(t(X_seq), alpha3_seq))) # [M,N]\n",
    "E_3_seq = Y_seq - (H_beta_3_seq +H_beta_2_seq + H_beta_1_seq )# [M,N]\n",
    "seq_train_graph = E_3_seq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.165924, train_accuracy: 0.894737\n",
      "test_loss: 0.617992, test_accuracy: 0.529412\n",
      "train_loss: 0.168286, train_accuracy: 0.842105\n",
      "test_loss: 0.614272, test_accuracy: 0.500000\n",
      "train_loss: 0.178053, train_accuracy: 0.868421\n",
      "test_loss: 0.610501, test_accuracy: 0.529412\n",
      "train_loss: 0.183075, train_accuracy: 0.868421\n",
      "test_loss: 0.611069, test_accuracy: 0.588235\n",
      "train_loss: 0.185207, train_accuracy: 0.842105\n",
      "test_loss: 0.612400, test_accuracy: 0.617647\n",
      "train_loss: 0.185786, train_accuracy: 0.842105\n",
      "test_loss: 0.613171, test_accuracy: 0.647059\n",
      "train_loss: 0.185491, train_accuracy: 0.815789\n",
      "test_loss: 0.613133, test_accuracy: 0.676471\n",
      "train_loss: 0.184680, train_accuracy: 0.842105\n",
      "test_loss: 0.612439, test_accuracy: 0.705882\n",
      "train_loss: 0.183563, train_accuracy: 0.842105\n",
      "test_loss: 0.611365, test_accuracy: 0.705882\n",
      "train_loss: 0.182270, train_accuracy: 0.842105\n",
      "test_loss: 0.610201, test_accuracy: 0.735294\n",
      "train_loss: 0.180886, train_accuracy: 0.894737\n",
      "test_loss: 0.609195, test_accuracy: 0.705882\n",
      "train_loss: 0.179468, train_accuracy: 0.894737\n",
      "test_loss: 0.608546, test_accuracy: 0.705882\n",
      "train_loss: 0.178054, train_accuracy: 0.894737\n",
      "test_loss: 0.608390, test_accuracy: 0.705882\n",
      "train_loss: 0.176671, train_accuracy: 0.894737\n",
      "test_loss: 0.608819, test_accuracy: 0.705882\n",
      "train_loss: 0.175334, train_accuracy: 0.894737\n",
      "test_loss: 0.609876, test_accuracy: 0.705882\n",
      "train_loss: 0.174054, train_accuracy: 0.894737\n",
      "test_loss: 0.611574, test_accuracy: 0.705882\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 15\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(16):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(seq_train_graph, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\"\n",
    "#tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_train, Y: y_train})\n",
    "#ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_test, Y: y_test})\n",
    "#print(\"Sequential train training loss: \", tr_loss)\n",
    "#print(\"Sequential train training accuracy: \", tr_acc)\n",
    "#print(\"Sequential train testing loss: \", ts_loss)\n",
    "#print(\"Sequential train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.298162, train_accuracy: 0.684211\n",
      "test_loss: 0.576820, test_accuracy: 0.647059\n",
      "train_loss: 0.300145, train_accuracy: 0.710526\n",
      "test_loss: 0.568116, test_accuracy: 0.617647\n",
      "train_loss: 0.302286, train_accuracy: 0.710526\n",
      "test_loss: 0.563159, test_accuracy: 0.617647\n",
      "train_loss: 0.303946, train_accuracy: 0.710526\n",
      "test_loss: 0.560523, test_accuracy: 0.617647\n",
      "train_loss: 0.305206, train_accuracy: 0.736842\n",
      "test_loss: 0.559341, test_accuracy: 0.558824\n",
      "train_loss: 0.306175, train_accuracy: 0.736842\n",
      "test_loss: 0.559122, test_accuracy: 0.500000\n",
      "train_loss: 0.306932, train_accuracy: 0.763158\n",
      "test_loss: 0.559576, test_accuracy: 0.470588\n",
      "train_loss: 0.307528, train_accuracy: 0.763158\n",
      "test_loss: 0.560525, test_accuracy: 0.529412\n",
      "train_loss: 0.307997, train_accuracy: 0.763158\n",
      "test_loss: 0.561847, test_accuracy: 0.529412\n",
      "train_loss: 0.308364, train_accuracy: 0.763158\n",
      "test_loss: 0.563458, test_accuracy: 0.529412\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 10\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(10):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_2_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.511930, train_accuracy: 0.447368\n",
      "test_loss: 0.530026, test_accuracy: 0.529412\n",
      "train_loss: 0.354036, train_accuracy: 0.605263\n",
      "test_loss: 0.693639, test_accuracy: 0.441176\n",
      "train_loss: 0.326663, train_accuracy: 0.657895\n",
      "test_loss: 0.702260, test_accuracy: 0.500000\n",
      "train_loss: 0.311414, train_accuracy: 0.657895\n",
      "test_loss: 0.688967, test_accuracy: 0.470588\n",
      "train_loss: 0.298190, train_accuracy: 0.631579\n",
      "test_loss: 0.678022, test_accuracy: 0.558824\n",
      "train_loss: 0.285516, train_accuracy: 0.657895\n",
      "test_loss: 0.669255, test_accuracy: 0.558824\n",
      "train_loss: 0.273352, train_accuracy: 0.736842\n",
      "test_loss: 0.662746, test_accuracy: 0.470588\n",
      "train_loss: 0.261862, train_accuracy: 0.763158\n",
      "test_loss: 0.658502, test_accuracy: 0.558824\n",
      "train_loss: 0.251152, train_accuracy: 0.789474\n",
      "test_loss: 0.656241, test_accuracy: 0.617647\n",
      "train_loss: 0.241251, train_accuracy: 0.815789\n",
      "test_loss: 0.655476, test_accuracy: 0.647059\n",
      "train_loss: 0.232145, train_accuracy: 0.815789\n",
      "test_loss: 0.655674, test_accuracy: 0.647059\n",
      "train_loss: 0.223794, train_accuracy: 0.842105\n",
      "test_loss: 0.656376, test_accuracy: 0.647059\n",
      "train_loss: 0.216154, train_accuracy: 0.842105\n",
      "test_loss: 0.657273, test_accuracy: 0.647059\n",
      "train_loss: 0.209175, train_accuracy: 0.868421\n",
      "test_loss: 0.658219, test_accuracy: 0.647059\n",
      "train_loss: 0.202811, train_accuracy: 0.842105\n",
      "test_loss: 0.659207, test_accuracy: 0.647059\n",
      "train_loss: 0.197015, train_accuracy: 0.842105\n",
      "test_loss: 0.660325, test_accuracy: 0.676471\n",
      "train_loss: 0.191741, train_accuracy: 0.842105\n",
      "test_loss: 0.661713, test_accuracy: 0.705882\n",
      "train_loss: 0.186947, train_accuracy: 0.842105\n",
      "test_loss: 0.663525, test_accuracy: 0.676471\n",
      "train_loss: 0.182590, train_accuracy: 0.842105\n",
      "test_loss: 0.665896, test_accuracy: 0.705882\n",
      "train_loss: 0.178632, train_accuracy: 0.842105\n",
      "test_loss: 0.668926, test_accuracy: 0.705882\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 15\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(20):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_1_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, classes):\n",
    "    I = np.eye(classes)\n",
    "    #print(I)\n",
    "    ret = []\n",
    "    l = 0\n",
    "    for y_ in y:\n",
    "        ret.append(I[y_])\n",
    "            \n",
    "    return np.array(ret)\n",
    "    \n",
    "y_train_onehot = one_hot(y_train, 3)\n",
    "y_test_onehot = one_hot(y_test,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-24 15:00:24.922861 ML-ELM Start training...\n",
      "AE Network parameters have been initialized\n",
      "Training AE 1/1\n",
      "2019-05-24 15:00:25.009471 Start training...\n",
      "Training of AE mlelm1 ended in 0:0:0.032973\n",
      "Evaluating AE performance...\n",
      "MSE: 0.0182392\n",
      "####################################################################################################\n",
      "Initializing last layer ELM...\n",
      "Training last layer ELM...\n",
      "Training of ELM mlelm1 ended in 0:0:0.032306\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "TOTAL Training of ML_ELM mlelm1 ended in 0:0:0.255272\n",
      "####################################################################################################\n",
      "Evaluating network performance\n",
      "Accuracy: 0.6315789\n",
      "Evaluating network performance\n",
      "Accuracy: 0.6764706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6764706"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "input_size = 7129\n",
    "output_size = 3 # mnist has 10 output classes \n",
    "from ml_elm import ML_ELM\n",
    "\n",
    "mlelm1 = ML_ELM(input_size=input_size, output_size=output_size, name='mlelm1')\n",
    "mlelm1.add_layer(n_neurons=101, l2norm=1e2); \n",
    "mlelm1.add_layer(n_neurons=101, activation=tf.tanh, l2norm=1e2); \n",
    "t1 = time.time()\n",
    "mlelm1.fit(x_train, y_train_onehot, batch_size=500)\n",
    "t2 = time.time()\n",
    "time_sum=(t2-t1) \n",
    "mlelm1.evaluate(x_test, y_test_onehot, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7953381f8c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubnet_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogits__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "logits_ = subnet_output(alpha_1, beta_1, X_) \n",
    "logits__ = sess.run(logits_, feed_dict={X: [x_test[4000]]})\n",
    "print(logits__)\n",
    "print(np.argmax(logits__))\n",
    "print(y_test[4000])\n",
    "plt.imshow(x_test[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29993462562561035"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alpha(alpha, size):\n",
    "    tmp = sess.run(alpha)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            plt.subplot(2,5,i*5+j+1)\n",
    "            plt.imshow(np.reshape(tmp[:,i*5+j], [size,size]))\n",
    "\n",
    "def visualize_beta(beta):\n",
    "    tmp = sess.run(beta)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(tmp)\n",
    "    \n",
    "            \n",
    "\"\"\"visualize subnet nodes\"\"\"            \n",
    "visualize_alpha(alpha_1, 28)\n",
    "visualize_beta(beta_1)\n",
    "visualize_alpha(alpha_2, 28)\n",
    "visualize_beta(beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_nodes = 180\n",
    "n_output_nodes = 180\n",
    "n_hidden_nodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.math.sin(tf.constant([0.9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
