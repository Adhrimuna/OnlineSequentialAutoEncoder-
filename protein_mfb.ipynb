{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import mnist_handler\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"; \n",
    "INPUT_DIMENSION = 180\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "def process_data(fname, NUM_ATTR, NUM_CLASSES):\n",
    "    I = np.eye(NUM_CLASSES)\n",
    "    with open(fname) as file:\n",
    "        xx = file.readlines()\n",
    "        #print(len(xx))\n",
    "        data = np.zeros([len(xx), NUM_ATTR])\n",
    "        #print(data.shape)\n",
    "        label = np.zeros(len(xx), dtype=int)\n",
    "        label_onehot = []\n",
    "        for i in range(len(xx)):\n",
    "            tmp = xx[i].split(' ')\n",
    "            #print(tmp)\n",
    "           # print(len(tmp))\n",
    "            label[i] = int(tmp[0])\n",
    "            if label[i] == 2:\n",
    "                label[i] = 1\n",
    "            #print(label[i])\n",
    "            label_onehot.append(I[label[i]])\n",
    "            for j in range(1,len(tmp)-1):\n",
    "                #print(tmp[j])\n",
    "                if tmp[j] == '':\n",
    "                    continue\n",
    "                #print(i)\n",
    "                #print(tmp[j].split(':'))\n",
    "                position = tmp[j].split(':')[0]\n",
    "                position = int(position)\n",
    "                value = 1*float(tmp[j].split(':')[1])\n",
    "                \n",
    "                data[i][position-1] = value \n",
    "    return data, label, np.array(label_onehot, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr_file = 'UCI dataset/protein'\n",
    "ts_file = 'UCI dataset/protein.t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, tr_label, tr_onehot = process_data(tr_file, NUM_ATTR=357, NUM_CLASSES=2)\n",
    "ts_data, ts_label, ts_onehot = process_data(ts_file, NUM_ATTR=357, NUM_CLASSES=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6621, 357)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = tr_data*0.01\n",
    "y_train = tr_label\n",
    "x_test = ts_data*0.01\n",
    "y_test = ts_label\n",
    "x_train\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.savetxt(\"x_train.csv\", x_train, delimiter=\",\")\n",
    "np.savetxt(\"y_train.csv\", y_train, delimiter=\",\")\n",
    "np.savetxt(\"x_test.csv\", x_test, delimiter=\",\")\n",
    "np.savetxt(\"y_test.csv\", y_test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "border = 621\n",
    "x_train_init = x_train[:border]\n",
    "y_train_init = y_train[:border]\n",
    "x_train_seq = x_train[border:]\n",
    "y_train_seq = y_train[border:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(0.55)\n",
    "time_sum = 0\n",
    "t1 = time.time()\n",
    "pca.fit(x_train)\n",
    "t2 = time.time()\n",
    "time_sum+= t2-t1\n",
    "train_img = pca.transform(x_train)\n",
    "test_img  = pca.transform(x_test)\n",
    "x_train.shape,train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,y_train.shape\n",
    "INPUT_DIMENSION = 357\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "border = 146\n",
    "x_train_init = train_img[:border]\n",
    "y_train_init = y_train[:border]\n",
    "x_train_seq = train_img[border:]\n",
    "y_train_seq = y_train[border:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # clear all the tensors\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "\"\"\"Placeholders\"\"\"\n",
    "X_ = tf.placeholder(tf.float32, [None, INPUT_DIMENSION])\n",
    "#X_ = tf.reshape(X, [-1, INPUT_DIMENSION]) # Flatten X: [N,D]\n",
    "Y = tf.placeholder(tf.int64, [None]) # labels\n",
    "Y_ = tf.one_hot(indices=Y, depth=NUM_CLASSES) # one_hot labels: [N,M]\n",
    "\n",
    "\"\"\"Some constants\"\"\"\n",
    "D = INPUT_DIMENSION\n",
    "M = NUM_CLASSES # Number of outputs\n",
    "C = tf.constant(2.0**(-3))\n",
    "\n",
    "\"\"\"Weights\"\"\"\n",
    "alpha_1 = tf.get_variable('alpha_1',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 1st subnetwork\n",
    "alpha_2 = tf.get_variable('alpha_2',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 2st subnetwork\n",
    "alpha_3 = tf.get_variable('alpha_3',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_1 = tf.get_variable('beta_1',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_2 = tf.get_variable('beta_2',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_3 = tf.get_variable('beta_3',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.get_variable('k',shape=[D, D],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "m = tf.get_variable('m',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions\"\"\"\n",
    "def mul(A, B):\n",
    "    return tf.matmul(A, B)\n",
    "\n",
    "def inv(A):\n",
    "    return tf.matrix_inverse(A)\n",
    "\n",
    "def t(A):\n",
    "    return tf.transpose(A)\n",
    "\n",
    "def sin(A):\n",
    "    return tf.math.sin(A)\n",
    "\n",
    "def asin(A):\n",
    "    return tf.math.asin(A)\n",
    "\n",
    "def sqrt(A):\n",
    "    return tf.sqrt(A)\n",
    "\n",
    "def sqr(A):\n",
    "    return tf.math.pow(A, 2)\n",
    "\n",
    "def pseudo_inv(A, I, C):\n",
    "    C_I = I/C\n",
    "    return mul(t(A), inv(C_I + mul(A, t(A))))\n",
    "\n",
    "def h(A):\n",
    "    '''activation function'''\n",
    "    return sin(A)\n",
    "\n",
    "def h_(A):\n",
    "    '''inverse activation function'''\n",
    "    return asin(A)\n",
    "\n",
    "def u(A):\n",
    "    '''normalize the input to (0,1]'''\n",
    "    return tf.math.sigmoid(A) # sigmoid\n",
    "    \n",
    "def u_(A):\n",
    "    '''the inverse of u'''\n",
    "    ONE = tf.constant(1.0)\n",
    "    return -(tf.math.log(ONE/A - ONE)) # the inverse of sigmoid\n",
    "    \n",
    "def subnet_output(alpha, beta, A):\n",
    "    return t(mul(beta, h(mul(t(alpha), t(A))))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "'''some pre-computations'''\n",
    "X_init = t(X_) # [D,N]\n",
    "Y_init = t(Y_) # [M,N]\n",
    "N_init = D # number of dimensions\n",
    "I_DxD = tf.eye(N_init, dtype=tf.float32) # [D,D]\n",
    "I_MxM = tf.eye(M, dtype=tf.float32) # [M,M]\n",
    "C_I = I_DxD/C\n",
    "H_I = I_MxM/C\n",
    "\n",
    "add = C_I + mul(X_init, t(X_init))\n",
    "k = tf.assign(k,add)\n",
    "X_inv_init = pseudo_inv(X_init, I_DxD, C) # [N,D]\n",
    "\n",
    "'''1st subnet'''\n",
    "alpha_1_init_calculated = t(mul(h_(Y_init), X_inv_init)) # ([M,N]x[N,D])T=[D,M]\n",
    "alpha_1_init = tf.assign(alpha_1, alpha_1_init_calculated) # [D,M]\n",
    "H_1_init = h(mul(t(alpha_1_init), X_init)) # [M,N]\n",
    "H_add = H_I + mul(H_1_init,t(H_1_init))\n",
    "m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_1_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_1_init_calculated = mul(Y_init, t(H_1_init))/sqr(tf.norm(H_1_init)) # [M,M]\n",
    "beta_1_init_calculated = mul(Y_init,H_pseudo_init)\n",
    "\n",
    "beta_1_init = tf.assign(beta_1, beta_1_init_calculated) # [M,M]\n",
    "H_beta_1_init = mul(beta_1_init, t(mul(t(X_init), alpha_1_init))) # [M,N]\n",
    "E_1_init = Y_init - H_beta_1_init # [M,N]\n",
    "\n",
    "'''2nd subnet'''\n",
    "#alpha_2_init_calculated = t(mul(h_(E_1_init), X_inv_init)) # [D,M]    \n",
    "alpha_2_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_2_init = tf.assign(alpha_2, alpha_2_init_calculated) # [D,M]\n",
    "H_2_init = h(mul(t(alpha_2_init), X_init)) # [M,N]\n",
    "H_2_inv_init = pseudo_inv(H_2_init, I_MxM, C) # [M,N]\n",
    "H_add = H_I + mul(H_2_init,t(H_2_init))\n",
    "#m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_2_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_2_init_calculated = mul(E_1_init, t(H_2_init))/sqr(tf.norm(H_2_init)) # [M,M]\n",
    "beta_2_init_calculated = mul(E_1_init, H_pseudo_init)\n",
    "\n",
    "beta_2_init = tf.assign(beta_2, beta_2_init_calculated) # [M,M]\n",
    "H_beta_2_init = mul(beta_2_init, t(mul(t(X_init), alpha_2_init))) # [M,N]\n",
    "E_2_init = Y_init - (H_beta_1_init+H_beta_2_init) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "alpha_3_init_calculated = t(mul(h_(E_2_init), X_inv_init)) # [D,M]    \n",
    "alpha_3_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_3_init = tf.assign(alpha_3, alpha_3_init_calculated) # [D,M]\n",
    "H_3_init = h(mul(t(alpha_3_init), X_init)) # [M,N]\n",
    "H_3_inv_init = pseudo_inv(H_3_init, I_MxM, C) # [M,N]\n",
    "\n",
    "beta_3_init_calculated = mul(E_2_init, t(H_3_init))/sqr(tf.norm(H_3_init)) # [M,M]\n",
    "beta_3_init_calculated = mul(E_2_init, H_3_inv_init)\n",
    "\n",
    "beta_3_init = tf.assign(beta_3, beta_3_init_calculated) # [M,M]\n",
    "H_beta_3_init = mul(beta_3_init, t(mul(t(X_init), alpha_3_init))) # [M,N]\n",
    "E_3_init = Y_init - (H_beta_3_init+H_beta_2_init+ H_beta_1_init) # [M,N]\n",
    "\n",
    "#init_train_graph = H_beta_1_init\n",
    "init_train_graph = E_3_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_3:0' shape=(2, 2) dtype=float32_ref>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_1_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''With one subnetwork'''\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.49712205\n",
      "Initial train training accuracy:  0.5385568\n",
      "Initial train testing loss:  0.49714962\n",
      "Initial train testing accuracy:  0.52998036\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(E_1_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.49427783\n",
      "Initial train training accuracy:  0.5385568\n",
      "Initial train testing loss:  0.49433267\n",
      "Initial train testing accuracy:  0.52998036\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "import time\n",
    "s = 0\n",
    "t1 = time.time()\n",
    "sess.run(E_2_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "t2 = time.time()\n",
    "s+=t2-t2\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.4914671\n",
      "Initial train training accuracy:  0.5385568\n",
      "Initial train testing loss:  0.4915487\n",
      "Initial train testing accuracy:  0.52998036\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(init_train_graph, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_init = subnet_output(alpha_1, beta_1, X_)+ subnet_output(alpha_2, beta_2, X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "logic = sess.run(logits_init, feed_dict={X_ : [x_test[4]]})\n",
    "print(np.argmax(logic,axis =1))\n",
    "print(y_test[4])\n",
    "#plt.imshow(x_test[4200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sequential Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "X_seq = t(X_) # [D,N]\n",
    "Y_seq = t(Y_) # [M,N]\n",
    "pseudo = mul(X_seq, X_) #DXD\n",
    "k = tf.assign(k, tf.add(k,pseudo)) #DXD\n",
    "k_inv = inv(k)\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_1))\n",
    "alpha1_seq = tf.assign(alpha_1,tf.add(alpha_1,new)) #DXM\n",
    "H_1_seq = h(mul(t(alpha1_seq), X_seq)) # [M,N]\n",
    "m_su = mul(H_1_seq,t(H_1_seq))\n",
    "m = tf.assign(m,tf.add(m,m_su))\n",
    "m_inv = inv(m)\n",
    "#update = tf.matmul(tf.matmul(m_inv,H_1_seq),h_(Y_seq)- tf.matmul())\n",
    "H_pseudo_init = pseudo_inv(H_1_seq,I_MxM,C) #[N,M]\n",
    "#UPDATE = tf.matmul(tf.matmul(K_inverse, HT), inverse_acti_y - tf.matmul(H, self.__outputWeight))\n",
    "beta_1_seq_calculated = mul(Y_seq, H_pseudo_init) # [M,M]\n",
    "beta_1_seq = tf.assign(beta_1, beta_1_seq_calculated) # [M,M]\n",
    "H_beta_1_seq = mul(beta_1_seq, t(mul(X_, alpha1_seq))) # [M,N]\n",
    "E_1_seq = Y_seq - H_beta_1_seq # [M,N]\n",
    "\n",
    "'''2nd subnetwork'''\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_2))\n",
    "alpha2_seq = tf.assign(alpha_2,tf.add(alpha_2,new)) #DXM\n",
    "H_2_seq = h(mul(t(alpha2_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_2_seq,I_MxM,C) #[N,M]\n",
    "beta_2_seq_calculated = mul(E_1_seq, H_pseudo_init) # [M,M]\n",
    "beta_2_seq = tf.assign(beta_2, beta_2_seq_calculated) # [M,M]\n",
    "H_beta_2_seq = mul(beta_2_seq, t(mul(t(X_seq), alpha2_seq))) # [M,N]\n",
    "E_2_seq = Y_seq - (H_beta_2_seq+ H_beta_1_seq) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_3))\n",
    "alpha3_seq = tf.assign(alpha_3,tf.add(alpha_3,new)) #DXM\n",
    "H_3_seq = h(mul(t(alpha3_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_3_seq,I_MxM,C) #[N,M]\n",
    "beta_3_seq_calculated = mul(E_2_seq, H_pseudo_init) # [M,M]\n",
    "beta_3_seq = tf.assign(beta_3, beta_3_seq_calculated) # [M,M]\n",
    "H_beta_3_seq = mul(beta_3_seq, t(mul(t(X_seq), alpha3_seq))) # [M,N]\n",
    "E_3_seq = Y_seq - (H_beta_3_seq +H_beta_2_seq + H_beta_1_seq )# [M,N]\n",
    "seq_train_graph = E_3_seq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.192263, train_accuracy: 0.735731\n",
      "test_loss: 0.189003, test_accuracy: 0.746715\n",
      "train_loss: 0.192259, train_accuracy: 0.735619\n",
      "test_loss: 0.189000, test_accuracy: 0.746715\n",
      "train_loss: 0.192255, train_accuracy: 0.735619\n",
      "test_loss: 0.188998, test_accuracy: 0.746866\n",
      "train_loss: 0.192250, train_accuracy: 0.735675\n",
      "test_loss: 0.188996, test_accuracy: 0.746866\n",
      "train_loss: 0.192246, train_accuracy: 0.735619\n",
      "test_loss: 0.188993, test_accuracy: 0.747168\n",
      "train_loss: 0.192241, train_accuracy: 0.735787\n",
      "test_loss: 0.188991, test_accuracy: 0.747017\n",
      "train_loss: 0.192237, train_accuracy: 0.735844\n",
      "test_loss: 0.188988, test_accuracy: 0.746866\n",
      "train_loss: 0.192232, train_accuracy: 0.735787\n",
      "test_loss: 0.188986, test_accuracy: 0.746866\n",
      "train_loss: 0.192228, train_accuracy: 0.735844\n",
      "test_loss: 0.188984, test_accuracy: 0.747017\n",
      "train_loss: 0.192224, train_accuracy: 0.735844\n",
      "test_loss: 0.188981, test_accuracy: 0.747017\n",
      "train_loss: 0.192219, train_accuracy: 0.735731\n",
      "test_loss: 0.188979, test_accuracy: 0.747168\n",
      "train_loss: 0.192215, train_accuracy: 0.735731\n",
      "test_loss: 0.188977, test_accuracy: 0.747168\n",
      "train_loss: 0.192211, train_accuracy: 0.735675\n",
      "test_loss: 0.188974, test_accuracy: 0.747168\n",
      "train_loss: 0.192207, train_accuracy: 0.735787\n",
      "test_loss: 0.188972, test_accuracy: 0.747319\n",
      "train_loss: 0.192202, train_accuracy: 0.735731\n",
      "test_loss: 0.188970, test_accuracy: 0.747319\n",
      "train_loss: 0.192198, train_accuracy: 0.735844\n",
      "test_loss: 0.188967, test_accuracy: 0.747168\n",
      "train_loss: 0.192194, train_accuracy: 0.735956\n",
      "test_loss: 0.188965, test_accuracy: 0.747168\n",
      "train_loss: 0.192190, train_accuracy: 0.735900\n",
      "test_loss: 0.188963, test_accuracy: 0.747168\n",
      "train_loss: 0.192186, train_accuracy: 0.735900\n",
      "test_loss: 0.188961, test_accuracy: 0.747168\n",
      "train_loss: 0.192182, train_accuracy: 0.735956\n",
      "test_loss: 0.188958, test_accuracy: 0.747319\n",
      "train_loss: 0.192178, train_accuracy: 0.736013\n",
      "test_loss: 0.188956, test_accuracy: 0.747319\n",
      "train_loss: 0.192174, train_accuracy: 0.735956\n",
      "test_loss: 0.188954, test_accuracy: 0.747319\n",
      "train_loss: 0.192170, train_accuracy: 0.736069\n",
      "test_loss: 0.188952, test_accuracy: 0.747168\n",
      "train_loss: 0.192166, train_accuracy: 0.736181\n",
      "test_loss: 0.188950, test_accuracy: 0.747168\n",
      "train_loss: 0.192162, train_accuracy: 0.736125\n",
      "test_loss: 0.188947, test_accuracy: 0.747168\n",
      "train_loss: 0.192159, train_accuracy: 0.736125\n",
      "test_loss: 0.188945, test_accuracy: 0.747168\n",
      "train_loss: 0.192155, train_accuracy: 0.736125\n",
      "test_loss: 0.188943, test_accuracy: 0.747168\n",
      "train_loss: 0.192151, train_accuracy: 0.736013\n",
      "test_loss: 0.188941, test_accuracy: 0.747319\n",
      "train_loss: 0.192147, train_accuracy: 0.736069\n",
      "test_loss: 0.188939, test_accuracy: 0.747319\n",
      "train_loss: 0.192144, train_accuracy: 0.736238\n",
      "test_loss: 0.188937, test_accuracy: 0.747470\n",
      "train_loss: 0.192140, train_accuracy: 0.736238\n",
      "test_loss: 0.188935, test_accuracy: 0.747470\n",
      "train_loss: 0.192137, train_accuracy: 0.736238\n",
      "test_loss: 0.188933, test_accuracy: 0.747621\n",
      "train_loss: 0.192133, train_accuracy: 0.736181\n",
      "test_loss: 0.188931, test_accuracy: 0.747621\n",
      "train_loss: 0.192129, train_accuracy: 0.736125\n",
      "test_loss: 0.188929, test_accuracy: 0.747470\n",
      "train_loss: 0.192126, train_accuracy: 0.736238\n",
      "test_loss: 0.188927, test_accuracy: 0.747772\n",
      "train_loss: 0.192122, train_accuracy: 0.736350\n",
      "test_loss: 0.188925, test_accuracy: 0.747772\n",
      "train_loss: 0.192119, train_accuracy: 0.736350\n",
      "test_loss: 0.188923, test_accuracy: 0.747772\n",
      "train_loss: 0.192116, train_accuracy: 0.736519\n",
      "test_loss: 0.188921, test_accuracy: 0.747772\n",
      "train_loss: 0.192112, train_accuracy: 0.736407\n",
      "test_loss: 0.188919, test_accuracy: 0.747923\n",
      "train_loss: 0.192109, train_accuracy: 0.736463\n",
      "test_loss: 0.188917, test_accuracy: 0.747923\n",
      "train_loss: 0.192106, train_accuracy: 0.736463\n",
      "test_loss: 0.188916, test_accuracy: 0.747923\n",
      "train_loss: 0.192102, train_accuracy: 0.736575\n",
      "test_loss: 0.188914, test_accuracy: 0.748074\n",
      "train_loss: 0.192099, train_accuracy: 0.736575\n",
      "test_loss: 0.188912, test_accuracy: 0.748074\n",
      "train_loss: 0.192096, train_accuracy: 0.736575\n",
      "test_loss: 0.188910, test_accuracy: 0.747923\n",
      "train_loss: 0.192093, train_accuracy: 0.736744\n",
      "test_loss: 0.188908, test_accuracy: 0.748074\n",
      "train_loss: 0.192090, train_accuracy: 0.736913\n",
      "test_loss: 0.188906, test_accuracy: 0.748225\n",
      "train_loss: 0.192086, train_accuracy: 0.737026\n",
      "test_loss: 0.188905, test_accuracy: 0.748678\n",
      "train_loss: 0.192083, train_accuracy: 0.737307\n",
      "test_loss: 0.188903, test_accuracy: 0.748829\n",
      "train_loss: 0.192080, train_accuracy: 0.737364\n",
      "test_loss: 0.188901, test_accuracy: 0.748829\n",
      "train_loss: 0.192077, train_accuracy: 0.737476\n",
      "test_loss: 0.188900, test_accuracy: 0.748829\n",
      "train_loss: 0.192074, train_accuracy: 0.737476\n",
      "test_loss: 0.188898, test_accuracy: 0.748829\n",
      "train_loss: 0.192071, train_accuracy: 0.737476\n",
      "test_loss: 0.188896, test_accuracy: 0.748829\n",
      "train_loss: 0.192068, train_accuracy: 0.737364\n",
      "test_loss: 0.188894, test_accuracy: 0.748678\n",
      "train_loss: 0.192065, train_accuracy: 0.737251\n",
      "test_loss: 0.188893, test_accuracy: 0.748678\n",
      "train_loss: 0.192062, train_accuracy: 0.737195\n",
      "test_loss: 0.188891, test_accuracy: 0.748678\n",
      "train_loss: 0.192060, train_accuracy: 0.737195\n",
      "test_loss: 0.188890, test_accuracy: 0.748527\n",
      "train_loss: 0.192057, train_accuracy: 0.737195\n",
      "test_loss: 0.188888, test_accuracy: 0.748678\n",
      "train_loss: 0.192054, train_accuracy: 0.737307\n",
      "test_loss: 0.188886, test_accuracy: 0.748678\n",
      "train_loss: 0.192051, train_accuracy: 0.737195\n",
      "test_loss: 0.188885, test_accuracy: 0.748829\n",
      "train_loss: 0.192048, train_accuracy: 0.737082\n",
      "test_loss: 0.188883, test_accuracy: 0.748829\n",
      "train_loss: 0.192046, train_accuracy: 0.737082\n",
      "test_loss: 0.188882, test_accuracy: 0.748829\n",
      "train_loss: 0.192043, train_accuracy: 0.737026\n",
      "test_loss: 0.188880, test_accuracy: 0.748829\n",
      "train_loss: 0.192040, train_accuracy: 0.737138\n",
      "test_loss: 0.188879, test_accuracy: 0.748678\n",
      "train_loss: 0.192037, train_accuracy: 0.737195\n",
      "test_loss: 0.188877, test_accuracy: 0.748829\n",
      "train_loss: 0.192035, train_accuracy: 0.737251\n",
      "test_loss: 0.188876, test_accuracy: 0.748678\n",
      "train_loss: 0.192032, train_accuracy: 0.737307\n",
      "test_loss: 0.188874, test_accuracy: 0.748678\n",
      "train_loss: 0.192029, train_accuracy: 0.737420\n",
      "test_loss: 0.188873, test_accuracy: 0.748829\n",
      "train_loss: 0.192027, train_accuracy: 0.737476\n",
      "test_loss: 0.188871, test_accuracy: 0.748829\n",
      "train_loss: 0.192024, train_accuracy: 0.737532\n",
      "test_loss: 0.188870, test_accuracy: 0.748829\n",
      "train_loss: 0.192022, train_accuracy: 0.737420\n",
      "test_loss: 0.188868, test_accuracy: 0.748829\n",
      "train_loss: 0.192019, train_accuracy: 0.737476\n",
      "test_loss: 0.188867, test_accuracy: 0.748678\n",
      "train_loss: 0.192017, train_accuracy: 0.737476\n",
      "test_loss: 0.188866, test_accuracy: 0.748527\n",
      "train_loss: 0.192014, train_accuracy: 0.737420\n",
      "test_loss: 0.188864, test_accuracy: 0.748829\n",
      "train_loss: 0.192012, train_accuracy: 0.737420\n",
      "test_loss: 0.188863, test_accuracy: 0.748678\n",
      "train_loss: 0.192009, train_accuracy: 0.737364\n",
      "test_loss: 0.188862, test_accuracy: 0.748527\n",
      "train_loss: 0.192007, train_accuracy: 0.737420\n",
      "test_loss: 0.188860, test_accuracy: 0.748678\n",
      "train_loss: 0.192005, train_accuracy: 0.737307\n",
      "test_loss: 0.188859, test_accuracy: 0.748678\n",
      "train_loss: 0.192002, train_accuracy: 0.737364\n",
      "test_loss: 0.188858, test_accuracy: 0.748829\n",
      "train_loss: 0.192000, train_accuracy: 0.737364\n",
      "test_loss: 0.188856, test_accuracy: 0.748981\n",
      "train_loss: 0.191998, train_accuracy: 0.737364\n",
      "test_loss: 0.188855, test_accuracy: 0.748981\n",
      "train_loss: 0.191995, train_accuracy: 0.737307\n",
      "test_loss: 0.188854, test_accuracy: 0.749132\n",
      "train_loss: 0.191993, train_accuracy: 0.737420\n",
      "test_loss: 0.188853, test_accuracy: 0.749283\n",
      "train_loss: 0.191991, train_accuracy: 0.737476\n",
      "test_loss: 0.188851, test_accuracy: 0.749283\n",
      "train_loss: 0.191988, train_accuracy: 0.737476\n",
      "test_loss: 0.188850, test_accuracy: 0.749283\n",
      "train_loss: 0.191986, train_accuracy: 0.737420\n",
      "test_loss: 0.188849, test_accuracy: 0.749283\n",
      "train_loss: 0.191984, train_accuracy: 0.737476\n",
      "test_loss: 0.188848, test_accuracy: 0.749585\n",
      "train_loss: 0.191982, train_accuracy: 0.737589\n",
      "test_loss: 0.188846, test_accuracy: 0.749585\n",
      "train_loss: 0.191980, train_accuracy: 0.737701\n",
      "test_loss: 0.188845, test_accuracy: 0.749585\n",
      "train_loss: 0.191977, train_accuracy: 0.737870\n",
      "test_loss: 0.188844, test_accuracy: 0.749736\n",
      "train_loss: 0.191975, train_accuracy: 0.737983\n",
      "test_loss: 0.188843, test_accuracy: 0.749736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.191973, train_accuracy: 0.738095\n",
      "test_loss: 0.188842, test_accuracy: 0.749887\n",
      "train_loss: 0.191971, train_accuracy: 0.738095\n",
      "test_loss: 0.188841, test_accuracy: 0.750189\n",
      "train_loss: 0.191969, train_accuracy: 0.738208\n",
      "test_loss: 0.188840, test_accuracy: 0.750491\n",
      "train_loss: 0.191967, train_accuracy: 0.738208\n",
      "test_loss: 0.188838, test_accuracy: 0.750642\n",
      "train_loss: 0.191965, train_accuracy: 0.738152\n",
      "test_loss: 0.188837, test_accuracy: 0.750793\n",
      "train_loss: 0.191963, train_accuracy: 0.738208\n",
      "test_loss: 0.188836, test_accuracy: 0.750793\n",
      "train_loss: 0.191961, train_accuracy: 0.738208\n",
      "test_loss: 0.188835, test_accuracy: 0.750944\n",
      "train_loss: 0.191959, train_accuracy: 0.738208\n",
      "test_loss: 0.188834, test_accuracy: 0.750944\n",
      "train_loss: 0.191957, train_accuracy: 0.738264\n",
      "test_loss: 0.188833, test_accuracy: 0.751246\n",
      "train_loss: 0.191955, train_accuracy: 0.738264\n",
      "test_loss: 0.188832, test_accuracy: 0.751095\n",
      "train_loss: 0.191953, train_accuracy: 0.738208\n",
      "test_loss: 0.188831, test_accuracy: 0.750944\n",
      "train_loss: 0.191951, train_accuracy: 0.738264\n",
      "test_loss: 0.188830, test_accuracy: 0.750944\n",
      "train_loss: 0.191949, train_accuracy: 0.738208\n",
      "test_loss: 0.188829, test_accuracy: 0.750944\n",
      "train_loss: 0.191947, train_accuracy: 0.738264\n",
      "test_loss: 0.188828, test_accuracy: 0.751095\n",
      "train_loss: 0.191945, train_accuracy: 0.738264\n",
      "test_loss: 0.188827, test_accuracy: 0.751095\n",
      "train_loss: 0.191943, train_accuracy: 0.738152\n",
      "test_loss: 0.188826, test_accuracy: 0.751095\n",
      "train_loss: 0.191942, train_accuracy: 0.738208\n",
      "test_loss: 0.188825, test_accuracy: 0.751095\n",
      "train_loss: 0.191940, train_accuracy: 0.738264\n",
      "test_loss: 0.188824, test_accuracy: 0.751095\n",
      "train_loss: 0.191938, train_accuracy: 0.738264\n",
      "test_loss: 0.188823, test_accuracy: 0.751246\n",
      "train_loss: 0.191936, train_accuracy: 0.738208\n",
      "test_loss: 0.188822, test_accuracy: 0.751246\n",
      "train_loss: 0.191934, train_accuracy: 0.738152\n",
      "test_loss: 0.188821, test_accuracy: 0.751246\n",
      "train_loss: 0.191933, train_accuracy: 0.738208\n",
      "test_loss: 0.188820, test_accuracy: 0.751397\n",
      "train_loss: 0.191931, train_accuracy: 0.738152\n",
      "test_loss: 0.188819, test_accuracy: 0.751397\n",
      "train_loss: 0.191929, train_accuracy: 0.738264\n",
      "test_loss: 0.188818, test_accuracy: 0.751548\n",
      "train_loss: 0.191927, train_accuracy: 0.738320\n",
      "test_loss: 0.188817, test_accuracy: 0.751548\n",
      "train_loss: 0.191926, train_accuracy: 0.738377\n",
      "test_loss: 0.188817, test_accuracy: 0.751548\n",
      "train_loss: 0.191924, train_accuracy: 0.738433\n",
      "test_loss: 0.188816, test_accuracy: 0.751397\n",
      "train_loss: 0.191922, train_accuracy: 0.738433\n",
      "test_loss: 0.188815, test_accuracy: 0.751850\n",
      "train_loss: 0.191921, train_accuracy: 0.738489\n",
      "test_loss: 0.188814, test_accuracy: 0.751548\n",
      "train_loss: 0.191919, train_accuracy: 0.738433\n",
      "test_loss: 0.188813, test_accuracy: 0.751548\n",
      "train_loss: 0.191917, train_accuracy: 0.738433\n",
      "test_loss: 0.188812, test_accuracy: 0.751548\n",
      "train_loss: 0.191916, train_accuracy: 0.738433\n",
      "test_loss: 0.188811, test_accuracy: 0.751548\n",
      "train_loss: 0.191914, train_accuracy: 0.738489\n",
      "test_loss: 0.188811, test_accuracy: 0.751548\n",
      "train_loss: 0.191912, train_accuracy: 0.738433\n",
      "test_loss: 0.188810, test_accuracy: 0.751548\n",
      "train_loss: 0.191911, train_accuracy: 0.738433\n",
      "test_loss: 0.188809, test_accuracy: 0.751699\n",
      "train_loss: 0.191909, train_accuracy: 0.738489\n",
      "test_loss: 0.188808, test_accuracy: 0.751699\n",
      "train_loss: 0.191908, train_accuracy: 0.738489\n",
      "test_loss: 0.188807, test_accuracy: 0.751548\n",
      "train_loss: 0.191906, train_accuracy: 0.738546\n",
      "test_loss: 0.188807, test_accuracy: 0.751548\n",
      "train_loss: 0.191905, train_accuracy: 0.738546\n",
      "test_loss: 0.188806, test_accuracy: 0.751548\n",
      "train_loss: 0.191903, train_accuracy: 0.738546\n",
      "test_loss: 0.188805, test_accuracy: 0.751548\n",
      "train_loss: 0.191902, train_accuracy: 0.738546\n",
      "test_loss: 0.188804, test_accuracy: 0.751699\n",
      "train_loss: 0.191900, train_accuracy: 0.738546\n",
      "test_loss: 0.188803, test_accuracy: 0.751850\n",
      "train_loss: 0.191899, train_accuracy: 0.738546\n",
      "test_loss: 0.188803, test_accuracy: 0.752001\n",
      "train_loss: 0.191897, train_accuracy: 0.738658\n",
      "test_loss: 0.188802, test_accuracy: 0.752001\n",
      "train_loss: 0.191896, train_accuracy: 0.738714\n",
      "test_loss: 0.188801, test_accuracy: 0.752001\n",
      "train_loss: 0.191894, train_accuracy: 0.738658\n",
      "test_loss: 0.188800, test_accuracy: 0.752152\n",
      "train_loss: 0.191893, train_accuracy: 0.738658\n",
      "test_loss: 0.188800, test_accuracy: 0.752001\n",
      "train_loss: 0.191891, train_accuracy: 0.738658\n",
      "test_loss: 0.188799, test_accuracy: 0.752001\n",
      "train_loss: 0.191890, train_accuracy: 0.738714\n",
      "test_loss: 0.188798, test_accuracy: 0.751850\n",
      "train_loss: 0.191889, train_accuracy: 0.738714\n",
      "test_loss: 0.188798, test_accuracy: 0.751850\n",
      "train_loss: 0.191887, train_accuracy: 0.738827\n",
      "test_loss: 0.188797, test_accuracy: 0.751850\n",
      "train_loss: 0.191886, train_accuracy: 0.738883\n",
      "test_loss: 0.188796, test_accuracy: 0.751850\n",
      "train_loss: 0.191884, train_accuracy: 0.738883\n",
      "test_loss: 0.188796, test_accuracy: 0.752001\n",
      "train_loss: 0.191883, train_accuracy: 0.738996\n",
      "test_loss: 0.188795, test_accuracy: 0.752152\n",
      "train_loss: 0.191882, train_accuracy: 0.739052\n",
      "test_loss: 0.188794, test_accuracy: 0.752152\n",
      "train_loss: 0.191880, train_accuracy: 0.738996\n",
      "test_loss: 0.188793, test_accuracy: 0.752152\n",
      "train_loss: 0.191879, train_accuracy: 0.739052\n",
      "test_loss: 0.188793, test_accuracy: 0.752152\n",
      "train_loss: 0.191878, train_accuracy: 0.739052\n",
      "test_loss: 0.188792, test_accuracy: 0.752001\n",
      "train_loss: 0.191876, train_accuracy: 0.739108\n",
      "test_loss: 0.188792, test_accuracy: 0.752152\n",
      "train_loss: 0.191875, train_accuracy: 0.739108\n",
      "test_loss: 0.188791, test_accuracy: 0.752152\n",
      "train_loss: 0.191874, train_accuracy: 0.739108\n",
      "test_loss: 0.188790, test_accuracy: 0.752303\n",
      "train_loss: 0.191872, train_accuracy: 0.739221\n",
      "test_loss: 0.188790, test_accuracy: 0.752303\n",
      "train_loss: 0.191871, train_accuracy: 0.739165\n",
      "test_loss: 0.188789, test_accuracy: 0.752303\n",
      "train_loss: 0.191870, train_accuracy: 0.739277\n",
      "test_loss: 0.188788, test_accuracy: 0.752303\n",
      "train_loss: 0.191869, train_accuracy: 0.739277\n",
      "test_loss: 0.188788, test_accuracy: 0.752454\n",
      "train_loss: 0.191867, train_accuracy: 0.739277\n",
      "test_loss: 0.188787, test_accuracy: 0.752303\n",
      "train_loss: 0.191866, train_accuracy: 0.739277\n",
      "test_loss: 0.188787, test_accuracy: 0.752303\n",
      "train_loss: 0.191865, train_accuracy: 0.739277\n",
      "test_loss: 0.188786, test_accuracy: 0.752303\n",
      "train_loss: 0.191864, train_accuracy: 0.739277\n",
      "test_loss: 0.188785, test_accuracy: 0.752303\n",
      "train_loss: 0.191863, train_accuracy: 0.739277\n",
      "test_loss: 0.188785, test_accuracy: 0.752303\n",
      "train_loss: 0.191861, train_accuracy: 0.739334\n",
      "test_loss: 0.188784, test_accuracy: 0.752303\n",
      "train_loss: 0.191860, train_accuracy: 0.739277\n",
      "test_loss: 0.188784, test_accuracy: 0.752303\n",
      "train_loss: 0.191859, train_accuracy: 0.739277\n",
      "test_loss: 0.188783, test_accuracy: 0.752303\n",
      "train_loss: 0.191858, train_accuracy: 0.739277\n",
      "test_loss: 0.188783, test_accuracy: 0.752303\n",
      "train_loss: 0.191857, train_accuracy: 0.739277\n",
      "test_loss: 0.188782, test_accuracy: 0.752303\n",
      "train_loss: 0.191856, train_accuracy: 0.739334\n",
      "test_loss: 0.188781, test_accuracy: 0.752303\n",
      "train_loss: 0.191854, train_accuracy: 0.739334\n",
      "test_loss: 0.188781, test_accuracy: 0.752303\n",
      "train_loss: 0.191853, train_accuracy: 0.739334\n",
      "test_loss: 0.188780, test_accuracy: 0.752152\n",
      "train_loss: 0.191852, train_accuracy: 0.739277\n",
      "test_loss: 0.188780, test_accuracy: 0.752303\n",
      "train_loss: 0.191851, train_accuracy: 0.739559\n",
      "test_loss: 0.188779, test_accuracy: 0.752303\n",
      "train_loss: 0.191850, train_accuracy: 0.739615\n",
      "test_loss: 0.188779, test_accuracy: 0.752152\n",
      "train_loss: 0.191849, train_accuracy: 0.739615\n",
      "test_loss: 0.188778, test_accuracy: 0.752152\n",
      "train_loss: 0.191848, train_accuracy: 0.739559\n",
      "test_loss: 0.188778, test_accuracy: 0.752152\n",
      "train_loss: 0.191847, train_accuracy: 0.739559\n",
      "test_loss: 0.188777, test_accuracy: 0.752152\n",
      "train_loss: 0.191846, train_accuracy: 0.739559\n",
      "test_loss: 0.188777, test_accuracy: 0.752152\n",
      "train_loss: 0.191845, train_accuracy: 0.739446\n",
      "test_loss: 0.188776, test_accuracy: 0.752303\n",
      "train_loss: 0.191844, train_accuracy: 0.739502\n",
      "test_loss: 0.188776, test_accuracy: 0.752303\n",
      "train_loss: 0.191843, train_accuracy: 0.739559\n",
      "test_loss: 0.188775, test_accuracy: 0.752303\n",
      "train_loss: 0.191841, train_accuracy: 0.739615\n",
      "test_loss: 0.188775, test_accuracy: 0.752152\n",
      "train_loss: 0.191840, train_accuracy: 0.739615\n",
      "test_loss: 0.188774, test_accuracy: 0.752001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.191839, train_accuracy: 0.739615\n",
      "test_loss: 0.188774, test_accuracy: 0.752001\n",
      "train_loss: 0.191838, train_accuracy: 0.739615\n",
      "test_loss: 0.188773, test_accuracy: 0.752303\n",
      "train_loss: 0.191837, train_accuracy: 0.739559\n",
      "test_loss: 0.188773, test_accuracy: 0.752454\n",
      "train_loss: 0.191836, train_accuracy: 0.739502\n",
      "test_loss: 0.188772, test_accuracy: 0.752454\n",
      "train_loss: 0.191835, train_accuracy: 0.739446\n",
      "test_loss: 0.188772, test_accuracy: 0.752454\n",
      "train_loss: 0.191834, train_accuracy: 0.739559\n",
      "test_loss: 0.188771, test_accuracy: 0.752454\n",
      "train_loss: 0.191833, train_accuracy: 0.739559\n",
      "test_loss: 0.188771, test_accuracy: 0.752454\n",
      "train_loss: 0.191832, train_accuracy: 0.739615\n",
      "test_loss: 0.188771, test_accuracy: 0.752454\n",
      "train_loss: 0.191832, train_accuracy: 0.739615\n",
      "test_loss: 0.188770, test_accuracy: 0.752454\n",
      "train_loss: 0.191831, train_accuracy: 0.739671\n",
      "test_loss: 0.188770, test_accuracy: 0.752303\n",
      "train_loss: 0.191830, train_accuracy: 0.739728\n",
      "test_loss: 0.188769, test_accuracy: 0.752303\n",
      "train_loss: 0.191829, train_accuracy: 0.739840\n",
      "test_loss: 0.188769, test_accuracy: 0.752303\n",
      "train_loss: 0.191828, train_accuracy: 0.739896\n",
      "test_loss: 0.188768, test_accuracy: 0.752303\n",
      "train_loss: 0.191827, train_accuracy: 0.739896\n",
      "test_loss: 0.188768, test_accuracy: 0.752454\n",
      "train_loss: 0.191826, train_accuracy: 0.739953\n",
      "test_loss: 0.188767, test_accuracy: 0.752454\n",
      "train_loss: 0.191825, train_accuracy: 0.739896\n",
      "test_loss: 0.188767, test_accuracy: 0.752454\n",
      "train_loss: 0.191824, train_accuracy: 0.739953\n",
      "test_loss: 0.188767, test_accuracy: 0.752454\n",
      "train_loss: 0.191823, train_accuracy: 0.739896\n",
      "test_loss: 0.188766, test_accuracy: 0.752303\n",
      "train_loss: 0.191822, train_accuracy: 0.739896\n",
      "test_loss: 0.188766, test_accuracy: 0.752303\n",
      "train_loss: 0.191821, train_accuracy: 0.739953\n",
      "test_loss: 0.188765, test_accuracy: 0.752303\n",
      "train_loss: 0.191821, train_accuracy: 0.739953\n",
      "test_loss: 0.188765, test_accuracy: 0.752303\n",
      "train_loss: 0.191820, train_accuracy: 0.739953\n",
      "test_loss: 0.188765, test_accuracy: 0.752152\n",
      "train_loss: 0.191819, train_accuracy: 0.739896\n",
      "test_loss: 0.188764, test_accuracy: 0.752303\n",
      "train_loss: 0.191818, train_accuracy: 0.739840\n",
      "test_loss: 0.188764, test_accuracy: 0.752303\n",
      "train_loss: 0.191817, train_accuracy: 0.739896\n",
      "test_loss: 0.188763, test_accuracy: 0.752152\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 1000\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "t1 = time.time()\n",
    "for epoch in range(205):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(seq_train_graph, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "t2 = time.time()\n",
    "s+=(t2-t1)/205\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\"\n",
    "#tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_train, Y: y_train})\n",
    "#ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_test, Y: y_test})\n",
    "#print(\"Sequential train training loss: \", tr_loss)\n",
    "#print(\"Sequential train training accuracy: \", tr_acc)\n",
    "#print(\"Sequential train testing loss: \", ts_loss)\n",
    "#print(\"Sequential train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11988424557011301"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.145569, train_accuracy: 0.684512\n",
      "test_loss: 0.152528, test_accuracy: 0.661046\n",
      "train_loss: 0.142330, train_accuracy: 0.719885\n",
      "test_loss: 0.149371, test_accuracy: 0.687184\n",
      "train_loss: 0.139225, train_accuracy: 0.748566\n",
      "test_loss: 0.146340, test_accuracy: 0.713322\n",
      "train_loss: 0.136255, train_accuracy: 0.778203\n",
      "test_loss: 0.143435, test_accuracy: 0.737774\n",
      "train_loss: 0.133418, train_accuracy: 0.810707\n",
      "test_loss: 0.140656, test_accuracy: 0.761383\n",
      "train_loss: 0.130713, train_accuracy: 0.828872\n",
      "test_loss: 0.138003, test_accuracy: 0.779089\n",
      "train_loss: 0.128138, train_accuracy: 0.845124\n",
      "test_loss: 0.135472, test_accuracy: 0.794266\n",
      "train_loss: 0.125690, train_accuracy: 0.855641\n",
      "test_loss: 0.133062, test_accuracy: 0.806914\n",
      "train_loss: 0.123365, train_accuracy: 0.868069\n",
      "test_loss: 0.130770, test_accuracy: 0.818718\n",
      "train_loss: 0.121160, train_accuracy: 0.879541\n",
      "test_loss: 0.128592, test_accuracy: 0.829680\n",
      "train_loss: 0.119071, train_accuracy: 0.884321\n",
      "test_loss: 0.126525, test_accuracy: 0.836425\n",
      "train_loss: 0.117093, train_accuracy: 0.890057\n",
      "test_loss: 0.124565, test_accuracy: 0.847386\n",
      "train_loss: 0.115221, train_accuracy: 0.895793\n",
      "test_loss: 0.122708, test_accuracy: 0.852445\n",
      "train_loss: 0.113453, train_accuracy: 0.900574\n",
      "test_loss: 0.120950, test_accuracy: 0.857504\n",
      "train_loss: 0.111782, train_accuracy: 0.901530\n",
      "test_loss: 0.119286, test_accuracy: 0.861720\n",
      "train_loss: 0.110205, train_accuracy: 0.904398\n",
      "test_loss: 0.117713, test_accuracy: 0.865936\n",
      "train_loss: 0.108717, train_accuracy: 0.905354\n",
      "test_loss: 0.116226, test_accuracy: 0.868465\n",
      "train_loss: 0.107313, train_accuracy: 0.906310\n",
      "test_loss: 0.114821, test_accuracy: 0.870995\n",
      "train_loss: 0.105990, train_accuracy: 0.909178\n",
      "test_loss: 0.113495, test_accuracy: 0.869309\n",
      "train_loss: 0.104743, train_accuracy: 0.909178\n",
      "test_loss: 0.112242, test_accuracy: 0.871838\n",
      "train_loss: 0.103568, train_accuracy: 0.910134\n",
      "test_loss: 0.111061, test_accuracy: 0.874368\n",
      "train_loss: 0.102462, train_accuracy: 0.909178\n",
      "test_loss: 0.109946, test_accuracy: 0.875211\n",
      "train_loss: 0.101421, train_accuracy: 0.913002\n",
      "test_loss: 0.108894, test_accuracy: 0.877740\n",
      "train_loss: 0.100440, train_accuracy: 0.913958\n",
      "test_loss: 0.107903, test_accuracy: 0.884486\n",
      "train_loss: 0.099517, train_accuracy: 0.915870\n",
      "test_loss: 0.106968, test_accuracy: 0.886172\n",
      "train_loss: 0.098649, train_accuracy: 0.915870\n",
      "test_loss: 0.106087, test_accuracy: 0.887858\n",
      "train_loss: 0.097831, train_accuracy: 0.916826\n",
      "test_loss: 0.105256, test_accuracy: 0.889545\n",
      "train_loss: 0.097063, train_accuracy: 0.915870\n",
      "test_loss: 0.104473, test_accuracy: 0.892917\n",
      "train_loss: 0.096339, train_accuracy: 0.915870\n",
      "test_loss: 0.103736, test_accuracy: 0.890388\n",
      "train_loss: 0.095659, train_accuracy: 0.916826\n",
      "test_loss: 0.103041, test_accuracy: 0.892074\n",
      "train_loss: 0.095019, train_accuracy: 0.915870\n",
      "test_loss: 0.102386, test_accuracy: 0.893761\n",
      "train_loss: 0.094417, train_accuracy: 0.916826\n",
      "test_loss: 0.101768, test_accuracy: 0.892917\n",
      "train_loss: 0.093850, train_accuracy: 0.916826\n",
      "test_loss: 0.101187, test_accuracy: 0.892917\n",
      "train_loss: 0.093318, train_accuracy: 0.919694\n",
      "test_loss: 0.100639, test_accuracy: 0.897133\n",
      "train_loss: 0.092817, train_accuracy: 0.919694\n",
      "test_loss: 0.100123, test_accuracy: 0.896290\n",
      "train_loss: 0.092346, train_accuracy: 0.920650\n",
      "test_loss: 0.099637, test_accuracy: 0.897976\n",
      "train_loss: 0.091903, train_accuracy: 0.920650\n",
      "test_loss: 0.099180, test_accuracy: 0.897976\n",
      "train_loss: 0.091487, train_accuracy: 0.921606\n",
      "test_loss: 0.098749, test_accuracy: 0.897133\n",
      "train_loss: 0.091095, train_accuracy: 0.920650\n",
      "test_loss: 0.098343, test_accuracy: 0.897133\n",
      "train_loss: 0.090727, train_accuracy: 0.920650\n",
      "test_loss: 0.097960, test_accuracy: 0.897133\n",
      "train_loss: 0.090381, train_accuracy: 0.923518\n",
      "test_loss: 0.097600, test_accuracy: 0.897976\n",
      "train_loss: 0.090056, train_accuracy: 0.923518\n",
      "test_loss: 0.097261, test_accuracy: 0.900506\n",
      "train_loss: 0.089751, train_accuracy: 0.923518\n",
      "test_loss: 0.096943, test_accuracy: 0.899663\n",
      "train_loss: 0.089464, train_accuracy: 0.923518\n",
      "test_loss: 0.096642, test_accuracy: 0.900506\n",
      "train_loss: 0.089194, train_accuracy: 0.925430\n",
      "test_loss: 0.096360, test_accuracy: 0.900506\n",
      "train_loss: 0.088941, train_accuracy: 0.926386\n",
      "test_loss: 0.096095, test_accuracy: 0.899663\n",
      "train_loss: 0.088704, train_accuracy: 0.926386\n",
      "test_loss: 0.095845, test_accuracy: 0.898820\n",
      "train_loss: 0.088481, train_accuracy: 0.926386\n",
      "test_loss: 0.095610, test_accuracy: 0.898820\n",
      "train_loss: 0.088272, train_accuracy: 0.928298\n",
      "test_loss: 0.095390, test_accuracy: 0.898820\n",
      "train_loss: 0.088076, train_accuracy: 0.930210\n",
      "test_loss: 0.095182, test_accuracy: 0.900506\n",
      "train_loss: 0.087892, train_accuracy: 0.930210\n",
      "test_loss: 0.094988, test_accuracy: 0.900506\n",
      "train_loss: 0.087720, train_accuracy: 0.930210\n",
      "test_loss: 0.094806, test_accuracy: 0.901349\n",
      "train_loss: 0.087559, train_accuracy: 0.930210\n",
      "test_loss: 0.094634, test_accuracy: 0.901349\n",
      "train_loss: 0.087408, train_accuracy: 0.931166\n",
      "test_loss: 0.094474, test_accuracy: 0.901349\n",
      "train_loss: 0.087268, train_accuracy: 0.931166\n",
      "test_loss: 0.094324, test_accuracy: 0.901349\n",
      "train_loss: 0.087136, train_accuracy: 0.931166\n",
      "test_loss: 0.094184, test_accuracy: 0.902192\n",
      "train_loss: 0.087014, train_accuracy: 0.930210\n",
      "test_loss: 0.094052, test_accuracy: 0.902192\n",
      "train_loss: 0.086900, train_accuracy: 0.930210\n",
      "test_loss: 0.093930, test_accuracy: 0.903035\n",
      "train_loss: 0.086794, train_accuracy: 0.931166\n",
      "test_loss: 0.093816, test_accuracy: 0.903035\n",
      "train_loss: 0.086695, train_accuracy: 0.931166\n",
      "test_loss: 0.093709, test_accuracy: 0.903879\n",
      "train_loss: 0.086604, train_accuracy: 0.931166\n",
      "test_loss: 0.093610, test_accuracy: 0.903879\n",
      "train_loss: 0.086519, train_accuracy: 0.932122\n",
      "test_loss: 0.093518, test_accuracy: 0.903879\n",
      "train_loss: 0.086441, train_accuracy: 0.932122\n",
      "test_loss: 0.093433, test_accuracy: 0.903879\n",
      "train_loss: 0.086369, train_accuracy: 0.932122\n",
      "test_loss: 0.093354, test_accuracy: 0.903879\n",
      "train_loss: 0.086303, train_accuracy: 0.931166\n",
      "test_loss: 0.093281, test_accuracy: 0.903879\n",
      "train_loss: 0.086242, train_accuracy: 0.931166\n",
      "test_loss: 0.093215, test_accuracy: 0.904722\n",
      "train_loss: 0.086187, train_accuracy: 0.932122\n",
      "test_loss: 0.093153, test_accuracy: 0.904722\n",
      "train_loss: 0.086137, train_accuracy: 0.931166\n",
      "test_loss: 0.093097, test_accuracy: 0.904722\n",
      "train_loss: 0.086091, train_accuracy: 0.931166\n",
      "test_loss: 0.093046, test_accuracy: 0.905565\n",
      "train_loss: 0.086050, train_accuracy: 0.931166\n",
      "test_loss: 0.092999, test_accuracy: 0.906408\n",
      "train_loss: 0.086014, train_accuracy: 0.931166\n",
      "test_loss: 0.092958, test_accuracy: 0.906408\n",
      "train_loss: 0.085981, train_accuracy: 0.931166\n",
      "test_loss: 0.092920, test_accuracy: 0.906408\n",
      "train_loss: 0.085953, train_accuracy: 0.932122\n",
      "test_loss: 0.092887, test_accuracy: 0.907251\n",
      "train_loss: 0.085928, train_accuracy: 0.932122\n",
      "test_loss: 0.092857, test_accuracy: 0.907251\n",
      "train_loss: 0.085907, train_accuracy: 0.934034\n",
      "test_loss: 0.092832, test_accuracy: 0.907251\n",
      "train_loss: 0.085889, train_accuracy: 0.934990\n",
      "test_loss: 0.092810, test_accuracy: 0.906408\n",
      "train_loss: 0.085875, train_accuracy: 0.934990\n",
      "test_loss: 0.092791, test_accuracy: 0.906408\n",
      "train_loss: 0.085863, train_accuracy: 0.934990\n",
      "test_loss: 0.092776, test_accuracy: 0.906408\n",
      "train_loss: 0.085855, train_accuracy: 0.934990\n",
      "test_loss: 0.092764, test_accuracy: 0.906408\n",
      "train_loss: 0.085850, train_accuracy: 0.934034\n",
      "test_loss: 0.092754, test_accuracy: 0.906408\n",
      "train_loss: 0.085847, train_accuracy: 0.934034\n",
      "test_loss: 0.092748, test_accuracy: 0.906408\n",
      "train_loss: 0.085847, train_accuracy: 0.934034\n",
      "test_loss: 0.092745, test_accuracy: 0.906408\n",
      "train_loss: 0.085849, train_accuracy: 0.934034\n",
      "test_loss: 0.092744, test_accuracy: 0.906408\n",
      "train_loss: 0.085854, train_accuracy: 0.934034\n",
      "test_loss: 0.092745, test_accuracy: 0.906408\n",
      "train_loss: 0.085861, train_accuracy: 0.934034\n",
      "test_loss: 0.092749, test_accuracy: 0.906408\n",
      "train_loss: 0.085871, train_accuracy: 0.935946\n",
      "test_loss: 0.092756, test_accuracy: 0.907251\n",
      "train_loss: 0.085882, train_accuracy: 0.935946\n",
      "test_loss: 0.092764, test_accuracy: 0.907251\n",
      "train_loss: 0.085896, train_accuracy: 0.935946\n",
      "test_loss: 0.092775, test_accuracy: 0.907251\n",
      "train_loss: 0.085912, train_accuracy: 0.935946\n",
      "test_loss: 0.092788, test_accuracy: 0.907251\n",
      "train_loss: 0.085929, train_accuracy: 0.935946\n",
      "test_loss: 0.092803, test_accuracy: 0.907251\n",
      "train_loss: 0.085948, train_accuracy: 0.936902\n",
      "test_loss: 0.092819, test_accuracy: 0.907251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.085969, train_accuracy: 0.936902\n",
      "test_loss: 0.092838, test_accuracy: 0.907251\n",
      "train_loss: 0.085992, train_accuracy: 0.936902\n",
      "test_loss: 0.092858, test_accuracy: 0.907251\n",
      "train_loss: 0.086016, train_accuracy: 0.936902\n",
      "test_loss: 0.092880, test_accuracy: 0.907251\n",
      "train_loss: 0.086041, train_accuracy: 0.936902\n",
      "test_loss: 0.092903, test_accuracy: 0.907251\n",
      "train_loss: 0.086069, train_accuracy: 0.936902\n",
      "test_loss: 0.092928, test_accuracy: 0.908094\n",
      "train_loss: 0.086097, train_accuracy: 0.936902\n",
      "test_loss: 0.092954, test_accuracy: 0.908094\n",
      "train_loss: 0.086127, train_accuracy: 0.936902\n",
      "test_loss: 0.092982, test_accuracy: 0.908094\n",
      "train_loss: 0.086158, train_accuracy: 0.936902\n",
      "test_loss: 0.093011, test_accuracy: 0.908094\n",
      "train_loss: 0.086191, train_accuracy: 0.936902\n",
      "test_loss: 0.093042, test_accuracy: 0.908094\n",
      "train_loss: 0.086224, train_accuracy: 0.936902\n",
      "test_loss: 0.093073, test_accuracy: 0.908094\n",
      "train_loss: 0.086259, train_accuracy: 0.936902\n",
      "test_loss: 0.093106, test_accuracy: 0.907251\n",
      "train_loss: 0.086295, train_accuracy: 0.936902\n",
      "test_loss: 0.093140, test_accuracy: 0.906408\n",
      "train_loss: 0.086332, train_accuracy: 0.936902\n",
      "test_loss: 0.093176, test_accuracy: 0.906408\n",
      "train_loss: 0.086370, train_accuracy: 0.936902\n",
      "test_loss: 0.093212, test_accuracy: 0.906408\n",
      "train_loss: 0.086409, train_accuracy: 0.936902\n",
      "test_loss: 0.093249, test_accuracy: 0.906408\n",
      "train_loss: 0.086449, train_accuracy: 0.936902\n",
      "test_loss: 0.093287, test_accuracy: 0.906408\n",
      "train_loss: 0.086490, train_accuracy: 0.936902\n",
      "test_loss: 0.093327, test_accuracy: 0.906408\n",
      "train_loss: 0.086532, train_accuracy: 0.936902\n",
      "test_loss: 0.093367, test_accuracy: 0.906408\n",
      "train_loss: 0.086574, train_accuracy: 0.936902\n",
      "test_loss: 0.093408, test_accuracy: 0.906408\n",
      "train_loss: 0.086618, train_accuracy: 0.936902\n",
      "test_loss: 0.093450, test_accuracy: 0.906408\n",
      "train_loss: 0.086662, train_accuracy: 0.936902\n",
      "test_loss: 0.093492, test_accuracy: 0.906408\n",
      "train_loss: 0.086707, train_accuracy: 0.936902\n",
      "test_loss: 0.093536, test_accuracy: 0.906408\n",
      "train_loss: 0.086753, train_accuracy: 0.936902\n",
      "test_loss: 0.093580, test_accuracy: 0.905565\n",
      "train_loss: 0.086799, train_accuracy: 0.936902\n",
      "test_loss: 0.093625, test_accuracy: 0.905565\n",
      "train_loss: 0.086846, train_accuracy: 0.936902\n",
      "test_loss: 0.093671, test_accuracy: 0.906408\n",
      "train_loss: 0.086894, train_accuracy: 0.936902\n",
      "test_loss: 0.093717, test_accuracy: 0.906408\n",
      "train_loss: 0.086942, train_accuracy: 0.936902\n",
      "test_loss: 0.093764, test_accuracy: 0.906408\n",
      "train_loss: 0.086991, train_accuracy: 0.936902\n",
      "test_loss: 0.093812, test_accuracy: 0.906408\n",
      "train_loss: 0.087041, train_accuracy: 0.937859\n",
      "test_loss: 0.093860, test_accuracy: 0.906408\n",
      "train_loss: 0.087091, train_accuracy: 0.937859\n",
      "test_loss: 0.093908, test_accuracy: 0.906408\n",
      "train_loss: 0.087141, train_accuracy: 0.937859\n",
      "test_loss: 0.093958, test_accuracy: 0.906408\n",
      "train_loss: 0.087192, train_accuracy: 0.937859\n",
      "test_loss: 0.094007, test_accuracy: 0.906408\n",
      "train_loss: 0.087244, train_accuracy: 0.937859\n",
      "test_loss: 0.094058, test_accuracy: 0.906408\n",
      "train_loss: 0.087296, train_accuracy: 0.938815\n",
      "test_loss: 0.094109, test_accuracy: 0.906408\n",
      "train_loss: 0.087349, train_accuracy: 0.938815\n",
      "test_loss: 0.094160, test_accuracy: 0.906408\n",
      "train_loss: 0.087401, train_accuracy: 0.938815\n",
      "test_loss: 0.094212, test_accuracy: 0.906408\n",
      "train_loss: 0.087455, train_accuracy: 0.938815\n",
      "test_loss: 0.094264, test_accuracy: 0.906408\n",
      "train_loss: 0.087509, train_accuracy: 0.938815\n",
      "test_loss: 0.094316, test_accuracy: 0.907251\n",
      "train_loss: 0.087563, train_accuracy: 0.938815\n",
      "test_loss: 0.094369, test_accuracy: 0.907251\n",
      "train_loss: 0.087617, train_accuracy: 0.939771\n",
      "test_loss: 0.094423, test_accuracy: 0.907251\n",
      "train_loss: 0.087672, train_accuracy: 0.939771\n",
      "test_loss: 0.094476, test_accuracy: 0.907251\n",
      "train_loss: 0.087727, train_accuracy: 0.939771\n",
      "test_loss: 0.094530, test_accuracy: 0.907251\n",
      "train_loss: 0.087783, train_accuracy: 0.939771\n",
      "test_loss: 0.094585, test_accuracy: 0.907251\n",
      "train_loss: 0.087839, train_accuracy: 0.939771\n",
      "test_loss: 0.094639, test_accuracy: 0.907251\n",
      "train_loss: 0.087895, train_accuracy: 0.938815\n",
      "test_loss: 0.094694, test_accuracy: 0.907251\n",
      "train_loss: 0.087951, train_accuracy: 0.938815\n",
      "test_loss: 0.094750, test_accuracy: 0.907251\n",
      "train_loss: 0.088008, train_accuracy: 0.938815\n",
      "test_loss: 0.094805, test_accuracy: 0.907251\n",
      "train_loss: 0.088065, train_accuracy: 0.938815\n",
      "test_loss: 0.094861, test_accuracy: 0.907251\n",
      "train_loss: 0.088122, train_accuracy: 0.938815\n",
      "test_loss: 0.094917, test_accuracy: 0.907251\n",
      "train_loss: 0.088179, train_accuracy: 0.937859\n",
      "test_loss: 0.094973, test_accuracy: 0.907251\n",
      "train_loss: 0.088237, train_accuracy: 0.937859\n",
      "test_loss: 0.095030, test_accuracy: 0.907251\n",
      "train_loss: 0.088295, train_accuracy: 0.937859\n",
      "test_loss: 0.095087, test_accuracy: 0.907251\n",
      "train_loss: 0.088353, train_accuracy: 0.937859\n",
      "test_loss: 0.095144, test_accuracy: 0.908094\n",
      "train_loss: 0.088411, train_accuracy: 0.937859\n",
      "test_loss: 0.095201, test_accuracy: 0.908094\n",
      "train_loss: 0.088469, train_accuracy: 0.938815\n",
      "test_loss: 0.095258, test_accuracy: 0.908094\n",
      "train_loss: 0.088528, train_accuracy: 0.938815\n",
      "test_loss: 0.095316, test_accuracy: 0.908094\n",
      "train_loss: 0.088586, train_accuracy: 0.938815\n",
      "test_loss: 0.095373, test_accuracy: 0.908094\n",
      "train_loss: 0.088645, train_accuracy: 0.938815\n",
      "test_loss: 0.095431, test_accuracy: 0.908094\n",
      "train_loss: 0.088704, train_accuracy: 0.938815\n",
      "test_loss: 0.095489, test_accuracy: 0.908938\n",
      "train_loss: 0.088763, train_accuracy: 0.938815\n",
      "test_loss: 0.095547, test_accuracy: 0.908938\n",
      "train_loss: 0.088822, train_accuracy: 0.938815\n",
      "test_loss: 0.095606, test_accuracy: 0.908094\n",
      "train_loss: 0.088882, train_accuracy: 0.939771\n",
      "test_loss: 0.095664, test_accuracy: 0.908094\n",
      "train_loss: 0.088941, train_accuracy: 0.939771\n",
      "test_loss: 0.095722, test_accuracy: 0.908094\n",
      "train_loss: 0.089001, train_accuracy: 0.939771\n",
      "test_loss: 0.095781, test_accuracy: 0.908094\n",
      "train_loss: 0.089060, train_accuracy: 0.939771\n",
      "test_loss: 0.095840, test_accuracy: 0.908094\n",
      "train_loss: 0.089120, train_accuracy: 0.939771\n",
      "test_loss: 0.095899, test_accuracy: 0.908938\n",
      "train_loss: 0.089180, train_accuracy: 0.939771\n",
      "test_loss: 0.095957, test_accuracy: 0.908938\n",
      "train_loss: 0.089239, train_accuracy: 0.939771\n",
      "test_loss: 0.096016, test_accuracy: 0.908938\n",
      "train_loss: 0.089299, train_accuracy: 0.939771\n",
      "test_loss: 0.096075, test_accuracy: 0.909781\n",
      "train_loss: 0.089359, train_accuracy: 0.939771\n",
      "test_loss: 0.096134, test_accuracy: 0.909781\n",
      "train_loss: 0.089419, train_accuracy: 0.939771\n",
      "test_loss: 0.096194, test_accuracy: 0.909781\n",
      "train_loss: 0.089479, train_accuracy: 0.939771\n",
      "test_loss: 0.096253, test_accuracy: 0.909781\n",
      "train_loss: 0.089539, train_accuracy: 0.939771\n",
      "test_loss: 0.096312, test_accuracy: 0.909781\n",
      "train_loss: 0.089599, train_accuracy: 0.939771\n",
      "test_loss: 0.096371, test_accuracy: 0.909781\n",
      "train_loss: 0.089659, train_accuracy: 0.939771\n",
      "test_loss: 0.096430, test_accuracy: 0.909781\n",
      "train_loss: 0.089719, train_accuracy: 0.939771\n",
      "test_loss: 0.096490, test_accuracy: 0.909781\n",
      "train_loss: 0.089779, train_accuracy: 0.939771\n",
      "test_loss: 0.096549, test_accuracy: 0.909781\n",
      "train_loss: 0.089840, train_accuracy: 0.939771\n",
      "test_loss: 0.096608, test_accuracy: 0.909781\n",
      "train_loss: 0.089900, train_accuracy: 0.940727\n",
      "test_loss: 0.096668, test_accuracy: 0.909781\n",
      "train_loss: 0.089960, train_accuracy: 0.940727\n",
      "test_loss: 0.096727, test_accuracy: 0.909781\n",
      "train_loss: 0.090020, train_accuracy: 0.941683\n",
      "test_loss: 0.096787, test_accuracy: 0.909781\n",
      "train_loss: 0.090080, train_accuracy: 0.941683\n",
      "test_loss: 0.096846, test_accuracy: 0.909781\n",
      "train_loss: 0.090140, train_accuracy: 0.941683\n",
      "test_loss: 0.096905, test_accuracy: 0.909781\n",
      "train_loss: 0.090200, train_accuracy: 0.941683\n",
      "test_loss: 0.096965, test_accuracy: 0.909781\n",
      "train_loss: 0.090260, train_accuracy: 0.942639\n",
      "test_loss: 0.097024, test_accuracy: 0.909781\n",
      "train_loss: 0.090320, train_accuracy: 0.942639\n",
      "test_loss: 0.097083, test_accuracy: 0.909781\n",
      "train_loss: 0.090379, train_accuracy: 0.942639\n",
      "test_loss: 0.097142, test_accuracy: 0.909781\n",
      "train_loss: 0.090439, train_accuracy: 0.943595\n",
      "test_loss: 0.097202, test_accuracy: 0.908938\n",
      "train_loss: 0.090499, train_accuracy: 0.943595\n",
      "test_loss: 0.097261, test_accuracy: 0.908938\n",
      "train_loss: 0.090559, train_accuracy: 0.943595\n",
      "test_loss: 0.097320, test_accuracy: 0.908938\n",
      "train_loss: 0.090618, train_accuracy: 0.943595\n",
      "test_loss: 0.097379, test_accuracy: 0.908938\n",
      "train_loss: 0.090678, train_accuracy: 0.943595\n",
      "test_loss: 0.097438, test_accuracy: 0.908094\n",
      "train_loss: 0.090737, train_accuracy: 0.943595\n",
      "test_loss: 0.097497, test_accuracy: 0.908094\n",
      "train_loss: 0.090797, train_accuracy: 0.943595\n",
      "test_loss: 0.097556, test_accuracy: 0.908094\n",
      "train_loss: 0.090856, train_accuracy: 0.943595\n",
      "test_loss: 0.097615, test_accuracy: 0.908094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.090915, train_accuracy: 0.943595\n",
      "test_loss: 0.097673, test_accuracy: 0.908094\n",
      "train_loss: 0.090975, train_accuracy: 0.943595\n",
      "test_loss: 0.097732, test_accuracy: 0.908094\n",
      "train_loss: 0.091034, train_accuracy: 0.944551\n",
      "test_loss: 0.097791, test_accuracy: 0.908094\n",
      "train_loss: 0.091093, train_accuracy: 0.944551\n",
      "test_loss: 0.097849, test_accuracy: 0.908938\n",
      "train_loss: 0.091151, train_accuracy: 0.944551\n",
      "test_loss: 0.097908, test_accuracy: 0.908938\n",
      "train_loss: 0.091210, train_accuracy: 0.944551\n",
      "test_loss: 0.097966, test_accuracy: 0.908094\n",
      "train_loss: 0.091269, train_accuracy: 0.944551\n",
      "test_loss: 0.098025, test_accuracy: 0.908094\n",
      "train_loss: 0.091328, train_accuracy: 0.946463\n",
      "test_loss: 0.098083, test_accuracy: 0.908094\n",
      "train_loss: 0.091386, train_accuracy: 0.946463\n",
      "test_loss: 0.098141, test_accuracy: 0.908094\n",
      "train_loss: 0.091444, train_accuracy: 0.946463\n",
      "test_loss: 0.098199, test_accuracy: 0.908094\n",
      "train_loss: 0.091503, train_accuracy: 0.946463\n",
      "test_loss: 0.098257, test_accuracy: 0.908094\n",
      "train_loss: 0.091561, train_accuracy: 0.946463\n",
      "test_loss: 0.098315, test_accuracy: 0.907251\n",
      "train_loss: 0.091619, train_accuracy: 0.946463\n",
      "test_loss: 0.098373, test_accuracy: 0.907251\n",
      "train_loss: 0.091677, train_accuracy: 0.947419\n",
      "test_loss: 0.098431, test_accuracy: 0.907251\n",
      "train_loss: 0.091735, train_accuracy: 0.947419\n",
      "test_loss: 0.098488, test_accuracy: 0.907251\n",
      "train_loss: 0.091792, train_accuracy: 0.947419\n",
      "test_loss: 0.098546, test_accuracy: 0.907251\n",
      "train_loss: 0.091850, train_accuracy: 0.947419\n",
      "test_loss: 0.098603, test_accuracy: 0.907251\n",
      "train_loss: 0.091907, train_accuracy: 0.947419\n",
      "test_loss: 0.098660, test_accuracy: 0.907251\n",
      "train_loss: 0.091964, train_accuracy: 0.947419\n",
      "test_loss: 0.098717, test_accuracy: 0.907251\n",
      "train_loss: 0.092022, train_accuracy: 0.947419\n",
      "test_loss: 0.098774, test_accuracy: 0.907251\n",
      "train_loss: 0.092079, train_accuracy: 0.947419\n",
      "test_loss: 0.098831, test_accuracy: 0.907251\n",
      "train_loss: 0.092135, train_accuracy: 0.947419\n",
      "test_loss: 0.098888, test_accuracy: 0.907251\n",
      "train_loss: 0.092192, train_accuracy: 0.947419\n",
      "test_loss: 0.098945, test_accuracy: 0.907251\n",
      "train_loss: 0.092249, train_accuracy: 0.947419\n",
      "test_loss: 0.099001, test_accuracy: 0.907251\n",
      "train_loss: 0.092305, train_accuracy: 0.947419\n",
      "test_loss: 0.099058, test_accuracy: 0.907251\n",
      "train_loss: 0.092361, train_accuracy: 0.947419\n",
      "test_loss: 0.099114, test_accuracy: 0.907251\n",
      "train_loss: 0.092418, train_accuracy: 0.947419\n",
      "test_loss: 0.099170, test_accuracy: 0.907251\n",
      "train_loss: 0.092474, train_accuracy: 0.947419\n",
      "test_loss: 0.099226, test_accuracy: 0.907251\n",
      "train_loss: 0.092529, train_accuracy: 0.947419\n",
      "test_loss: 0.099282, test_accuracy: 0.906408\n",
      "train_loss: 0.092585, train_accuracy: 0.947419\n",
      "test_loss: 0.099338, test_accuracy: 0.906408\n",
      "train_loss: 0.092641, train_accuracy: 0.948375\n",
      "test_loss: 0.099394, test_accuracy: 0.907251\n",
      "train_loss: 0.092696, train_accuracy: 0.948375\n",
      "test_loss: 0.099449, test_accuracy: 0.907251\n",
      "train_loss: 0.092751, train_accuracy: 0.947419\n",
      "test_loss: 0.099505, test_accuracy: 0.908094\n",
      "train_loss: 0.092806, train_accuracy: 0.947419\n",
      "test_loss: 0.099560, test_accuracy: 0.908094\n",
      "train_loss: 0.092861, train_accuracy: 0.947419\n",
      "test_loss: 0.099615, test_accuracy: 0.908938\n",
      "train_loss: 0.092916, train_accuracy: 0.947419\n",
      "test_loss: 0.099670, test_accuracy: 0.908938\n",
      "train_loss: 0.092970, train_accuracy: 0.947419\n",
      "test_loss: 0.099725, test_accuracy: 0.908938\n",
      "train_loss: 0.093025, train_accuracy: 0.947419\n",
      "test_loss: 0.099779, test_accuracy: 0.908938\n",
      "train_loss: 0.093079, train_accuracy: 0.947419\n",
      "test_loss: 0.099834, test_accuracy: 0.908938\n",
      "train_loss: 0.093133, train_accuracy: 0.947419\n",
      "test_loss: 0.099888, test_accuracy: 0.908938\n",
      "train_loss: 0.093187, train_accuracy: 0.947419\n",
      "test_loss: 0.099943, test_accuracy: 0.908938\n",
      "train_loss: 0.093241, train_accuracy: 0.947419\n",
      "test_loss: 0.099997, test_accuracy: 0.908938\n",
      "train_loss: 0.093294, train_accuracy: 0.948375\n",
      "test_loss: 0.100051, test_accuracy: 0.908938\n",
      "train_loss: 0.093347, train_accuracy: 0.948375\n",
      "test_loss: 0.100104, test_accuracy: 0.908938\n",
      "train_loss: 0.093401, train_accuracy: 0.948375\n",
      "test_loss: 0.100158, test_accuracy: 0.908938\n",
      "train_loss: 0.093454, train_accuracy: 0.948375\n",
      "test_loss: 0.100212, test_accuracy: 0.908938\n",
      "train_loss: 0.093506, train_accuracy: 0.948375\n",
      "test_loss: 0.100265, test_accuracy: 0.908938\n",
      "train_loss: 0.093559, train_accuracy: 0.948375\n",
      "test_loss: 0.100318, test_accuracy: 0.908938\n",
      "train_loss: 0.093612, train_accuracy: 0.948375\n",
      "test_loss: 0.100371, test_accuracy: 0.908938\n",
      "train_loss: 0.093664, train_accuracy: 0.948375\n",
      "test_loss: 0.100424, test_accuracy: 0.908938\n",
      "train_loss: 0.093716, train_accuracy: 0.948375\n",
      "test_loss: 0.100477, test_accuracy: 0.908938\n",
      "train_loss: 0.093768, train_accuracy: 0.948375\n",
      "test_loss: 0.100529, test_accuracy: 0.908938\n",
      "train_loss: 0.093820, train_accuracy: 0.948375\n",
      "test_loss: 0.100582, test_accuracy: 0.908938\n",
      "train_loss: 0.093871, train_accuracy: 0.948375\n",
      "test_loss: 0.100634, test_accuracy: 0.908938\n",
      "train_loss: 0.093923, train_accuracy: 0.949331\n",
      "test_loss: 0.100686, test_accuracy: 0.908938\n",
      "train_loss: 0.093974, train_accuracy: 0.949331\n",
      "test_loss: 0.100738, test_accuracy: 0.908938\n",
      "train_loss: 0.094025, train_accuracy: 0.949331\n",
      "test_loss: 0.100790, test_accuracy: 0.908938\n",
      "train_loss: 0.094076, train_accuracy: 0.949331\n",
      "test_loss: 0.100841, test_accuracy: 0.908938\n",
      "train_loss: 0.094126, train_accuracy: 0.949331\n",
      "test_loss: 0.100893, test_accuracy: 0.908938\n",
      "train_loss: 0.094177, train_accuracy: 0.949331\n",
      "test_loss: 0.100944, test_accuracy: 0.908938\n",
      "train_loss: 0.094227, train_accuracy: 0.949331\n",
      "test_loss: 0.100995, test_accuracy: 0.909781\n",
      "train_loss: 0.094277, train_accuracy: 0.949331\n",
      "test_loss: 0.101046, test_accuracy: 0.909781\n",
      "train_loss: 0.094327, train_accuracy: 0.949331\n",
      "test_loss: 0.101097, test_accuracy: 0.909781\n",
      "train_loss: 0.094377, train_accuracy: 0.949331\n",
      "test_loss: 0.101147, test_accuracy: 0.909781\n",
      "train_loss: 0.094426, train_accuracy: 0.949331\n",
      "test_loss: 0.101198, test_accuracy: 0.909781\n",
      "train_loss: 0.094476, train_accuracy: 0.950287\n",
      "test_loss: 0.101248, test_accuracy: 0.911467\n",
      "train_loss: 0.094525, train_accuracy: 0.950287\n",
      "test_loss: 0.101298, test_accuracy: 0.911467\n",
      "train_loss: 0.094574, train_accuracy: 0.950287\n",
      "test_loss: 0.101348, test_accuracy: 0.911467\n",
      "train_loss: 0.094622, train_accuracy: 0.950287\n",
      "test_loss: 0.101398, test_accuracy: 0.911467\n",
      "train_loss: 0.094671, train_accuracy: 0.950287\n",
      "test_loss: 0.101447, test_accuracy: 0.911467\n",
      "train_loss: 0.094719, train_accuracy: 0.950287\n",
      "test_loss: 0.101497, test_accuracy: 0.911467\n",
      "train_loss: 0.094768, train_accuracy: 0.950287\n",
      "test_loss: 0.101546, test_accuracy: 0.911467\n",
      "train_loss: 0.094816, train_accuracy: 0.950287\n",
      "test_loss: 0.101595, test_accuracy: 0.911467\n",
      "train_loss: 0.094863, train_accuracy: 0.950287\n",
      "test_loss: 0.101644, test_accuracy: 0.911467\n",
      "train_loss: 0.094911, train_accuracy: 0.950287\n",
      "test_loss: 0.101693, test_accuracy: 0.911467\n",
      "train_loss: 0.094958, train_accuracy: 0.950287\n",
      "test_loss: 0.101742, test_accuracy: 0.911467\n",
      "train_loss: 0.095006, train_accuracy: 0.950287\n",
      "test_loss: 0.101790, test_accuracy: 0.911467\n",
      "train_loss: 0.095053, train_accuracy: 0.950287\n",
      "test_loss: 0.101838, test_accuracy: 0.912310\n",
      "train_loss: 0.095099, train_accuracy: 0.950287\n",
      "test_loss: 0.101886, test_accuracy: 0.912310\n",
      "train_loss: 0.095146, train_accuracy: 0.950287\n",
      "test_loss: 0.101934, test_accuracy: 0.912310\n",
      "train_loss: 0.095193, train_accuracy: 0.950287\n",
      "test_loss: 0.101982, test_accuracy: 0.912310\n",
      "train_loss: 0.095239, train_accuracy: 0.950287\n",
      "test_loss: 0.102030, test_accuracy: 0.912310\n",
      "train_loss: 0.095285, train_accuracy: 0.950287\n",
      "test_loss: 0.102077, test_accuracy: 0.912310\n",
      "train_loss: 0.095331, train_accuracy: 0.950287\n",
      "test_loss: 0.102124, test_accuracy: 0.913153\n",
      "train_loss: 0.095376, train_accuracy: 0.950287\n",
      "test_loss: 0.102171, test_accuracy: 0.913153\n",
      "train_loss: 0.095422, train_accuracy: 0.950287\n",
      "test_loss: 0.102218, test_accuracy: 0.913153\n",
      "train_loss: 0.095467, train_accuracy: 0.950287\n",
      "test_loss: 0.102265, test_accuracy: 0.913153\n",
      "train_loss: 0.095512, train_accuracy: 0.950287\n",
      "test_loss: 0.102312, test_accuracy: 0.913153\n",
      "train_loss: 0.095557, train_accuracy: 0.950287\n",
      "test_loss: 0.102358, test_accuracy: 0.913153\n",
      "train_loss: 0.095602, train_accuracy: 0.950287\n",
      "test_loss: 0.102404, test_accuracy: 0.913153\n",
      "train_loss: 0.095646, train_accuracy: 0.950287\n",
      "test_loss: 0.102450, test_accuracy: 0.913153\n",
      "train_loss: 0.095691, train_accuracy: 0.950287\n",
      "test_loss: 0.102496, test_accuracy: 0.913153\n",
      "train_loss: 0.095735, train_accuracy: 0.950287\n",
      "test_loss: 0.102542, test_accuracy: 0.913153\n",
      "train_loss: 0.095779, train_accuracy: 0.950287\n",
      "test_loss: 0.102587, test_accuracy: 0.912310\n",
      "train_loss: 0.095822, train_accuracy: 0.950287\n",
      "test_loss: 0.102633, test_accuracy: 0.912310\n",
      "train_loss: 0.095866, train_accuracy: 0.950287\n",
      "test_loss: 0.102678, test_accuracy: 0.912310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.095909, train_accuracy: 0.950287\n",
      "test_loss: 0.102723, test_accuracy: 0.912310\n",
      "train_loss: 0.095953, train_accuracy: 0.950287\n",
      "test_loss: 0.102768, test_accuracy: 0.912310\n",
      "train_loss: 0.095996, train_accuracy: 0.950287\n",
      "test_loss: 0.102813, test_accuracy: 0.911467\n",
      "train_loss: 0.096038, train_accuracy: 0.950287\n",
      "test_loss: 0.102857, test_accuracy: 0.911467\n",
      "train_loss: 0.096081, train_accuracy: 0.950287\n",
      "test_loss: 0.102902, test_accuracy: 0.911467\n",
      "train_loss: 0.096123, train_accuracy: 0.950287\n",
      "test_loss: 0.102946, test_accuracy: 0.911467\n",
      "train_loss: 0.096165, train_accuracy: 0.950287\n",
      "test_loss: 0.102990, test_accuracy: 0.911467\n",
      "train_loss: 0.096207, train_accuracy: 0.950287\n",
      "test_loss: 0.103034, test_accuracy: 0.911467\n",
      "train_loss: 0.096249, train_accuracy: 0.950287\n",
      "test_loss: 0.103077, test_accuracy: 0.912310\n",
      "train_loss: 0.096291, train_accuracy: 0.950287\n",
      "test_loss: 0.103121, test_accuracy: 0.912310\n",
      "train_loss: 0.096332, train_accuracy: 0.950287\n",
      "test_loss: 0.103164, test_accuracy: 0.912310\n",
      "train_loss: 0.096374, train_accuracy: 0.950287\n",
      "test_loss: 0.103207, test_accuracy: 0.912310\n",
      "train_loss: 0.096415, train_accuracy: 0.950287\n",
      "test_loss: 0.103250, test_accuracy: 0.912310\n",
      "train_loss: 0.096456, train_accuracy: 0.950287\n",
      "test_loss: 0.103293, test_accuracy: 0.912310\n",
      "train_loss: 0.096496, train_accuracy: 0.950287\n",
      "test_loss: 0.103336, test_accuracy: 0.912310\n",
      "train_loss: 0.096537, train_accuracy: 0.950287\n",
      "test_loss: 0.103378, test_accuracy: 0.912310\n",
      "train_loss: 0.096577, train_accuracy: 0.950287\n",
      "test_loss: 0.103421, test_accuracy: 0.912310\n",
      "train_loss: 0.096617, train_accuracy: 0.950287\n",
      "test_loss: 0.103463, test_accuracy: 0.912310\n",
      "train_loss: 0.096657, train_accuracy: 0.950287\n",
      "test_loss: 0.103505, test_accuracy: 0.912310\n",
      "train_loss: 0.096697, train_accuracy: 0.950287\n",
      "test_loss: 0.103547, test_accuracy: 0.912310\n",
      "train_loss: 0.096736, train_accuracy: 0.950287\n",
      "test_loss: 0.103589, test_accuracy: 0.912310\n",
      "train_loss: 0.096776, train_accuracy: 0.950287\n",
      "test_loss: 0.103630, test_accuracy: 0.912310\n",
      "train_loss: 0.096815, train_accuracy: 0.950287\n",
      "test_loss: 0.103671, test_accuracy: 0.912310\n",
      "train_loss: 0.096854, train_accuracy: 0.950287\n",
      "test_loss: 0.103713, test_accuracy: 0.912310\n",
      "train_loss: 0.096893, train_accuracy: 0.950287\n",
      "test_loss: 0.103754, test_accuracy: 0.912310\n",
      "train_loss: 0.096931, train_accuracy: 0.950287\n",
      "test_loss: 0.103794, test_accuracy: 0.912310\n",
      "train_loss: 0.096970, train_accuracy: 0.951243\n",
      "test_loss: 0.103835, test_accuracy: 0.912310\n",
      "train_loss: 0.097008, train_accuracy: 0.951243\n",
      "test_loss: 0.103876, test_accuracy: 0.912310\n",
      "train_loss: 0.097046, train_accuracy: 0.951243\n",
      "test_loss: 0.103916, test_accuracy: 0.912310\n",
      "train_loss: 0.097084, train_accuracy: 0.951243\n",
      "test_loss: 0.103956, test_accuracy: 0.912310\n",
      "train_loss: 0.097121, train_accuracy: 0.951243\n",
      "test_loss: 0.103996, test_accuracy: 0.912310\n",
      "train_loss: 0.097159, train_accuracy: 0.951243\n",
      "test_loss: 0.104036, test_accuracy: 0.912310\n",
      "train_loss: 0.097196, train_accuracy: 0.951243\n",
      "test_loss: 0.104076, test_accuracy: 0.912310\n",
      "train_loss: 0.097233, train_accuracy: 0.951243\n",
      "test_loss: 0.104115, test_accuracy: 0.912310\n",
      "train_loss: 0.097270, train_accuracy: 0.951243\n",
      "test_loss: 0.104155, test_accuracy: 0.913153\n",
      "train_loss: 0.097307, train_accuracy: 0.951243\n",
      "test_loss: 0.104194, test_accuracy: 0.913153\n",
      "train_loss: 0.097344, train_accuracy: 0.952199\n",
      "test_loss: 0.104233, test_accuracy: 0.913153\n",
      "train_loss: 0.097380, train_accuracy: 0.952199\n",
      "test_loss: 0.104272, test_accuracy: 0.913153\n",
      "train_loss: 0.097416, train_accuracy: 0.952199\n",
      "test_loss: 0.104311, test_accuracy: 0.913153\n",
      "train_loss: 0.097452, train_accuracy: 0.952199\n",
      "test_loss: 0.104349, test_accuracy: 0.913153\n",
      "train_loss: 0.097488, train_accuracy: 0.952199\n",
      "test_loss: 0.104388, test_accuracy: 0.913153\n",
      "train_loss: 0.097524, train_accuracy: 0.952199\n",
      "test_loss: 0.104426, test_accuracy: 0.913153\n",
      "train_loss: 0.097560, train_accuracy: 0.953155\n",
      "test_loss: 0.104464, test_accuracy: 0.912310\n",
      "train_loss: 0.097595, train_accuracy: 0.953155\n",
      "test_loss: 0.104502, test_accuracy: 0.912310\n",
      "train_loss: 0.097630, train_accuracy: 0.953155\n",
      "test_loss: 0.104540, test_accuracy: 0.912310\n",
      "train_loss: 0.097665, train_accuracy: 0.953155\n",
      "test_loss: 0.104578, test_accuracy: 0.912310\n",
      "train_loss: 0.097700, train_accuracy: 0.953155\n",
      "test_loss: 0.104615, test_accuracy: 0.913153\n",
      "train_loss: 0.097735, train_accuracy: 0.953155\n",
      "test_loss: 0.104652, test_accuracy: 0.913153\n",
      "train_loss: 0.097769, train_accuracy: 0.953155\n",
      "test_loss: 0.104690, test_accuracy: 0.913153\n",
      "train_loss: 0.097803, train_accuracy: 0.953155\n",
      "test_loss: 0.104727, test_accuracy: 0.913153\n",
      "train_loss: 0.097837, train_accuracy: 0.953155\n",
      "test_loss: 0.104764, test_accuracy: 0.913153\n",
      "train_loss: 0.097871, train_accuracy: 0.953155\n",
      "test_loss: 0.104800, test_accuracy: 0.912310\n",
      "train_loss: 0.097905, train_accuracy: 0.953155\n",
      "test_loss: 0.104837, test_accuracy: 0.912310\n",
      "train_loss: 0.097939, train_accuracy: 0.953155\n",
      "test_loss: 0.104873, test_accuracy: 0.912310\n",
      "train_loss: 0.097972, train_accuracy: 0.953155\n",
      "test_loss: 0.104910, test_accuracy: 0.912310\n",
      "train_loss: 0.098005, train_accuracy: 0.953155\n",
      "test_loss: 0.104946, test_accuracy: 0.912310\n",
      "train_loss: 0.098038, train_accuracy: 0.953155\n",
      "test_loss: 0.104982, test_accuracy: 0.912310\n",
      "train_loss: 0.098071, train_accuracy: 0.953155\n",
      "test_loss: 0.105017, test_accuracy: 0.912310\n",
      "train_loss: 0.098104, train_accuracy: 0.953155\n",
      "test_loss: 0.105053, test_accuracy: 0.912310\n",
      "train_loss: 0.098137, train_accuracy: 0.953155\n",
      "test_loss: 0.105089, test_accuracy: 0.912310\n",
      "train_loss: 0.098169, train_accuracy: 0.954111\n",
      "test_loss: 0.105124, test_accuracy: 0.912310\n",
      "train_loss: 0.098201, train_accuracy: 0.954111\n",
      "test_loss: 0.105159, test_accuracy: 0.912310\n",
      "train_loss: 0.098233, train_accuracy: 0.954111\n",
      "test_loss: 0.105194, test_accuracy: 0.912310\n",
      "train_loss: 0.098265, train_accuracy: 0.954111\n",
      "test_loss: 0.105229, test_accuracy: 0.912310\n",
      "train_loss: 0.098297, train_accuracy: 0.954111\n",
      "test_loss: 0.105264, test_accuracy: 0.912310\n",
      "train_loss: 0.098329, train_accuracy: 0.954111\n",
      "test_loss: 0.105298, test_accuracy: 0.912310\n",
      "train_loss: 0.098360, train_accuracy: 0.954111\n",
      "test_loss: 0.105333, test_accuracy: 0.912310\n",
      "train_loss: 0.098391, train_accuracy: 0.954111\n",
      "test_loss: 0.105367, test_accuracy: 0.912310\n",
      "train_loss: 0.098422, train_accuracy: 0.954111\n",
      "test_loss: 0.105401, test_accuracy: 0.912310\n",
      "train_loss: 0.098453, train_accuracy: 0.954111\n",
      "test_loss: 0.105435, test_accuracy: 0.912310\n",
      "train_loss: 0.098484, train_accuracy: 0.954111\n",
      "test_loss: 0.105469, test_accuracy: 0.912310\n",
      "train_loss: 0.098515, train_accuracy: 0.954111\n",
      "test_loss: 0.105503, test_accuracy: 0.912310\n",
      "train_loss: 0.098545, train_accuracy: 0.954111\n",
      "test_loss: 0.105537, test_accuracy: 0.912310\n",
      "train_loss: 0.098575, train_accuracy: 0.954111\n",
      "test_loss: 0.105570, test_accuracy: 0.912310\n",
      "train_loss: 0.098606, train_accuracy: 0.954111\n",
      "test_loss: 0.105603, test_accuracy: 0.912310\n",
      "train_loss: 0.098636, train_accuracy: 0.954111\n",
      "test_loss: 0.105637, test_accuracy: 0.912310\n",
      "train_loss: 0.098665, train_accuracy: 0.954111\n",
      "test_loss: 0.105670, test_accuracy: 0.912310\n",
      "train_loss: 0.098695, train_accuracy: 0.954111\n",
      "test_loss: 0.105702, test_accuracy: 0.912310\n",
      "train_loss: 0.098724, train_accuracy: 0.954111\n",
      "test_loss: 0.105735, test_accuracy: 0.912310\n",
      "train_loss: 0.098754, train_accuracy: 0.954111\n",
      "test_loss: 0.105768, test_accuracy: 0.912310\n",
      "train_loss: 0.098783, train_accuracy: 0.954111\n",
      "test_loss: 0.105800, test_accuracy: 0.912310\n",
      "train_loss: 0.098812, train_accuracy: 0.954111\n",
      "test_loss: 0.105833, test_accuracy: 0.912310\n",
      "train_loss: 0.098841, train_accuracy: 0.954111\n",
      "test_loss: 0.105865, test_accuracy: 0.912310\n",
      "train_loss: 0.098870, train_accuracy: 0.954111\n",
      "test_loss: 0.105897, test_accuracy: 0.912310\n",
      "train_loss: 0.098898, train_accuracy: 0.954111\n",
      "test_loss: 0.105929, test_accuracy: 0.912310\n",
      "train_loss: 0.098927, train_accuracy: 0.955067\n",
      "test_loss: 0.105960, test_accuracy: 0.912310\n",
      "train_loss: 0.098955, train_accuracy: 0.955067\n",
      "test_loss: 0.105992, test_accuracy: 0.912310\n",
      "train_loss: 0.098983, train_accuracy: 0.955067\n",
      "test_loss: 0.106024, test_accuracy: 0.912310\n",
      "train_loss: 0.099011, train_accuracy: 0.955067\n",
      "test_loss: 0.106055, test_accuracy: 0.912310\n",
      "train_loss: 0.099039, train_accuracy: 0.955067\n",
      "test_loss: 0.106086, test_accuracy: 0.913997\n",
      "train_loss: 0.099066, train_accuracy: 0.955067\n",
      "test_loss: 0.106117, test_accuracy: 0.913997\n",
      "train_loss: 0.099094, train_accuracy: 0.955067\n",
      "test_loss: 0.106148, test_accuracy: 0.913997\n",
      "train_loss: 0.099121, train_accuracy: 0.955067\n",
      "test_loss: 0.106179, test_accuracy: 0.913997\n",
      "train_loss: 0.099148, train_accuracy: 0.955067\n",
      "test_loss: 0.106210, test_accuracy: 0.913997\n",
      "train_loss: 0.099175, train_accuracy: 0.955067\n",
      "test_loss: 0.106240, test_accuracy: 0.913997\n",
      "train_loss: 0.099202, train_accuracy: 0.955067\n",
      "test_loss: 0.106271, test_accuracy: 0.913997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.099229, train_accuracy: 0.955067\n",
      "test_loss: 0.106301, test_accuracy: 0.913997\n",
      "train_loss: 0.099256, train_accuracy: 0.954111\n",
      "test_loss: 0.106331, test_accuracy: 0.913997\n",
      "train_loss: 0.099282, train_accuracy: 0.954111\n",
      "test_loss: 0.106361, test_accuracy: 0.913997\n",
      "train_loss: 0.099308, train_accuracy: 0.954111\n",
      "test_loss: 0.106391, test_accuracy: 0.913997\n",
      "train_loss: 0.099335, train_accuracy: 0.954111\n",
      "test_loss: 0.106421, test_accuracy: 0.913997\n",
      "train_loss: 0.099361, train_accuracy: 0.954111\n",
      "test_loss: 0.106451, test_accuracy: 0.914840\n",
      "train_loss: 0.099387, train_accuracy: 0.954111\n",
      "test_loss: 0.106480, test_accuracy: 0.914840\n",
      "train_loss: 0.099412, train_accuracy: 0.954111\n",
      "test_loss: 0.106509, test_accuracy: 0.914840\n",
      "train_loss: 0.099438, train_accuracy: 0.954111\n",
      "test_loss: 0.106539, test_accuracy: 0.914840\n",
      "train_loss: 0.099463, train_accuracy: 0.954111\n",
      "test_loss: 0.106568, test_accuracy: 0.914840\n",
      "train_loss: 0.099489, train_accuracy: 0.954111\n",
      "test_loss: 0.106597, test_accuracy: 0.914840\n",
      "train_loss: 0.099514, train_accuracy: 0.955067\n",
      "test_loss: 0.106626, test_accuracy: 0.914840\n",
      "train_loss: 0.099539, train_accuracy: 0.955067\n",
      "test_loss: 0.106654, test_accuracy: 0.914840\n",
      "train_loss: 0.099564, train_accuracy: 0.955067\n",
      "test_loss: 0.106683, test_accuracy: 0.916526\n",
      "train_loss: 0.099589, train_accuracy: 0.955067\n",
      "test_loss: 0.106712, test_accuracy: 0.916526\n",
      "train_loss: 0.099613, train_accuracy: 0.954111\n",
      "test_loss: 0.106740, test_accuracy: 0.916526\n",
      "train_loss: 0.099638, train_accuracy: 0.954111\n",
      "test_loss: 0.106768, test_accuracy: 0.916526\n",
      "train_loss: 0.099662, train_accuracy: 0.954111\n",
      "test_loss: 0.106796, test_accuracy: 0.916526\n",
      "train_loss: 0.099687, train_accuracy: 0.954111\n",
      "test_loss: 0.106824, test_accuracy: 0.916526\n",
      "train_loss: 0.099711, train_accuracy: 0.954111\n",
      "test_loss: 0.106852, test_accuracy: 0.916526\n",
      "train_loss: 0.099735, train_accuracy: 0.954111\n",
      "test_loss: 0.106880, test_accuracy: 0.916526\n",
      "train_loss: 0.099758, train_accuracy: 0.954111\n",
      "test_loss: 0.106908, test_accuracy: 0.916526\n",
      "train_loss: 0.099782, train_accuracy: 0.954111\n",
      "test_loss: 0.106935, test_accuracy: 0.916526\n",
      "train_loss: 0.099806, train_accuracy: 0.954111\n",
      "test_loss: 0.106963, test_accuracy: 0.916526\n",
      "train_loss: 0.099829, train_accuracy: 0.954111\n",
      "test_loss: 0.106990, test_accuracy: 0.916526\n",
      "train_loss: 0.099853, train_accuracy: 0.954111\n",
      "test_loss: 0.107017, test_accuracy: 0.915683\n",
      "train_loss: 0.099876, train_accuracy: 0.954111\n",
      "test_loss: 0.107044, test_accuracy: 0.915683\n",
      "train_loss: 0.099899, train_accuracy: 0.954111\n",
      "test_loss: 0.107071, test_accuracy: 0.915683\n",
      "train_loss: 0.099922, train_accuracy: 0.954111\n",
      "test_loss: 0.107098, test_accuracy: 0.915683\n",
      "train_loss: 0.099945, train_accuracy: 0.954111\n",
      "test_loss: 0.107125, test_accuracy: 0.915683\n",
      "train_loss: 0.099967, train_accuracy: 0.954111\n",
      "test_loss: 0.107151, test_accuracy: 0.913997\n",
      "train_loss: 0.099990, train_accuracy: 0.954111\n",
      "test_loss: 0.107178, test_accuracy: 0.913997\n",
      "train_loss: 0.100012, train_accuracy: 0.954111\n",
      "test_loss: 0.107204, test_accuracy: 0.913997\n",
      "train_loss: 0.100034, train_accuracy: 0.954111\n",
      "test_loss: 0.107230, test_accuracy: 0.913997\n",
      "train_loss: 0.100057, train_accuracy: 0.954111\n",
      "test_loss: 0.107257, test_accuracy: 0.913997\n",
      "train_loss: 0.100079, train_accuracy: 0.954111\n",
      "test_loss: 0.107282, test_accuracy: 0.913997\n",
      "train_loss: 0.100101, train_accuracy: 0.954111\n",
      "test_loss: 0.107308, test_accuracy: 0.913997\n",
      "train_loss: 0.100122, train_accuracy: 0.954111\n",
      "test_loss: 0.107334, test_accuracy: 0.913997\n",
      "train_loss: 0.100144, train_accuracy: 0.954111\n",
      "test_loss: 0.107360, test_accuracy: 0.913153\n",
      "train_loss: 0.100166, train_accuracy: 0.954111\n",
      "test_loss: 0.107385, test_accuracy: 0.913153\n",
      "train_loss: 0.100187, train_accuracy: 0.954111\n",
      "test_loss: 0.107411, test_accuracy: 0.913153\n",
      "train_loss: 0.100208, train_accuracy: 0.954111\n",
      "test_loss: 0.107436, test_accuracy: 0.913153\n",
      "train_loss: 0.100229, train_accuracy: 0.954111\n",
      "test_loss: 0.107461, test_accuracy: 0.913153\n",
      "train_loss: 0.100251, train_accuracy: 0.954111\n",
      "test_loss: 0.107487, test_accuracy: 0.913153\n",
      "train_loss: 0.100272, train_accuracy: 0.954111\n",
      "test_loss: 0.107512, test_accuracy: 0.913153\n",
      "train_loss: 0.100292, train_accuracy: 0.954111\n",
      "test_loss: 0.107537, test_accuracy: 0.913153\n",
      "train_loss: 0.100313, train_accuracy: 0.954111\n",
      "test_loss: 0.107561, test_accuracy: 0.913153\n",
      "train_loss: 0.100334, train_accuracy: 0.954111\n",
      "test_loss: 0.107586, test_accuracy: 0.913153\n",
      "train_loss: 0.100354, train_accuracy: 0.954111\n",
      "test_loss: 0.107611, test_accuracy: 0.913153\n",
      "train_loss: 0.100375, train_accuracy: 0.954111\n",
      "test_loss: 0.107635, test_accuracy: 0.913153\n",
      "train_loss: 0.100395, train_accuracy: 0.954111\n",
      "test_loss: 0.107660, test_accuracy: 0.913153\n",
      "train_loss: 0.100415, train_accuracy: 0.954111\n",
      "test_loss: 0.107684, test_accuracy: 0.912310\n",
      "train_loss: 0.100435, train_accuracy: 0.954111\n",
      "test_loss: 0.107708, test_accuracy: 0.912310\n",
      "train_loss: 0.100455, train_accuracy: 0.954111\n",
      "test_loss: 0.107732, test_accuracy: 0.912310\n",
      "train_loss: 0.100475, train_accuracy: 0.954111\n",
      "test_loss: 0.107756, test_accuracy: 0.912310\n",
      "train_loss: 0.100494, train_accuracy: 0.954111\n",
      "test_loss: 0.107780, test_accuracy: 0.912310\n",
      "train_loss: 0.100514, train_accuracy: 0.954111\n",
      "test_loss: 0.107804, test_accuracy: 0.912310\n",
      "train_loss: 0.100533, train_accuracy: 0.954111\n",
      "test_loss: 0.107827, test_accuracy: 0.912310\n",
      "train_loss: 0.100553, train_accuracy: 0.954111\n",
      "test_loss: 0.107851, test_accuracy: 0.912310\n",
      "train_loss: 0.100572, train_accuracy: 0.954111\n",
      "test_loss: 0.107874, test_accuracy: 0.911467\n",
      "train_loss: 0.100591, train_accuracy: 0.954111\n",
      "test_loss: 0.107898, test_accuracy: 0.911467\n",
      "train_loss: 0.100610, train_accuracy: 0.954111\n",
      "test_loss: 0.107921, test_accuracy: 0.911467\n",
      "train_loss: 0.100629, train_accuracy: 0.954111\n",
      "test_loss: 0.107944, test_accuracy: 0.911467\n",
      "train_loss: 0.100648, train_accuracy: 0.954111\n",
      "test_loss: 0.107967, test_accuracy: 0.911467\n",
      "train_loss: 0.100666, train_accuracy: 0.954111\n",
      "test_loss: 0.107990, test_accuracy: 0.911467\n",
      "train_loss: 0.100685, train_accuracy: 0.954111\n",
      "test_loss: 0.108013, test_accuracy: 0.911467\n",
      "train_loss: 0.100704, train_accuracy: 0.954111\n",
      "test_loss: 0.108036, test_accuracy: 0.911467\n",
      "train_loss: 0.100722, train_accuracy: 0.954111\n",
      "test_loss: 0.108058, test_accuracy: 0.911467\n",
      "train_loss: 0.100740, train_accuracy: 0.955067\n",
      "test_loss: 0.108081, test_accuracy: 0.911467\n",
      "train_loss: 0.100758, train_accuracy: 0.955067\n",
      "test_loss: 0.108103, test_accuracy: 0.911467\n",
      "train_loss: 0.100776, train_accuracy: 0.955067\n",
      "test_loss: 0.108126, test_accuracy: 0.911467\n",
      "train_loss: 0.100794, train_accuracy: 0.955067\n",
      "test_loss: 0.108148, test_accuracy: 0.912310\n",
      "train_loss: 0.100812, train_accuracy: 0.955067\n",
      "test_loss: 0.108170, test_accuracy: 0.912310\n",
      "train_loss: 0.100830, train_accuracy: 0.955067\n",
      "test_loss: 0.108192, test_accuracy: 0.912310\n",
      "train_loss: 0.100848, train_accuracy: 0.955067\n",
      "test_loss: 0.108214, test_accuracy: 0.912310\n",
      "train_loss: 0.100865, train_accuracy: 0.955067\n",
      "test_loss: 0.108236, test_accuracy: 0.912310\n",
      "train_loss: 0.100883, train_accuracy: 0.955067\n",
      "test_loss: 0.108258, test_accuracy: 0.912310\n",
      "train_loss: 0.100900, train_accuracy: 0.955067\n",
      "test_loss: 0.108279, test_accuracy: 0.912310\n",
      "train_loss: 0.100917, train_accuracy: 0.955067\n",
      "test_loss: 0.108301, test_accuracy: 0.912310\n",
      "train_loss: 0.100934, train_accuracy: 0.955067\n",
      "test_loss: 0.108323, test_accuracy: 0.912310\n",
      "train_loss: 0.100951, train_accuracy: 0.955067\n",
      "test_loss: 0.108344, test_accuracy: 0.912310\n",
      "train_loss: 0.100968, train_accuracy: 0.955067\n",
      "test_loss: 0.108365, test_accuracy: 0.912310\n",
      "train_loss: 0.100985, train_accuracy: 0.955067\n",
      "test_loss: 0.108387, test_accuracy: 0.912310\n",
      "train_loss: 0.101002, train_accuracy: 0.955067\n",
      "test_loss: 0.108408, test_accuracy: 0.912310\n",
      "train_loss: 0.101019, train_accuracy: 0.955067\n",
      "test_loss: 0.108429, test_accuracy: 0.912310\n",
      "train_loss: 0.101035, train_accuracy: 0.955067\n",
      "test_loss: 0.108450, test_accuracy: 0.912310\n",
      "train_loss: 0.101052, train_accuracy: 0.955067\n",
      "test_loss: 0.108471, test_accuracy: 0.912310\n",
      "train_loss: 0.101068, train_accuracy: 0.955067\n",
      "test_loss: 0.108491, test_accuracy: 0.912310\n",
      "train_loss: 0.101084, train_accuracy: 0.955067\n",
      "test_loss: 0.108512, test_accuracy: 0.912310\n",
      "train_loss: 0.101100, train_accuracy: 0.955067\n",
      "test_loss: 0.108533, test_accuracy: 0.912310\n",
      "train_loss: 0.101116, train_accuracy: 0.955067\n",
      "test_loss: 0.108553, test_accuracy: 0.912310\n",
      "train_loss: 0.101132, train_accuracy: 0.955067\n",
      "test_loss: 0.108574, test_accuracy: 0.912310\n",
      "train_loss: 0.101148, train_accuracy: 0.955067\n",
      "test_loss: 0.108594, test_accuracy: 0.912310\n",
      "train_loss: 0.101164, train_accuracy: 0.955067\n",
      "test_loss: 0.108614, test_accuracy: 0.912310\n",
      "train_loss: 0.101180, train_accuracy: 0.955067\n",
      "test_loss: 0.108634, test_accuracy: 0.912310\n",
      "train_loss: 0.101195, train_accuracy: 0.955067\n",
      "test_loss: 0.108654, test_accuracy: 0.912310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.101211, train_accuracy: 0.955067\n",
      "test_loss: 0.108675, test_accuracy: 0.912310\n",
      "train_loss: 0.101226, train_accuracy: 0.955067\n",
      "test_loss: 0.108694, test_accuracy: 0.912310\n",
      "train_loss: 0.101242, train_accuracy: 0.955067\n",
      "test_loss: 0.108714, test_accuracy: 0.912310\n",
      "train_loss: 0.101257, train_accuracy: 0.955067\n",
      "test_loss: 0.108734, test_accuracy: 0.912310\n",
      "train_loss: 0.101272, train_accuracy: 0.955067\n",
      "test_loss: 0.108754, test_accuracy: 0.912310\n",
      "train_loss: 0.101287, train_accuracy: 0.955067\n",
      "test_loss: 0.108773, test_accuracy: 0.912310\n",
      "train_loss: 0.101302, train_accuracy: 0.955067\n",
      "test_loss: 0.108793, test_accuracy: 0.912310\n",
      "train_loss: 0.101317, train_accuracy: 0.955067\n",
      "test_loss: 0.108812, test_accuracy: 0.912310\n",
      "train_loss: 0.101332, train_accuracy: 0.955067\n",
      "test_loss: 0.108832, test_accuracy: 0.912310\n",
      "train_loss: 0.101347, train_accuracy: 0.955067\n",
      "test_loss: 0.108851, test_accuracy: 0.912310\n",
      "train_loss: 0.101361, train_accuracy: 0.955067\n",
      "test_loss: 0.108870, test_accuracy: 0.912310\n",
      "train_loss: 0.101376, train_accuracy: 0.955067\n",
      "test_loss: 0.108889, test_accuracy: 0.912310\n",
      "train_loss: 0.101390, train_accuracy: 0.955067\n",
      "test_loss: 0.108908, test_accuracy: 0.912310\n",
      "train_loss: 0.101405, train_accuracy: 0.955067\n",
      "test_loss: 0.108927, test_accuracy: 0.912310\n",
      "train_loss: 0.101419, train_accuracy: 0.955067\n",
      "test_loss: 0.108946, test_accuracy: 0.912310\n",
      "train_loss: 0.101433, train_accuracy: 0.955067\n",
      "test_loss: 0.108965, test_accuracy: 0.912310\n",
      "train_loss: 0.101448, train_accuracy: 0.955067\n",
      "test_loss: 0.108983, test_accuracy: 0.912310\n",
      "train_loss: 0.101462, train_accuracy: 0.955067\n",
      "test_loss: 0.109002, test_accuracy: 0.912310\n",
      "train_loss: 0.101476, train_accuracy: 0.955067\n",
      "test_loss: 0.109021, test_accuracy: 0.912310\n",
      "train_loss: 0.101489, train_accuracy: 0.955067\n",
      "test_loss: 0.109039, test_accuracy: 0.912310\n",
      "train_loss: 0.101503, train_accuracy: 0.955067\n",
      "test_loss: 0.109058, test_accuracy: 0.912310\n",
      "train_loss: 0.101517, train_accuracy: 0.955067\n",
      "test_loss: 0.109076, test_accuracy: 0.912310\n",
      "train_loss: 0.101531, train_accuracy: 0.955067\n",
      "test_loss: 0.109094, test_accuracy: 0.912310\n",
      "train_loss: 0.101544, train_accuracy: 0.956023\n",
      "test_loss: 0.109112, test_accuracy: 0.912310\n",
      "train_loss: 0.101558, train_accuracy: 0.956023\n",
      "test_loss: 0.109130, test_accuracy: 0.912310\n",
      "train_loss: 0.101571, train_accuracy: 0.956023\n",
      "test_loss: 0.109148, test_accuracy: 0.912310\n",
      "train_loss: 0.101584, train_accuracy: 0.956023\n",
      "test_loss: 0.109166, test_accuracy: 0.912310\n",
      "train_loss: 0.101598, train_accuracy: 0.956023\n",
      "test_loss: 0.109184, test_accuracy: 0.912310\n",
      "train_loss: 0.101611, train_accuracy: 0.956023\n",
      "test_loss: 0.109202, test_accuracy: 0.912310\n",
      "train_loss: 0.101624, train_accuracy: 0.956023\n",
      "test_loss: 0.109220, test_accuracy: 0.912310\n",
      "train_loss: 0.101637, train_accuracy: 0.956023\n",
      "test_loss: 0.109237, test_accuracy: 0.912310\n",
      "train_loss: 0.101650, train_accuracy: 0.956023\n",
      "test_loss: 0.109255, test_accuracy: 0.912310\n",
      "train_loss: 0.101663, train_accuracy: 0.956023\n",
      "test_loss: 0.109272, test_accuracy: 0.912310\n",
      "train_loss: 0.101676, train_accuracy: 0.956023\n",
      "test_loss: 0.109290, test_accuracy: 0.912310\n",
      "train_loss: 0.101688, train_accuracy: 0.956023\n",
      "test_loss: 0.109307, test_accuracy: 0.912310\n",
      "train_loss: 0.101701, train_accuracy: 0.956023\n",
      "test_loss: 0.109324, test_accuracy: 0.912310\n",
      "train_loss: 0.101714, train_accuracy: 0.956023\n",
      "test_loss: 0.109342, test_accuracy: 0.911467\n",
      "train_loss: 0.101726, train_accuracy: 0.956023\n",
      "test_loss: 0.109359, test_accuracy: 0.911467\n",
      "train_loss: 0.101739, train_accuracy: 0.956023\n",
      "test_loss: 0.109376, test_accuracy: 0.911467\n",
      "train_loss: 0.101751, train_accuracy: 0.956023\n",
      "test_loss: 0.109393, test_accuracy: 0.911467\n",
      "train_loss: 0.101763, train_accuracy: 0.956023\n",
      "test_loss: 0.109410, test_accuracy: 0.910624\n",
      "train_loss: 0.101775, train_accuracy: 0.956023\n",
      "test_loss: 0.109427, test_accuracy: 0.910624\n",
      "train_loss: 0.101788, train_accuracy: 0.956023\n",
      "test_loss: 0.109444, test_accuracy: 0.910624\n",
      "train_loss: 0.101800, train_accuracy: 0.956023\n",
      "test_loss: 0.109460, test_accuracy: 0.910624\n",
      "train_loss: 0.101812, train_accuracy: 0.956023\n",
      "test_loss: 0.109477, test_accuracy: 0.910624\n",
      "train_loss: 0.101824, train_accuracy: 0.956023\n",
      "test_loss: 0.109494, test_accuracy: 0.910624\n",
      "train_loss: 0.101835, train_accuracy: 0.956023\n",
      "test_loss: 0.109510, test_accuracy: 0.911467\n",
      "train_loss: 0.101847, train_accuracy: 0.956023\n",
      "test_loss: 0.109526, test_accuracy: 0.911467\n",
      "train_loss: 0.101859, train_accuracy: 0.956023\n",
      "test_loss: 0.109543, test_accuracy: 0.911467\n",
      "train_loss: 0.101871, train_accuracy: 0.956023\n",
      "test_loss: 0.109559, test_accuracy: 0.911467\n",
      "train_loss: 0.101882, train_accuracy: 0.956023\n",
      "test_loss: 0.109575, test_accuracy: 0.911467\n",
      "train_loss: 0.101894, train_accuracy: 0.956023\n",
      "test_loss: 0.109592, test_accuracy: 0.911467\n",
      "train_loss: 0.101905, train_accuracy: 0.956023\n",
      "test_loss: 0.109608, test_accuracy: 0.911467\n",
      "train_loss: 0.101916, train_accuracy: 0.956023\n",
      "test_loss: 0.109624, test_accuracy: 0.911467\n",
      "train_loss: 0.101928, train_accuracy: 0.956023\n",
      "test_loss: 0.109640, test_accuracy: 0.911467\n",
      "train_loss: 0.101939, train_accuracy: 0.956023\n",
      "test_loss: 0.109656, test_accuracy: 0.910624\n",
      "train_loss: 0.101950, train_accuracy: 0.956023\n",
      "test_loss: 0.109672, test_accuracy: 0.910624\n",
      "train_loss: 0.101961, train_accuracy: 0.956023\n",
      "test_loss: 0.109687, test_accuracy: 0.910624\n",
      "train_loss: 0.101972, train_accuracy: 0.956023\n",
      "test_loss: 0.109703, test_accuracy: 0.910624\n",
      "train_loss: 0.101983, train_accuracy: 0.956023\n",
      "test_loss: 0.109719, test_accuracy: 0.910624\n",
      "train_loss: 0.101994, train_accuracy: 0.956023\n",
      "test_loss: 0.109734, test_accuracy: 0.910624\n",
      "train_loss: 0.102005, train_accuracy: 0.956023\n",
      "test_loss: 0.109750, test_accuracy: 0.910624\n",
      "train_loss: 0.102016, train_accuracy: 0.956023\n",
      "test_loss: 0.109766, test_accuracy: 0.910624\n",
      "train_loss: 0.102027, train_accuracy: 0.956023\n",
      "test_loss: 0.109781, test_accuracy: 0.910624\n",
      "train_loss: 0.102037, train_accuracy: 0.956023\n",
      "test_loss: 0.109796, test_accuracy: 0.910624\n",
      "train_loss: 0.102048, train_accuracy: 0.956023\n",
      "test_loss: 0.109812, test_accuracy: 0.910624\n",
      "train_loss: 0.102058, train_accuracy: 0.956023\n",
      "test_loss: 0.109827, test_accuracy: 0.910624\n",
      "train_loss: 0.102069, train_accuracy: 0.956023\n",
      "test_loss: 0.109842, test_accuracy: 0.910624\n",
      "train_loss: 0.102079, train_accuracy: 0.956023\n",
      "test_loss: 0.109857, test_accuracy: 0.910624\n",
      "train_loss: 0.102090, train_accuracy: 0.956979\n",
      "test_loss: 0.109872, test_accuracy: 0.910624\n",
      "train_loss: 0.102100, train_accuracy: 0.956979\n",
      "test_loss: 0.109887, test_accuracy: 0.910624\n",
      "train_loss: 0.102110, train_accuracy: 0.956979\n",
      "test_loss: 0.109902, test_accuracy: 0.910624\n",
      "train_loss: 0.102120, train_accuracy: 0.956979\n",
      "test_loss: 0.109917, test_accuracy: 0.910624\n",
      "train_loss: 0.102130, train_accuracy: 0.956979\n",
      "test_loss: 0.109932, test_accuracy: 0.909781\n",
      "train_loss: 0.102140, train_accuracy: 0.956979\n",
      "test_loss: 0.109947, test_accuracy: 0.909781\n",
      "train_loss: 0.102150, train_accuracy: 0.956979\n",
      "test_loss: 0.109961, test_accuracy: 0.909781\n",
      "train_loss: 0.102160, train_accuracy: 0.956979\n",
      "test_loss: 0.109976, test_accuracy: 0.909781\n",
      "train_loss: 0.102170, train_accuracy: 0.956979\n",
      "test_loss: 0.109991, test_accuracy: 0.909781\n",
      "train_loss: 0.102180, train_accuracy: 0.956979\n",
      "test_loss: 0.110005, test_accuracy: 0.909781\n",
      "train_loss: 0.102190, train_accuracy: 0.956979\n",
      "test_loss: 0.110020, test_accuracy: 0.909781\n",
      "train_loss: 0.102199, train_accuracy: 0.956979\n",
      "test_loss: 0.110034, test_accuracy: 0.909781\n",
      "train_loss: 0.102209, train_accuracy: 0.956979\n",
      "test_loss: 0.110048, test_accuracy: 0.909781\n",
      "train_loss: 0.102219, train_accuracy: 0.956979\n",
      "test_loss: 0.110063, test_accuracy: 0.909781\n",
      "train_loss: 0.102228, train_accuracy: 0.956979\n",
      "test_loss: 0.110077, test_accuracy: 0.909781\n",
      "train_loss: 0.102238, train_accuracy: 0.956979\n",
      "test_loss: 0.110091, test_accuracy: 0.909781\n",
      "train_loss: 0.102247, train_accuracy: 0.956979\n",
      "test_loss: 0.110105, test_accuracy: 0.909781\n",
      "train_loss: 0.102256, train_accuracy: 0.956979\n",
      "test_loss: 0.110120, test_accuracy: 0.909781\n",
      "train_loss: 0.102266, train_accuracy: 0.956979\n",
      "test_loss: 0.110134, test_accuracy: 0.909781\n",
      "train_loss: 0.102275, train_accuracy: 0.956979\n",
      "test_loss: 0.110148, test_accuracy: 0.909781\n",
      "train_loss: 0.102284, train_accuracy: 0.956979\n",
      "test_loss: 0.110161, test_accuracy: 0.909781\n",
      "train_loss: 0.102293, train_accuracy: 0.956979\n",
      "test_loss: 0.110175, test_accuracy: 0.909781\n",
      "train_loss: 0.102302, train_accuracy: 0.956979\n",
      "test_loss: 0.110189, test_accuracy: 0.909781\n",
      "train_loss: 0.102311, train_accuracy: 0.956979\n",
      "test_loss: 0.110203, test_accuracy: 0.909781\n",
      "train_loss: 0.102320, train_accuracy: 0.956979\n",
      "test_loss: 0.110217, test_accuracy: 0.909781\n",
      "train_loss: 0.102329, train_accuracy: 0.956979\n",
      "test_loss: 0.110230, test_accuracy: 0.909781\n",
      "train_loss: 0.102338, train_accuracy: 0.956979\n",
      "test_loss: 0.110244, test_accuracy: 0.909781\n",
      "train_loss: 0.102347, train_accuracy: 0.956979\n",
      "test_loss: 0.110257, test_accuracy: 0.909781\n",
      "train_loss: 0.102356, train_accuracy: 0.956979\n",
      "test_loss: 0.110271, test_accuracy: 0.909781\n",
      "train_loss: 0.102364, train_accuracy: 0.956979\n",
      "test_loss: 0.110284, test_accuracy: 0.909781\n",
      "train_loss: 0.102373, train_accuracy: 0.956979\n",
      "test_loss: 0.110298, test_accuracy: 0.909781\n",
      "train_loss: 0.102382, train_accuracy: 0.956979\n",
      "test_loss: 0.110311, test_accuracy: 0.909781\n",
      "train_loss: 0.102390, train_accuracy: 0.956979\n",
      "test_loss: 0.110324, test_accuracy: 0.909781\n",
      "train_loss: 0.102399, train_accuracy: 0.956979\n",
      "test_loss: 0.110338, test_accuracy: 0.909781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.102407, train_accuracy: 0.956979\n",
      "test_loss: 0.110351, test_accuracy: 0.909781\n",
      "train_loss: 0.102416, train_accuracy: 0.956979\n",
      "test_loss: 0.110364, test_accuracy: 0.909781\n",
      "train_loss: 0.102424, train_accuracy: 0.956023\n",
      "test_loss: 0.110377, test_accuracy: 0.909781\n",
      "train_loss: 0.102432, train_accuracy: 0.956023\n",
      "test_loss: 0.110390, test_accuracy: 0.909781\n",
      "train_loss: 0.102440, train_accuracy: 0.956023\n",
      "test_loss: 0.110403, test_accuracy: 0.909781\n",
      "train_loss: 0.102449, train_accuracy: 0.956023\n",
      "test_loss: 0.110416, test_accuracy: 0.909781\n",
      "train_loss: 0.102457, train_accuracy: 0.956023\n",
      "test_loss: 0.110429, test_accuracy: 0.909781\n",
      "train_loss: 0.102465, train_accuracy: 0.956979\n",
      "test_loss: 0.110442, test_accuracy: 0.909781\n",
      "train_loss: 0.102473, train_accuracy: 0.956979\n",
      "test_loss: 0.110455, test_accuracy: 0.909781\n",
      "train_loss: 0.102481, train_accuracy: 0.956979\n",
      "test_loss: 0.110468, test_accuracy: 0.909781\n",
      "train_loss: 0.102489, train_accuracy: 0.956979\n",
      "test_loss: 0.110480, test_accuracy: 0.909781\n",
      "train_loss: 0.102497, train_accuracy: 0.957935\n",
      "test_loss: 0.110493, test_accuracy: 0.909781\n",
      "train_loss: 0.102505, train_accuracy: 0.957935\n",
      "test_loss: 0.110506, test_accuracy: 0.910624\n",
      "train_loss: 0.102513, train_accuracy: 0.957935\n",
      "test_loss: 0.110518, test_accuracy: 0.910624\n",
      "train_loss: 0.102520, train_accuracy: 0.957935\n",
      "test_loss: 0.110531, test_accuracy: 0.910624\n",
      "train_loss: 0.102528, train_accuracy: 0.957935\n",
      "test_loss: 0.110543, test_accuracy: 0.910624\n",
      "train_loss: 0.102536, train_accuracy: 0.957935\n",
      "test_loss: 0.110556, test_accuracy: 0.910624\n",
      "train_loss: 0.102543, train_accuracy: 0.957935\n",
      "test_loss: 0.110568, test_accuracy: 0.910624\n",
      "train_loss: 0.102551, train_accuracy: 0.957935\n",
      "test_loss: 0.110580, test_accuracy: 0.910624\n",
      "train_loss: 0.102559, train_accuracy: 0.957935\n",
      "test_loss: 0.110593, test_accuracy: 0.910624\n",
      "train_loss: 0.102566, train_accuracy: 0.957935\n",
      "test_loss: 0.110605, test_accuracy: 0.910624\n",
      "train_loss: 0.102574, train_accuracy: 0.957935\n",
      "test_loss: 0.110617, test_accuracy: 0.910624\n",
      "train_loss: 0.102581, train_accuracy: 0.957935\n",
      "test_loss: 0.110629, test_accuracy: 0.910624\n",
      "train_loss: 0.102588, train_accuracy: 0.957935\n",
      "test_loss: 0.110641, test_accuracy: 0.910624\n",
      "train_loss: 0.102596, train_accuracy: 0.958891\n",
      "test_loss: 0.110653, test_accuracy: 0.910624\n",
      "train_loss: 0.102603, train_accuracy: 0.958891\n",
      "test_loss: 0.110665, test_accuracy: 0.910624\n",
      "train_loss: 0.102610, train_accuracy: 0.958891\n",
      "test_loss: 0.110677, test_accuracy: 0.910624\n",
      "train_loss: 0.102617, train_accuracy: 0.958891\n",
      "test_loss: 0.110689, test_accuracy: 0.911467\n",
      "train_loss: 0.102625, train_accuracy: 0.958891\n",
      "test_loss: 0.110701, test_accuracy: 0.911467\n",
      "train_loss: 0.102632, train_accuracy: 0.958891\n",
      "test_loss: 0.110713, test_accuracy: 0.911467\n",
      "train_loss: 0.102639, train_accuracy: 0.958891\n",
      "test_loss: 0.110725, test_accuracy: 0.911467\n",
      "train_loss: 0.102646, train_accuracy: 0.958891\n",
      "test_loss: 0.110737, test_accuracy: 0.911467\n",
      "train_loss: 0.102653, train_accuracy: 0.958891\n",
      "test_loss: 0.110748, test_accuracy: 0.912310\n",
      "train_loss: 0.102660, train_accuracy: 0.958891\n",
      "test_loss: 0.110760, test_accuracy: 0.912310\n",
      "train_loss: 0.102667, train_accuracy: 0.958891\n",
      "test_loss: 0.110772, test_accuracy: 0.912310\n",
      "train_loss: 0.102674, train_accuracy: 0.958891\n",
      "test_loss: 0.110783, test_accuracy: 0.912310\n",
      "train_loss: 0.102680, train_accuracy: 0.958891\n",
      "test_loss: 0.110795, test_accuracy: 0.912310\n",
      "train_loss: 0.102687, train_accuracy: 0.958891\n",
      "test_loss: 0.110806, test_accuracy: 0.912310\n",
      "train_loss: 0.102694, train_accuracy: 0.958891\n",
      "test_loss: 0.110818, test_accuracy: 0.912310\n",
      "train_loss: 0.102701, train_accuracy: 0.958891\n",
      "test_loss: 0.110829, test_accuracy: 0.912310\n",
      "train_loss: 0.102707, train_accuracy: 0.958891\n",
      "test_loss: 0.110841, test_accuracy: 0.912310\n",
      "train_loss: 0.102714, train_accuracy: 0.959847\n",
      "test_loss: 0.110852, test_accuracy: 0.912310\n",
      "train_loss: 0.102720, train_accuracy: 0.959847\n",
      "test_loss: 0.110863, test_accuracy: 0.912310\n",
      "train_loss: 0.102727, train_accuracy: 0.959847\n",
      "test_loss: 0.110875, test_accuracy: 0.912310\n",
      "train_loss: 0.102734, train_accuracy: 0.960803\n",
      "test_loss: 0.110886, test_accuracy: 0.912310\n",
      "train_loss: 0.102740, train_accuracy: 0.961759\n",
      "test_loss: 0.110897, test_accuracy: 0.912310\n",
      "train_loss: 0.102746, train_accuracy: 0.961759\n",
      "test_loss: 0.110908, test_accuracy: 0.912310\n",
      "train_loss: 0.102753, train_accuracy: 0.961759\n",
      "test_loss: 0.110919, test_accuracy: 0.913153\n",
      "train_loss: 0.102759, train_accuracy: 0.961759\n",
      "test_loss: 0.110930, test_accuracy: 0.913153\n",
      "train_loss: 0.102766, train_accuracy: 0.961759\n",
      "test_loss: 0.110941, test_accuracy: 0.913997\n",
      "train_loss: 0.102772, train_accuracy: 0.961759\n",
      "test_loss: 0.110952, test_accuracy: 0.913997\n",
      "train_loss: 0.102778, train_accuracy: 0.961759\n",
      "test_loss: 0.110963, test_accuracy: 0.913997\n",
      "train_loss: 0.102784, train_accuracy: 0.961759\n",
      "test_loss: 0.110974, test_accuracy: 0.913997\n",
      "train_loss: 0.102790, train_accuracy: 0.961759\n",
      "test_loss: 0.110985, test_accuracy: 0.913997\n",
      "train_loss: 0.102797, train_accuracy: 0.961759\n",
      "test_loss: 0.110996, test_accuracy: 0.913997\n",
      "train_loss: 0.102803, train_accuracy: 0.961759\n",
      "test_loss: 0.111007, test_accuracy: 0.913997\n",
      "train_loss: 0.102809, train_accuracy: 0.961759\n",
      "test_loss: 0.111017, test_accuracy: 0.913997\n",
      "train_loss: 0.102815, train_accuracy: 0.961759\n",
      "test_loss: 0.111028, test_accuracy: 0.913997\n",
      "train_loss: 0.102821, train_accuracy: 0.961759\n",
      "test_loss: 0.111039, test_accuracy: 0.913997\n",
      "train_loss: 0.102827, train_accuracy: 0.961759\n",
      "test_loss: 0.111049, test_accuracy: 0.913997\n",
      "train_loss: 0.102833, train_accuracy: 0.961759\n",
      "test_loss: 0.111060, test_accuracy: 0.913997\n",
      "train_loss: 0.102839, train_accuracy: 0.961759\n",
      "test_loss: 0.111071, test_accuracy: 0.913997\n",
      "train_loss: 0.102844, train_accuracy: 0.961759\n",
      "test_loss: 0.111081, test_accuracy: 0.913997\n",
      "train_loss: 0.102850, train_accuracy: 0.961759\n",
      "test_loss: 0.111092, test_accuracy: 0.913997\n",
      "train_loss: 0.102856, train_accuracy: 0.961759\n",
      "test_loss: 0.111102, test_accuracy: 0.913997\n",
      "train_loss: 0.102862, train_accuracy: 0.961759\n",
      "test_loss: 0.111113, test_accuracy: 0.913997\n",
      "train_loss: 0.102867, train_accuracy: 0.961759\n",
      "test_loss: 0.111123, test_accuracy: 0.913997\n",
      "train_loss: 0.102873, train_accuracy: 0.961759\n",
      "test_loss: 0.111133, test_accuracy: 0.913997\n",
      "train_loss: 0.102879, train_accuracy: 0.961759\n",
      "test_loss: 0.111144, test_accuracy: 0.913997\n",
      "train_loss: 0.102884, train_accuracy: 0.961759\n",
      "test_loss: 0.111154, test_accuracy: 0.913997\n",
      "train_loss: 0.102890, train_accuracy: 0.961759\n",
      "test_loss: 0.111164, test_accuracy: 0.913997\n",
      "train_loss: 0.102896, train_accuracy: 0.961759\n",
      "test_loss: 0.111174, test_accuracy: 0.913997\n",
      "train_loss: 0.102901, train_accuracy: 0.961759\n",
      "test_loss: 0.111185, test_accuracy: 0.913997\n",
      "train_loss: 0.102907, train_accuracy: 0.961759\n",
      "test_loss: 0.111195, test_accuracy: 0.913997\n",
      "train_loss: 0.102912, train_accuracy: 0.961759\n",
      "test_loss: 0.111205, test_accuracy: 0.913997\n",
      "train_loss: 0.102917, train_accuracy: 0.961759\n",
      "test_loss: 0.111215, test_accuracy: 0.913997\n",
      "train_loss: 0.102923, train_accuracy: 0.961759\n",
      "test_loss: 0.111225, test_accuracy: 0.914840\n",
      "train_loss: 0.102928, train_accuracy: 0.961759\n",
      "test_loss: 0.111235, test_accuracy: 0.914840\n",
      "train_loss: 0.102934, train_accuracy: 0.961759\n",
      "test_loss: 0.111245, test_accuracy: 0.914840\n",
      "train_loss: 0.102939, train_accuracy: 0.961759\n",
      "test_loss: 0.111255, test_accuracy: 0.914840\n",
      "train_loss: 0.102944, train_accuracy: 0.961759\n",
      "test_loss: 0.111265, test_accuracy: 0.913997\n",
      "train_loss: 0.102949, train_accuracy: 0.961759\n",
      "test_loss: 0.111275, test_accuracy: 0.913997\n",
      "train_loss: 0.102955, train_accuracy: 0.961759\n",
      "test_loss: 0.111285, test_accuracy: 0.913997\n",
      "train_loss: 0.102960, train_accuracy: 0.960803\n",
      "test_loss: 0.111294, test_accuracy: 0.913997\n",
      "train_loss: 0.102965, train_accuracy: 0.960803\n",
      "test_loss: 0.111304, test_accuracy: 0.913997\n",
      "train_loss: 0.102970, train_accuracy: 0.960803\n",
      "test_loss: 0.111314, test_accuracy: 0.913997\n",
      "train_loss: 0.102975, train_accuracy: 0.960803\n",
      "test_loss: 0.111324, test_accuracy: 0.913997\n",
      "train_loss: 0.102980, train_accuracy: 0.960803\n",
      "test_loss: 0.111333, test_accuracy: 0.913997\n",
      "train_loss: 0.102985, train_accuracy: 0.960803\n",
      "test_loss: 0.111343, test_accuracy: 0.913997\n",
      "train_loss: 0.102990, train_accuracy: 0.960803\n",
      "test_loss: 0.111352, test_accuracy: 0.914840\n",
      "train_loss: 0.102995, train_accuracy: 0.960803\n",
      "test_loss: 0.111362, test_accuracy: 0.914840\n",
      "train_loss: 0.103000, train_accuracy: 0.960803\n",
      "test_loss: 0.111372, test_accuracy: 0.914840\n",
      "train_loss: 0.103005, train_accuracy: 0.960803\n",
      "test_loss: 0.111381, test_accuracy: 0.914840\n",
      "train_loss: 0.103010, train_accuracy: 0.960803\n",
      "test_loss: 0.111391, test_accuracy: 0.914840\n",
      "train_loss: 0.103015, train_accuracy: 0.960803\n",
      "test_loss: 0.111400, test_accuracy: 0.914840\n",
      "train_loss: 0.103020, train_accuracy: 0.960803\n",
      "test_loss: 0.111410, test_accuracy: 0.914840\n",
      "train_loss: 0.103024, train_accuracy: 0.960803\n",
      "test_loss: 0.111419, test_accuracy: 0.914840\n",
      "train_loss: 0.103029, train_accuracy: 0.960803\n",
      "test_loss: 0.111428, test_accuracy: 0.914840\n",
      "train_loss: 0.103034, train_accuracy: 0.960803\n",
      "test_loss: 0.111438, test_accuracy: 0.914840\n",
      "train_loss: 0.103039, train_accuracy: 0.960803\n",
      "test_loss: 0.111447, test_accuracy: 0.914840\n",
      "train_loss: 0.103043, train_accuracy: 0.960803\n",
      "test_loss: 0.111456, test_accuracy: 0.914840\n",
      "train_loss: 0.103048, train_accuracy: 0.960803\n",
      "test_loss: 0.111465, test_accuracy: 0.914840\n",
      "train_loss: 0.103052, train_accuracy: 0.960803\n",
      "test_loss: 0.111475, test_accuracy: 0.915683\n",
      "train_loss: 0.103057, train_accuracy: 0.960803\n",
      "test_loss: 0.111484, test_accuracy: 0.915683\n",
      "train_loss: 0.103062, train_accuracy: 0.960803\n",
      "test_loss: 0.111493, test_accuracy: 0.915683\n",
      "train_loss: 0.103066, train_accuracy: 0.960803\n",
      "test_loss: 0.111502, test_accuracy: 0.915683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.103071, train_accuracy: 0.960803\n",
      "test_loss: 0.111511, test_accuracy: 0.915683\n",
      "train_loss: 0.103075, train_accuracy: 0.960803\n",
      "test_loss: 0.111520, test_accuracy: 0.915683\n",
      "train_loss: 0.103080, train_accuracy: 0.960803\n",
      "test_loss: 0.111529, test_accuracy: 0.915683\n",
      "train_loss: 0.103084, train_accuracy: 0.960803\n",
      "test_loss: 0.111538, test_accuracy: 0.915683\n",
      "train_loss: 0.103089, train_accuracy: 0.960803\n",
      "test_loss: 0.111547, test_accuracy: 0.915683\n",
      "train_loss: 0.103093, train_accuracy: 0.959847\n",
      "test_loss: 0.111556, test_accuracy: 0.915683\n",
      "train_loss: 0.103097, train_accuracy: 0.959847\n",
      "test_loss: 0.111565, test_accuracy: 0.915683\n",
      "train_loss: 0.103102, train_accuracy: 0.959847\n",
      "test_loss: 0.111574, test_accuracy: 0.915683\n",
      "train_loss: 0.103106, train_accuracy: 0.959847\n",
      "test_loss: 0.111583, test_accuracy: 0.915683\n",
      "train_loss: 0.103110, train_accuracy: 0.959847\n",
      "test_loss: 0.111592, test_accuracy: 0.915683\n",
      "train_loss: 0.103115, train_accuracy: 0.959847\n",
      "test_loss: 0.111601, test_accuracy: 0.915683\n",
      "train_loss: 0.103119, train_accuracy: 0.959847\n",
      "test_loss: 0.111609, test_accuracy: 0.915683\n",
      "train_loss: 0.103123, train_accuracy: 0.959847\n",
      "test_loss: 0.111618, test_accuracy: 0.915683\n",
      "train_loss: 0.103127, train_accuracy: 0.959847\n",
      "test_loss: 0.111627, test_accuracy: 0.915683\n",
      "train_loss: 0.103131, train_accuracy: 0.959847\n",
      "test_loss: 0.111636, test_accuracy: 0.915683\n",
      "train_loss: 0.103136, train_accuracy: 0.959847\n",
      "test_loss: 0.111644, test_accuracy: 0.915683\n",
      "train_loss: 0.103140, train_accuracy: 0.959847\n",
      "test_loss: 0.111653, test_accuracy: 0.915683\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(700):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_2_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(40):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_1_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_ = subnet_output(alpha_1, beta_1, X_) \n",
    "logits__ = sess.run(logits_, feed_dict={X: [x_test[4000]]})\n",
    "print(logits__)\n",
    "print(np.argmax(logits__))\n",
    "print(y_test[4000])\n",
    "plt.imshow(x_test[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alpha(alpha, size):\n",
    "    tmp = sess.run(alpha)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            plt.subplot(2,5,i*5+j+1)\n",
    "            plt.imshow(np.reshape(tmp[:,i*5+j], [size,size]))\n",
    "\n",
    "def visualize_beta(beta):\n",
    "    tmp = sess.run(beta)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(tmp)\n",
    "    \n",
    "            \n",
    "\"\"\"visualize subnet nodes\"\"\"            \n",
    "visualize_alpha(alpha_1, 10)\n",
    "visualize_beta(beta_1)\n",
    "visualize_alpha(alpha_2, 10)\n",
    "visualize_beta(beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_elm import ML_ELM\n",
    "\n",
    "mlelm1 = ML_ELM(input_size=input_size, output_size=output_size, name='mlelm1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.math.sin(tf.constant([0.9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
