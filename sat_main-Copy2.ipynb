{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import mnist_handler\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"; \n",
    "INPUT_DIMENSION = 36\n",
    "NUM_CLASSES = 6\n",
    "\n",
    "train_dataset = np.array(pd.read_csv(\"Satimage_dataset/sat_trn.csv\",header = None, delimiter=' '))\n",
    "test_dataset = np.array(pd.read_csv(\"Satimage_dataset/sat_tst.csv\",header = None, delimiter=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 92 115 120 ... 113  87   3]\n",
      " [ 84 102 106 ... 104  79   3]\n",
      " [ 84 102 102 ... 104  79   3]\n",
      " ...\n",
      " [ 68  75 108 ... 104  85   4]\n",
      " [ 71  87 108 ... 104  85   4]\n",
      " [ 71  91 100 ... 100  81   4]]\n",
      "(4435, 37)\n",
      "[[ 80 102 102 ... 113  87   3]\n",
      " [ 76 102 102 ... 104  83   3]\n",
      " [ 80  98 106 ...  96  75   4]\n",
      " ...\n",
      " [ 56  68  91 ...  92  74   5]\n",
      " [ 56  68  87 ...  92  70   5]\n",
      " [ 60  71  91 ... 108  92   5]]\n",
      "(2000, 37)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(train_dataset.shape)\n",
    "print(test_dataset)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "y_train = train_dataset[:,36] -1\n",
    "y_test = test_dataset[:,36]-1\n",
    "x_train = train_dataset[:,0:36]\n",
    "x_test = test_dataset[:,0:36]\n",
    "print(y_test.shape)\n",
    "y_train = np.where(y_train < 6, y_train, 5)\n",
    "y_test = np.where(y_test < 6, y_test, 5)\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6435, 36)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = np.concatenate((x_train,x_test),axis = 0)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(total)\n",
    "total = scaler.transform(total)\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4435, 36)\n",
      "(2000, 36)\n",
      "[2 2 2 ... 3 3 3]\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "x_train = total[0:4435]\n",
    "#y_train = total[0:38,0]\n",
    "y_train = np.array(y_train, dtype=np.int16)\n",
    "x_test = total[4435:]\n",
    "#y_test = total[38:,0]\n",
    "y_test = np.array(y_test, dtype=np.int16)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 5, ..., 0, 0, 0], dtype=int16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.random.permutation(len(x_train))\n",
    "x_train = x_train[p]\n",
    "y_train = y_train[p]\n",
    "x_train.shape\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_elm import AUTO_ELM   # Our proposed algorithm\n",
    "n_input_nodes = 36\n",
    "n_hidden_nodes = 6  # used to be 256\n",
    "n_output_nodes = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "border = 435\n",
    "x_train_init = x_train[:border]\n",
    "x_train_seq = x_train[border:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "accuracy_sum = 0\n",
    "time_sum = 0\n",
    "for PPAP in range(10):\n",
    "\ttf.reset_default_graph()\n",
    "\t# ===========================================\n",
    "\t# Instantiate os-elm\n",
    "\t# ===========================================\n",
    "\tauto_elm = AUTO_ELM(\n",
    "\t\t# the number of input nodes.\n",
    "\t\tn_input_nodes=n_input_nodes,\n",
    "\t\t# the number of hidden nodes.\n",
    "\t\tn_hidden_nodes=n_hidden_nodes,\n",
    "\t\t# the number of output nodes.\n",
    "\t\tn_output_nodes=n_output_nodes,\n",
    "\t\t# loss function.\n",
    "\t\t# the default value is 'mean_squared_error'.\n",
    "\t\t# for the other functions, we support\n",
    "\t\t# 'mean_absolute_error', 'categorical_crossentropy', and 'binary_crossentropy'.\n",
    "\t\tc_value = 2.0**(-1),\n",
    "\t\tloss='mean_squared_error',\n",
    "\t\t# activation function applied to the hidden nodes.\n",
    "\t\t# the default value is 'sigmoid'.\n",
    "\t\t# for the other functions, we support 'linear' and 'tanh'.\n",
    "\t\t# NOTE: OS-ELM can apply an activation function only to the hidden nodes.\n",
    "\t\tactivation='sin',\n",
    "\t)\n",
    "\t# ===========================================\n",
    "\t# Training\n",
    "\t# ===========================================\n",
    "\t# the initial training phase\n",
    "\tt1 = time.time()\n",
    "\tauto_elm.init_train(x_train_init, x_train_init)\n",
    "\tt2 = time.time()\n",
    "\ttime_sum+=(t2-t1)\n",
    "\n",
    "\t# the sequential training phase\n",
    "\tbatch_size = 1000\n",
    "\n",
    "\tt1 = time.time()\n",
    "\tfor epoch in range(100):\n",
    "\t\tfor i in range(0, len(x_train_seq), batch_size):\n",
    "\t\t\tx_batch = x_train_seq[i:i+batch_size]\n",
    "\t\t\tif len(x_batch) != batch_size:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tauto_elm.seq_train(x_batch, x_batch)\n",
    "\tt2 = time.time()\n",
    "\ttime_sum+=(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4435, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_encoded = auto_elm.encoding(x_train)\n",
    "x_test_encoded = auto_elm.encoding(x_test)\n",
    "x_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "border = 435\n",
    "x_train_init = x_train_encoded[:border]\n",
    "y_train_init = y_train[:border]\n",
    "x_train_seq = x_train_encoded[border:]\n",
    "y_train_seq = y_train[border:]\n",
    "INPUT_DIMENSION = 6\n",
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # clear all the tensors\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "\"\"\"Placeholders\"\"\"\n",
    "X_ = tf.placeholder(tf.float32, [None, INPUT_DIMENSION])\n",
    "#X_ = tf.reshape(X, [-1, INPUT_DIMENSION]) # Flatten X: [N,D]\n",
    "Y = tf.placeholder(tf.int64, [None]) # labels\n",
    "Y_ = tf.one_hot(indices=Y, depth=NUM_CLASSES) # one_hot labels: [N,M]\n",
    "\n",
    "\"\"\"Some constants\"\"\"\n",
    "D = INPUT_DIMENSION\n",
    "M = NUM_CLASSES # Number of outputs\n",
    "C = tf.constant(2.0**(2))\n",
    "\n",
    "\"\"\"Weights\"\"\"\n",
    "alpha_1 = tf.get_variable('alpha_1',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 1st subnetwork\n",
    "alpha_2 = tf.get_variable('alpha_2',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False) # 2st subnetwork\n",
    "alpha_3 = tf.get_variable('alpha_3',shape=[D, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_1 = tf.get_variable('beta_1',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_2 = tf.get_variable('beta_2',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "beta_3 = tf.get_variable('beta_3',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.get_variable('k',shape=[D, D],initializer=tf.random_uniform_initializer(-1,1),trainable=False)\n",
    "m = tf.get_variable('m',shape=[M, M],initializer=tf.random_uniform_initializer(-1,1),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions\"\"\"\n",
    "def mul(A, B):\n",
    "    return tf.matmul(A, B)\n",
    "\n",
    "def inv(A):\n",
    "    return tf.matrix_inverse(A)\n",
    "\n",
    "def t(A):\n",
    "    return tf.transpose(A)\n",
    "\n",
    "def sin(A):\n",
    "    return tf.math.sin(A)\n",
    "\n",
    "def asin(A):\n",
    "    return tf.math.asin(A)\n",
    "\n",
    "def sqrt(A):\n",
    "    return tf.sqrt(A)\n",
    "\n",
    "def sqr(A):\n",
    "    return tf.math.pow(A, 2)\n",
    "\n",
    "def pseudo_inv(A, I, C):\n",
    "    C_I = I/C\n",
    "    return mul(t(A), inv(C_I + mul(A, t(A))))\n",
    "\n",
    "def h(A):\n",
    "    '''activation function'''\n",
    "    return sin(A)\n",
    "\n",
    "def h_(A):\n",
    "    '''inverse activation function'''\n",
    "    return asin(A)\n",
    "\n",
    "def u(A):\n",
    "    '''normalize the input to (0,1]'''\n",
    "    return tf.math.sigmoid(A) # sigmoid\n",
    "    \n",
    "def u_(A):\n",
    "    '''the inverse of u'''\n",
    "    ONE = tf.constant(1.0)\n",
    "    return -(tf.math.log(ONE/A - ONE)) # the inverse of sigmoid\n",
    "    \n",
    "def subnet_output(alpha, beta, A):\n",
    "    return t(mul(beta, h(mul(t(alpha), t(A))))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "'''some pre-computations'''\n",
    "X_init = t(X_) # [D,N]\n",
    "Y_init = t(Y_) # [M,N]\n",
    "N_init = D # number of dimensions\n",
    "I_DxD = tf.eye(N_init, dtype=tf.float32) # [D,D]\n",
    "I_MxM = tf.eye(M, dtype=tf.float32) # [M,M]\n",
    "C_I = I_DxD/C\n",
    "H_I = I_MxM/C\n",
    "\n",
    "add = C_I + mul(X_init, t(X_init))\n",
    "k = tf.assign(k,add)\n",
    "X_inv_init = pseudo_inv(X_init, I_DxD, C) # [N,D]\n",
    "\n",
    "'''1st subnet'''\n",
    "alpha_1_init_calculated = t(mul(h_(Y_init), X_inv_init)) # ([M,N]x[N,D])T=[D,M]\n",
    "alpha_1_init = tf.assign(alpha_1, alpha_1_init_calculated) # [D,M]\n",
    "H_1_init = h(mul(t(alpha_1_init), X_init)) # [M,N]\n",
    "H_add = H_I + mul(H_1_init,t(H_1_init))\n",
    "m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_1_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_1_init_calculated = mul(Y_init, t(H_1_init))/sqr(tf.norm(H_1_init)) # [M,M]\n",
    "beta_1_init_calculated = mul(Y_init,H_pseudo_init)\n",
    "\n",
    "beta_1_init = tf.assign(beta_1, beta_1_init_calculated) # [M,M]\n",
    "H_beta_1_init = mul(beta_1_init, t(mul(t(X_init), alpha_1_init))) # [M,N]\n",
    "E_1_init = Y_init - H_beta_1_init # [M,N]\n",
    "\n",
    "'''2nd subnet'''\n",
    "#alpha_2_init_calculated = t(mul(h_(E_1_init), X_inv_init)) # [D,M]    \n",
    "alpha_2_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_2_init = tf.assign(alpha_2, alpha_2_init_calculated) # [D,M]\n",
    "H_2_init = h(mul(t(alpha_2_init), X_init)) # [M,N]\n",
    "H_2_inv_init = pseudo_inv(H_2_init, I_MxM, C) # [M,N]\n",
    "H_add = H_I + mul(H_2_init,t(H_2_init))\n",
    "#m = tf.assign(m,H_add)\n",
    "H_pseudo_init = pseudo_inv(H_2_init,I_MxM,C) #[N,M]\n",
    "\n",
    "beta_2_init_calculated = mul(E_1_init, t(H_2_init))/sqr(tf.norm(H_2_init)) # [M,M]\n",
    "beta_2_init_calculated = mul(E_1_init, H_pseudo_init)\n",
    "\n",
    "beta_2_init = tf.assign(beta_2, beta_2_init_calculated) # [M,M]\n",
    "H_beta_2_init = mul(beta_2_init, t(mul(t(X_init), alpha_2_init))) # [M,N]\n",
    "E_2_init = Y_init - (H_beta_1_init+H_beta_2_init) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "alpha_3_init_calculated = t(mul(h_(E_2_init), X_inv_init)) # [D,M]    \n",
    "alpha_3_init_calculated = t(mul(asin(Y_init), X_inv_init)) # [D,M]\n",
    "\n",
    "alpha_3_init = tf.assign(alpha_3, alpha_3_init_calculated) # [D,M]\n",
    "H_3_init = h(mul(t(alpha_3_init), X_init)) # [M,N]\n",
    "H_3_inv_init = pseudo_inv(H_3_init, I_MxM, C) # [M,N]\n",
    "\n",
    "beta_3_init_calculated = mul(E_2_init, t(H_3_init))/sqr(tf.norm(H_3_init)) # [M,M]\n",
    "beta_3_init_calculated = mul(E_2_init, H_3_inv_init)\n",
    "\n",
    "beta_3_init = tf.assign(beta_3, beta_3_init_calculated) # [M,M]\n",
    "H_beta_3_init = mul(beta_3_init, t(mul(t(X_init), alpha_3_init))) # [M,N]\n",
    "E_3_init = Y_init - (H_beta_3_init+H_beta_2_init+ H_beta_1_init) # [M,N]\n",
    "\n",
    "#init_train_graph = H_beta_1_init\n",
    "init_train_graph = E_3_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_3:0' shape=(6, 6) dtype=float32_ref>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_1_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''With one subnetwork'''\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.11576978\n",
      "Initial train training accuracy:  0.33799323\n",
      "Initial train testing loss:  0.11593522\n",
      "Initial train testing accuracy:  0.343\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(E_1_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.11652064\n",
      "Initial train training accuracy:  0.36933485\n",
      "Initial train testing loss:  0.11689528\n",
      "Initial train testing accuracy:  0.373\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(E_2_init, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "#logits_init =  subnet_output(alpha_1, beta_1, X_)\n",
    "logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "loss_init = tf.losses.mean_squared_error(labels=Y_, predictions=logits_init)\n",
    "accuracy_init = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_init, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training done\n",
      "Initial train training loss:  0.11673035\n",
      "Initial train training accuracy:  0.39774522\n",
      "Initial train testing loss:  0.11748238\n",
      "Initial train testing accuracy:  0.3955\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial training\"\"\"\n",
    "sess.run(init_train_graph, feed_dict={X_: x_train_init, Y: y_train_init})\n",
    "print(\"Initial training done\")\n",
    "\n",
    "\"\"\"Initial training evaluation\"\"\"\n",
    "tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "print(\"Initial train training loss: \", tr_loss)\n",
    "print(\"Initial train training accuracy: \", tr_acc)\n",
    "print(\"Initial train testing loss: \", ts_loss)\n",
    "print(\"Initial train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_init = subnet_output(alpha_1, beta_1, X_)+ subnet_output(alpha_2, beta_2, X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "logic = sess.run(logits_init, feed_dict={X_ : [x_test_encoded[4]]})\n",
    "print(np.argmax(logic,axis =1))\n",
    "print(y_test[4])\n",
    "#plt.imshow(x_test[4200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "logic = sess.run(logits_init, feed_dict={X_ : [x_test_encoded[40]]})\n",
    "print(np.argmax(logic,axis =1))\n",
    "print(y_test[40])\n",
    "#plt.imshow(x_test[4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sequential Training Graph\"\"\"\n",
    "# D: input dimension\n",
    "# N: number of input samples\n",
    "# M: number of classes (number of outputs)\n",
    "X_seq = t(X_) # [D,N]\n",
    "Y_seq = t(Y_) # [M,N]\n",
    "pseudo = mul(X_seq, X_) #DXD\n",
    "k = tf.assign(k, tf.add(k,pseudo)) #DXD\n",
    "k_inv = inv(k)\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_1))\n",
    "alpha1_seq = tf.assign(alpha_1,tf.add(alpha_1,new)) #DXM\n",
    "H_1_seq = h(mul(t(alpha1_seq), X_seq)) # [M,N]\n",
    "m_su = mul(H_1_seq,t(H_1_seq))\n",
    "m = tf.assign(m,tf.add(m,m_su))\n",
    "m_inv = inv(m)\n",
    "#update = tf.matmul(tf.matmul(m_inv,H_1_seq),h_(Y_seq)- tf.matmul())\n",
    "H_pseudo_init = pseudo_inv(H_1_seq,I_MxM,C) #[N,M]\n",
    "#UPDATE = tf.matmul(tf.matmul(K_inverse, HT), inverse_acti_y - tf.matmul(H, self.__outputWeight))\n",
    "beta_1_seq_calculated = mul(Y_seq, H_pseudo_init) # [M,M]\n",
    "beta_1_seq = tf.assign(beta_1, beta_1_seq_calculated) # [M,M]\n",
    "H_beta_1_seq = mul(beta_1_seq, t(mul(X_, alpha1_seq))) # [M,N]\n",
    "E_1_seq = Y_seq - H_beta_1_seq # [M,N]\n",
    "\n",
    "'''2nd subnetwork'''\n",
    "\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_2))\n",
    "alpha2_seq = tf.assign(alpha_2,tf.add(alpha_2,new)) #DXM\n",
    "H_2_seq = h(mul(t(alpha2_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_2_seq,I_MxM,C) #[N,M]\n",
    "beta_2_seq_calculated = mul(E_1_seq, H_pseudo_init) # [M,M]\n",
    "beta_2_seq = tf.assign(beta_2, beta_2_seq_calculated) # [M,M]\n",
    "H_beta_2_seq = mul(beta_2_seq, t(mul(t(X_seq), alpha2_seq))) # [M,N]\n",
    "E_2_seq = Y_seq - (H_beta_2_seq+ H_beta_1_seq) # [M,N]\n",
    "\n",
    "'''3rd subnetwork'''\n",
    "new = tf.matmul(tf.matmul(k_inv, X_seq),h_(Y_) - tf.matmul(X_, alpha_3))\n",
    "alpha3_seq = tf.assign(alpha_3,tf.add(alpha_3,new)) #DXM\n",
    "H_3_seq = h(mul(t(alpha3_seq), X_seq)) # [M,N]\n",
    "H_pseudo_init = pseudo_inv(H_3_seq,I_MxM,C) #[N,M]\n",
    "beta_3_seq_calculated = mul(E_2_seq, H_pseudo_init) # [M,M]\n",
    "beta_3_seq = tf.assign(beta_3, beta_3_seq_calculated) # [M,M]\n",
    "H_beta_3_seq = mul(beta_3_seq, t(mul(t(X_seq), alpha3_seq))) # [M,N]\n",
    "E_3_seq = Y_seq - (H_beta_3_seq +H_beta_2_seq + H_beta_1_seq )# [M,N]\n",
    "seq_train_graph = E_3_seq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) + subnet_output(alpha_3, beta_3, X_)\n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.137019, train_accuracy: 0.371590\n",
      "test_loss: 0.141566, test_accuracy: 0.366500\n",
      "train_loss: 0.136059, train_accuracy: 0.340924\n",
      "test_loss: 0.140429, test_accuracy: 0.332000\n",
      "train_loss: 0.134589, train_accuracy: 0.307103\n",
      "test_loss: 0.138690, test_accuracy: 0.297000\n",
      "train_loss: 0.133487, train_accuracy: 0.298985\n",
      "test_loss: 0.137382, test_accuracy: 0.302000\n",
      "train_loss: 0.132733, train_accuracy: 0.316798\n",
      "test_loss: 0.136490, test_accuracy: 0.310000\n",
      "train_loss: 0.132226, train_accuracy: 0.335964\n",
      "test_loss: 0.135896, test_accuracy: 0.327000\n",
      "train_loss: 0.131879, train_accuracy: 0.359188\n",
      "test_loss: 0.135498, test_accuracy: 0.345000\n",
      "train_loss: 0.131635, train_accuracy: 0.375423\n",
      "test_loss: 0.135225, test_accuracy: 0.356500\n",
      "train_loss: 0.131450, train_accuracy: 0.385795\n",
      "test_loss: 0.135024, test_accuracy: 0.364000\n",
      "train_loss: 0.131302, train_accuracy: 0.391432\n",
      "test_loss: 0.134868, test_accuracy: 0.372500\n",
      "train_loss: 0.131183, train_accuracy: 0.395490\n",
      "test_loss: 0.134744, test_accuracy: 0.378000\n",
      "train_loss: 0.131074, train_accuracy: 0.397745\n",
      "test_loss: 0.134632, test_accuracy: 0.384000\n",
      "train_loss: 0.130977, train_accuracy: 0.399549\n",
      "test_loss: 0.134534, test_accuracy: 0.386000\n",
      "train_loss: 0.130886, train_accuracy: 0.402029\n",
      "test_loss: 0.134441, test_accuracy: 0.388000\n",
      "train_loss: 0.130795, train_accuracy: 0.404284\n",
      "test_loss: 0.134347, test_accuracy: 0.389500\n",
      "train_loss: 0.130710, train_accuracy: 0.406313\n",
      "test_loss: 0.134259, test_accuracy: 0.392000\n",
      "train_loss: 0.130624, train_accuracy: 0.407892\n",
      "test_loss: 0.134168, test_accuracy: 0.392000\n",
      "train_loss: 0.130538, train_accuracy: 0.408343\n",
      "test_loss: 0.134077, test_accuracy: 0.393000\n",
      "train_loss: 0.130454, train_accuracy: 0.407441\n",
      "test_loss: 0.133988, test_accuracy: 0.393000\n",
      "train_loss: 0.130367, train_accuracy: 0.408794\n",
      "test_loss: 0.133894, test_accuracy: 0.394000\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 1000\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(20):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(seq_train_graph, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\"\n",
    "#tr_loss, tr_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_train, Y: y_train})\n",
    "#ts_loss, ts_acc = sess.run([loss_init, accuracy_init], feed_dict={X: x_test, Y: y_test})\n",
    "#print(\"Sequential train training loss: \", tr_loss)\n",
    "#print(\"Sequential train training accuracy: \", tr_acc)\n",
    "#print(\"Sequential train testing loss: \", ts_loss)\n",
    "#print(\"Sequential train testing accuracy: \", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.119251, train_accuracy: 0.471251\n",
      "test_loss: 0.121382, test_accuracy: 0.463000\n",
      "train_loss: 0.118982, train_accuracy: 0.472604\n",
      "test_loss: 0.121083, test_accuracy: 0.464500\n",
      "train_loss: 0.118873, train_accuracy: 0.473732\n",
      "test_loss: 0.120963, test_accuracy: 0.464500\n",
      "train_loss: 0.118808, train_accuracy: 0.474183\n",
      "test_loss: 0.120892, test_accuracy: 0.466000\n",
      "train_loss: 0.118752, train_accuracy: 0.474634\n",
      "test_loss: 0.120831, test_accuracy: 0.467000\n",
      "train_loss: 0.118701, train_accuracy: 0.475536\n",
      "test_loss: 0.120774, test_accuracy: 0.467000\n",
      "train_loss: 0.118652, train_accuracy: 0.476212\n",
      "test_loss: 0.120720, test_accuracy: 0.467500\n",
      "train_loss: 0.118603, train_accuracy: 0.475536\n",
      "test_loss: 0.120666, test_accuracy: 0.467500\n",
      "train_loss: 0.118554, train_accuracy: 0.475310\n",
      "test_loss: 0.120611, test_accuracy: 0.468500\n",
      "train_loss: 0.118506, train_accuracy: 0.476212\n",
      "test_loss: 0.120557, test_accuracy: 0.469000\n",
      "train_loss: 0.118459, train_accuracy: 0.476437\n",
      "test_loss: 0.120505, test_accuracy: 0.469500\n",
      "train_loss: 0.118411, train_accuracy: 0.477565\n",
      "test_loss: 0.120452, test_accuracy: 0.469500\n",
      "train_loss: 0.118366, train_accuracy: 0.478241\n",
      "test_loss: 0.120400, test_accuracy: 0.470500\n",
      "train_loss: 0.118318, train_accuracy: 0.479594\n",
      "test_loss: 0.120347, test_accuracy: 0.471500\n",
      "train_loss: 0.118274, train_accuracy: 0.480496\n",
      "test_loss: 0.120297, test_accuracy: 0.471000\n",
      "train_loss: 0.118228, train_accuracy: 0.481173\n",
      "test_loss: 0.120246, test_accuracy: 0.471000\n",
      "train_loss: 0.118184, train_accuracy: 0.482074\n",
      "test_loss: 0.120196, test_accuracy: 0.472000\n",
      "train_loss: 0.118138, train_accuracy: 0.483653\n",
      "test_loss: 0.120145, test_accuracy: 0.473500\n",
      "train_loss: 0.118092, train_accuracy: 0.483653\n",
      "test_loss: 0.120093, test_accuracy: 0.472500\n",
      "train_loss: 0.118049, train_accuracy: 0.484555\n",
      "test_loss: 0.120044, test_accuracy: 0.472000\n",
      "train_loss: 0.118004, train_accuracy: 0.485231\n",
      "test_loss: 0.119994, test_accuracy: 0.473000\n",
      "train_loss: 0.117961, train_accuracy: 0.485457\n",
      "test_loss: 0.119946, test_accuracy: 0.473000\n",
      "train_loss: 0.117918, train_accuracy: 0.485682\n",
      "test_loss: 0.119897, test_accuracy: 0.474000\n",
      "train_loss: 0.117874, train_accuracy: 0.486359\n",
      "test_loss: 0.119848, test_accuracy: 0.475000\n",
      "train_loss: 0.117831, train_accuracy: 0.486809\n",
      "test_loss: 0.119800, test_accuracy: 0.475500\n",
      "train_loss: 0.117790, train_accuracy: 0.487486\n",
      "test_loss: 0.119754, test_accuracy: 0.475000\n",
      "train_loss: 0.117747, train_accuracy: 0.487711\n",
      "test_loss: 0.119705, test_accuracy: 0.475000\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 2000\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(27):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_2_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate initial-training graph\"\"\"\n",
    "logits_seq =  subnet_output(alpha_1, beta_1, X_) \n",
    "#logits_init = subnet_output(alpha_1, beta_1, X_) + subnet_output(alpha_2, beta_2, X_)\n",
    "loss_seq = tf.losses.mean_squared_error(labels=Y_, predictions=logits_seq)\n",
    "accuracy_seq = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_, axis=1), tf.argmax(logits_seq, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize variables\"\"\"\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.111231, train_accuracy: 0.499436\n",
      "test_loss: 0.111803, test_accuracy: 0.502000\n",
      "train_loss: 0.111465, train_accuracy: 0.479594\n",
      "test_loss: 0.111998, test_accuracy: 0.482500\n",
      "train_loss: 0.111246, train_accuracy: 0.480045\n",
      "test_loss: 0.111786, test_accuracy: 0.483500\n",
      "train_loss: 0.110977, train_accuracy: 0.487260\n",
      "test_loss: 0.111514, test_accuracy: 0.494000\n",
      "train_loss: 0.110690, train_accuracy: 0.500789\n",
      "test_loss: 0.111218, test_accuracy: 0.509500\n",
      "train_loss: 0.110410, train_accuracy: 0.511612\n",
      "test_loss: 0.110926, test_accuracy: 0.523500\n",
      "train_loss: 0.110151, train_accuracy: 0.529876\n",
      "test_loss: 0.110654, test_accuracy: 0.540500\n",
      "train_loss: 0.109921, train_accuracy: 0.544983\n",
      "test_loss: 0.110412, test_accuracy: 0.548500\n",
      "train_loss: 0.109722, train_accuracy: 0.557384\n",
      "test_loss: 0.110201, test_accuracy: 0.560000\n",
      "train_loss: 0.109551, train_accuracy: 0.567080\n",
      "test_loss: 0.110019, test_accuracy: 0.566000\n",
      "train_loss: 0.109405, train_accuracy: 0.574070\n",
      "test_loss: 0.109864, test_accuracy: 0.573000\n",
      "train_loss: 0.109281, train_accuracy: 0.578129\n",
      "test_loss: 0.109732, test_accuracy: 0.576500\n",
      "train_loss: 0.109176, train_accuracy: 0.579932\n",
      "test_loss: 0.109618, test_accuracy: 0.578500\n",
      "train_loss: 0.109085, train_accuracy: 0.582638\n",
      "test_loss: 0.109521, test_accuracy: 0.581500\n",
      "train_loss: 0.109008, train_accuracy: 0.585344\n",
      "test_loss: 0.109438, test_accuracy: 0.584000\n",
      "train_loss: 0.108941, train_accuracy: 0.587373\n",
      "test_loss: 0.109366, test_accuracy: 0.583500\n",
      "train_loss: 0.108883, train_accuracy: 0.586922\n",
      "test_loss: 0.109303, test_accuracy: 0.582500\n",
      "train_loss: 0.108832, train_accuracy: 0.585344\n",
      "test_loss: 0.109248, test_accuracy: 0.581000\n",
      "train_loss: 0.108787, train_accuracy: 0.583315\n",
      "test_loss: 0.109200, test_accuracy: 0.579500\n",
      "train_loss: 0.108748, train_accuracy: 0.583315\n",
      "test_loss: 0.109157, test_accuracy: 0.579000\n",
      "train_loss: 0.108714, train_accuracy: 0.582638\n",
      "test_loss: 0.109120, test_accuracy: 0.579000\n",
      "train_loss: 0.108683, train_accuracy: 0.581736\n",
      "test_loss: 0.109086, test_accuracy: 0.578500\n",
      "train_loss: 0.108656, train_accuracy: 0.580383\n",
      "test_loss: 0.109056, test_accuracy: 0.578500\n",
      "train_loss: 0.108631, train_accuracy: 0.579707\n",
      "test_loss: 0.109030, test_accuracy: 0.577500\n",
      "train_loss: 0.108609, train_accuracy: 0.578805\n",
      "test_loss: 0.109005, test_accuracy: 0.576000\n",
      "train_loss: 0.108590, train_accuracy: 0.578805\n",
      "test_loss: 0.108984, test_accuracy: 0.576500\n",
      "train_loss: 0.108572, train_accuracy: 0.579030\n",
      "test_loss: 0.108964, test_accuracy: 0.576500\n",
      "train_loss: 0.108556, train_accuracy: 0.578805\n",
      "test_loss: 0.108946, test_accuracy: 0.578000\n",
      "train_loss: 0.108542, train_accuracy: 0.577903\n",
      "test_loss: 0.108930, test_accuracy: 0.578500\n",
      "train_loss: 0.108529, train_accuracy: 0.577227\n",
      "test_loss: 0.108916, test_accuracy: 0.579000\n",
      "train_loss: 0.108517, train_accuracy: 0.577227\n",
      "test_loss: 0.108902, test_accuracy: 0.578500\n",
      "train_loss: 0.108506, train_accuracy: 0.577001\n",
      "test_loss: 0.108890, test_accuracy: 0.578000\n",
      "train_loss: 0.108497, train_accuracy: 0.575874\n",
      "test_loss: 0.108879, test_accuracy: 0.578500\n",
      "train_loss: 0.108488, train_accuracy: 0.574972\n",
      "test_loss: 0.108869, test_accuracy: 0.578000\n",
      "train_loss: 0.108480, train_accuracy: 0.574521\n",
      "test_loss: 0.108860, test_accuracy: 0.577000\n",
      "train_loss: 0.108472, train_accuracy: 0.574521\n",
      "test_loss: 0.108852, test_accuracy: 0.576000\n",
      "train_loss: 0.108466, train_accuracy: 0.574295\n",
      "test_loss: 0.108844, test_accuracy: 0.573500\n",
      "train_loss: 0.108460, train_accuracy: 0.574070\n",
      "test_loss: 0.108837, test_accuracy: 0.573000\n",
      "train_loss: 0.108454, train_accuracy: 0.574070\n",
      "test_loss: 0.108830, test_accuracy: 0.572500\n",
      "train_loss: 0.108449, train_accuracy: 0.573844\n",
      "test_loss: 0.108825, test_accuracy: 0.572500\n",
      "Sequential training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequential training evaluation'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sequential training\"\"\"\n",
    "batch_size = 1000\n",
    "\n",
    "epoch_train_accuracy = []\n",
    "epoch_test_accuracy = []\n",
    "for epoch in range(40):\n",
    "    #pbar = tqdm.tqdm(total=len(x_train), desc='sequential training phase')\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        y_batch = y_train_seq[i:i+batch_size]\n",
    "        if len(x_batch) != batch_size:\n",
    "            break\n",
    "        sess.run(E_1_seq, feed_dict={X_: x_batch, Y: y_batch})\n",
    "        #pbar.update(n=len(x_batch))\n",
    "    '''epoch evaluation'''\n",
    "    [train_loss, train_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_train_encoded, Y: y_train})\n",
    "    [test_loss, test_accuracy] = sess.run([loss_seq, accuracy_seq], feed_dict={X_: x_test_encoded, Y: y_test})\n",
    "    print('train_loss: %f, train_accuracy: %f' % (train_loss, train_accuracy))\n",
    "    print('test_loss: %f, test_accuracy: %f' % (test_loss, test_accuracy))\n",
    "    epoch_train_accuracy.append(train_accuracy)\n",
    "    epoch_test_accuracy.append(test_accuracy)\n",
    "#sess.run(init_train_graph, feed_dict={X: x_train_init, Y: y_train_init})\n",
    "print(\"Sequential training done\")\n",
    "\n",
    "\"\"\"Sequential training evaluation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08947542 -0.00926787  0.3444426   0.32700074  0.01942105  0.30486906]]\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-923348c55c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3099\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3100\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3101\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3102\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3103\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1717\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1718\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1719\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5125\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5127\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5128\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    609\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    610\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 611\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75483d2470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logits_ = subnet_output(alpha_1, beta_1, X_) \n",
    "logits__ = sess.run(logits_, feed_dict={X_: [x_test_encoded[4]]})\n",
    "print(logits__)\n",
    "print(np.argmax(logits__))\n",
    "print(y_test[4])\n",
    "plt.imshow(x_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alpha(alpha, size):\n",
    "    tmp = sess.run(alpha)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            plt.subplot(2,5,i*5+j+1)\n",
    "            plt.imshow(np.reshape(tmp[:,i*5+j], [size,size]))\n",
    "\n",
    "def visualize_beta(beta):\n",
    "    tmp = sess.run(beta)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(tmp)\n",
    "    \n",
    "            \n",
    "\"\"\"visualize subnet nodes\"\"\"            \n",
    "visualize_alpha(alpha_1, 28)\n",
    "visualize_beta(beta_1)\n",
    "visualize_alpha(alpha_2, 28)\n",
    "visualize_beta(beta_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-elm check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, classes):\n",
    "    I = np.eye(classes)\n",
    "    #print(I)\n",
    "    ret = []\n",
    "    l = 0\n",
    "    for y_ in y:\n",
    "        ret.append(I[y_])\n",
    "            \n",
    "    return np.array(ret)\n",
    "    \n",
    "y_train_onehot = one_hot(y_train, 6)\n",
    "y_test_onehot = one_hot(y_test,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 6\n",
    "output_size = 6 # mnist has 10 output classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_elm import ML_ELM\n",
    "\n",
    "mlelm1 = ML_ELM(input_size=input_size, output_size=output_size, name='mlelm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlelm1.add_layer(n_neurons=101, l2norm=1e2); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlelm1.add_layer(n_neurons=101, activation=tf.tanh, l2norm=1e2); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-10 12:37:29.618669 ML-ELM Start training...\n",
      "AE Network parameters have been initialized\n",
      "Training AE 1/1\n",
      "2019-05-10 12:37:29.792763 Start training...\n",
      "Training of AE mlelm1 ended in 0:0:0.093224\n",
      "Evaluating AE performance...\n",
      "MSE: 0.0002681\n",
      "####################################################################################################\n",
      "Initializing last layer ELM...\n",
      "Training last layer ELM...\n",
      "Training of ELM mlelm1 ended in 0:0:0.097100\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "TOTAL Training of ML_ELM mlelm1 ended in 0:0:0.583552\n",
      "####################################################################################################\n",
      "Evaluating network performance\n",
      "Accuracy: 0.5076603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.50766027"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlelm1.fit(x_train_encoded, y_train_onehot, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating network performance\n",
      "Accuracy: 0.5035000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5035"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlelm1.evaluate(x_test_encoded, y_test_onehot, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
